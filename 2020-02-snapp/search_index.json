[
["index.html", "Data Science and Collaboration Skills for Integrative Conservation Science 1 Data Science and Collaboration Skills for Integrative Conservation Science 1.1 Schedule", " Data Science and Collaboration Skills for Integrative Conservation Science February 18-21, 2020 1 Data Science and Collaboration Skills for Integrative Conservation Science This intensive 4-day workshop on Data Science and Collaboration Skills for Integrative Conservation Science will be held at NCEAS, Santa Barbara, CA from Feb 18 to Feb 21, 2020. This training, sponsored by SNAPP, aims to bring together the SNAPP and NCEAS postdoctoral associates to foster communities and collaboration, as well as promote scientific computing and open science best practices. 1.1 Schedule 1.1.1 Code of Conduct Please note that by participating in an NCEAS activity you agree to abide by our Code of Conduct "],
["motivation.html", "2 Motivation 2.1 Learning Objectives 2.2 Reproducible Research 2.3 How can version control help? 2.4 Collaborative Research", " 2 Motivation 2.1 Learning Objectives In this lesson, you will learn: What computational reproducibility is and why it is useful How version control can increase computational reproducibility How to check to make sure your RStudio environment is set up properly for analysis How to set up git 2.2 Reproducible Research Reproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. 2.2.1 What is needed for computational reproducibility? The first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. And yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). 2.2.2 Conceptualizing workflows Scientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization. Workflows can range in detail from simple flowcharts to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions. 2.3 How can version control help? 2.3.1 The problem with renaming files Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, often all we use to track all these changes are duplicating files and embedding some logic into filenames. You might think there is a better way, and you’d be right: version control. Version control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming and duplication. In version control systems like git, the system tracks not just the name of the file, but also its contents, so that when contents change, it can tell you which pieces went where. It tracks which version of a file a new version came from. So its easy to draw a graph showing all of the versions of a file, like this one: Version control systems assign an identifier to every version of every file, and track their relationships. They also allow branches in those versions, and merging those branches back into the main line of work. They also support having multiple copies on multiple computers for backup, and for collaboration. And finally, they let you tag particular versions, such that it is easy to return to a set of files exactly as they were when you tagged them. For example, the exact versions of data, code, and narrative that were used when a manuscript was submitted might be R2 in the graph above. These versions a tight together as an history that you can navigate to go back to any specific version of a file. A more concrete example will be an R script. After implementing several modifications, you realize that your code is not working anymore, with version control you can go back to the previous version that used to work. 2.4 Collaborative Research Most of the environmental sciences research is done in a collaborative set up. This especially true with the synthesis working group model that NCEAS has been using and refining over the years. These groups of experts with different background, tools and methods, have to collaborate together and build on others work (particularly data). In this tight collaborative setup involving people from many different institutions, we recommend to manage your data and codes on a unique server / remote machine using the following strategies to centralize and share your work. 2.4.1 Using Shared Folders for Data Sets To centralize the management of your data, we recommend to set up shared folder (or also named directories) on the server and give access to all the members of your group. This will prevent the duplication of data and reduce the risk of people working on different versions of your data sets. Regarding your input data, we recommend to write protect them, so nobody can overwrite your original data on accident. For output data, you can develop different strategies. Note it is often useful to see the outputs data more as a secondary product of a specific version of your code. In other words, you should be always able to reproduce a specific results if you go back to the corresponding version of your code. 2.4.2 Using git and GitHub for Codes For code sharing, we recommend a different setup than the one for data. Your scripts should be stored under your personal folder on the server and you should not be sharing this folder with others. You should rely on a version control system to version and share your code. This will allow to manage more efficiently potential conflict between different versions of the same script. In this workshop, we will use git a version control system and relying on the service GitHub to share or code. T References "],
["working-on-a-remote-machine.html", "3 Working on a Remote Machine 3.1 Learning Objectives 3.2 Why working on a remote machine? 3.3 RStudio Server", " 3 Working on a Remote Machine 3.1 Learning Objectives In this lesson, you will learn: How to connect to a remote server Get familiar with RStudio server 3.2 Why working on a remote machine? Often the main motivation is to scale your analysis beyond what a personal computer can handle. R being pretty memory intensive, moving to a server often provides you more RAM and thus allows to load larger data in R without the need of slicing your data into chunks. But there are also other advantages, here are the main for scientits: Power: More CPUs/Cores (24/32/48), More RAM (256/384GB) Capacity: More disk space and generally* faster storage* (in highly optimized RAID arrays) Security: Data are spread across multiple drives and have nightly backups Collaboration: shared folders for code, data, and other materials; same software versions 3.2.1 NCEAS analytical server 88 logical cores (vCPUs) / 44 physical cores with hyperthreading 3.6 GHz Turbo / 2.6 GHz minimum 512 GB 2133 MHz ECC DDR4 memory 48 TB fast storage array (RAID10 with 800GB SSD cache) Nightly Backups =&gt; For more info about Aurora: https://help.nceas.ucsb.edu/high_performance_computing 3.2.2 Working Group / Collaborative Setup There are additional reasons of particular importance in a collaborative set up, such as a working group: Centralizing data management: As you know synthesis science is data intensive and often require to deal with a large number of heterogeneous data files. It can be complicated to make sure every collaborators as access to all the data they need. It is even harder to ensure that the exact same version of the data is used by everyone. Moving your workflow to a server, will allow to have only one copy of the data that you can share with all your collaborators. Even better, since everyone can access the same data, everybody will have the exact same path in their script!! Make sure your files are safe: Generally, servers are managed by a System Administrator. This person is in charge of keeping the server up-to-date, secured from malwares and set up back up strategies to ensure all the files on the server are backed up. When using cloud solutions, you should always check if a back up plan is available for the resources your using. 3.2.3 What does working on a remote server means? What does it mean for your workflow? The good news is that RStudio Server makes it very easy for RStudio users to start using a server for their analysis. The main changes are about: File management: you will need to learn to move files (incluing your R scripts) to the server Package installation: You can still install the R packages you need under your user (with some limitations). However some R packages will be already installed at the server level. 3.3 RStudio Server From an user perspective, RStudio Server is your familiar RStudio interface in your web browser. The big difference however is that with RStudio Server the computation will be running on the remote machine instead of your local personal computer. This also means that the files you are seeing through the RStudio Server interface are located on the remote machine. And this also include your R packages!!! This remote file management is the main change you will have to adopt in your workflow. To help with remote files management, the RStudio Server interface as few additional features that we will be discussing in the following sections. 3.3.1 Connecting to NCEAS Analytical Server Got to: https://aurora.nceas.ucsb.edu/ Select Login to RStudio Server Enter your credentials You are in! "],
["rstudio-and-git-setup.html", "4 RStudio and Git Setup 4.1 Checking the RStudio environment 4.2 Setting up git 4.3 Other Thoughts", " 4 RStudio and Git Setup 4.1 Checking the RStudio environment 4.1.1 R Version We will use R version 3.6.1, which you can download and install from CRAN. To check your version, run this in your RStudio console: R.version$version.string 4.1.2 RStudio Version We will be using RStudio version 1.2.500 or later, which you can download and install here To check your RStudio version, run the following in your RStudio console: RStudio.Version()$version If the output of this does not say 1.2.500 or higher, you should update your RStudio. Do this by selecting Help -&gt; Check for Updates and follow the prompts. 4.1.3 Package installation Run the following lines to check that all of the packages we need for the training are installed on your computer. packages &lt;- c(&quot;DT&quot;, &quot;devtools&quot;, &quot;tidyverse&quot;, &quot;ggmap&quot;, &quot;ggplot2&quot;, &quot;leaflet&quot;, &quot;readxl&quot;, &quot;tidyr&quot;, &quot;scales&quot;, &quot;sf&quot;, &quot;raster&quot;, &quot;rmarkdown&quot;, &quot;roxygen2&quot;, &quot;broom&quot;, &quot;captioner&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } If you haven’t installed all of the packages, this will automatically start installing them. If they are installed, it won’t do anything. Next, create a new R Markdown (File -&gt; New File -&gt; R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes. At this point, RStudio and R should be all set up. 4.2 Setting up git If you haven’t already, go to github.com and create an account. If you haven’t downloaded git already, you can download it here. Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -&gt; Terminal -&gt; New Terminal. A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter: git config --global user.name &quot;Matt Jones&quot; Next, enter the following line, with the email address you used when you created your account on github.com: git config --global user.email &quot;gitcode@magisa.org&quot; Note that these lines need to be run one at a time. Finally, check to make sure everything looks correct by entering this line, which will return the options that you have set. git config --global user.name git config --global user.email Optional: Check that everything is correct: git config --global --list Modify everything at the same time: git config --global --edit Set your text editor: git config --system core.editor nano Here nano is used as example; you can choose most of the text editor you might have installed on your computer (atom, sublime, notepad++ …). Problem with any of those steps? Check out Jenny Bryan Happy git trouble shooting section 4.3 Other Thoughts 4.3.1 Linking git and RStudio In most of the cases, RStudio should automatically detect git when it is installed on your computer. The best way to check this is to go to the Tools menu -&gt; Global Options and click on git/SVN If git is properly setup, the window should look like this: Click OK. Note: if git was not enabled, you might be asked to restart RStudio to enable it. 4.3.2 Note for Windows Users If you get “command not found” (or similar) when you try these steps through the RStudio terminal tab, you may need to set the type of terminal that gets launched by RStudio. Under some git install scenarios, the git executable may not be available to the default terminal type. In addition, some versions of Windows have difficulties with the command line if you are using an account name with spaces in it (such as “Matt Jones”, rather than something like “mbjones”). You may need to use an account name without spaces. 4.3.3 Updating a previous R installation This is useful for users who already have R with some packages installed and need to upgrade R, but don’t want to lose packages. If you have never installed R or any R packages before, you can skip this section. If you already have R installed, but need to update, and don’t want to lose your packages, these two R functions can help you. The first will save all of your packages to a file. The second loads the packages from the file and installs packages that are missing. Save this script to a file (e.g. package_update.R). #&#39; Save R packages to a file. Useful when updating R version #&#39; #&#39; @param path path to rda file to save packages to. eg: installed_old.rda save_packages &lt;- function(path){ tmp &lt;- installed.packages() installedpkgs &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) save(installedpkgs, file = path) } #&#39; Update packages from a file. Useful when updating R version #&#39; #&#39; @param path path to rda file where packages were saved update_packages &lt;- function(path){ tmp &lt;- new.env() installedpkgs &lt;- load(file = path, envir = tmp) installedpkgs &lt;- tmp[[ls(tmp)[1]]] tmp &lt;- installed.packages() installedpkgs.new &lt;- as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) missing &lt;- setdiff(installedpkgs, installedpkgs.new) install.packages(missing) update.packages() } Source the file that you saved above (e.g.: source(package_update.R)). Then, run the save_packages function. save_packages(&quot;installed.rda&quot;) Then quit R, go to CRAN, and install the latest version of R. Source the R script that you saved above again, and then run: update_packages(&quot;installed.rda&quot;) This should install all of your R packages that you had before you upgraded. "],
["introduction-to-r-and-rmarkdown.html", "5 Introduction to R and RMarkdown 5.1 Learning Objectives 5.2 Introduction and Motivation 5.3 R at the console 5.4 RMarkdown 5.5 R functions, help pages 5.6 Using data.frames 5.7 Troubleshooting 5.8 Literate Analysis 5.9 Exercise", " 5 Introduction to R and RMarkdown 5.1 Learning Objectives In this lesson we will: get oriented to the RStudio interface work with R in the console explore RMarkdown be introduced to built-in R functions learn to use the help pages 5.2 Introduction and Motivation There is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source! So what is the difference between R and RStudio? Here is an analogy to start us off. If you were a chef, R is a knife. You have food to prepare, and the knife is one of the tools that you’ll use to accomplish your task. And if R were a knife, RStudio is the kitchen. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it’s not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It’s also data science philosophy, R packages, community, and more. So although you can prepare food without a kitchen and we could learn R without RStudio, that’s not what we’re going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together. Something else to start us off is to mention that you are learning a new language here. It’s an ongoing process, it takes time, you’ll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it’s a similar process, really. And no matter how fluent you are, you’ll always be learning, you’ll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunications that can be frustrating, but hands down we are all better off because of it. While language is a familiar concept, programming languages are in a different context from spoken languages, but you will get to know this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it’s “breakfast”. So if you’re learning Spanish, you could expect there is a word for this concept of a first meal. (And you’d be right: ‘desayuno’). We will get you to expect that programming languages also have words (called functions in R) for concepts as well. You’ll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you increase your expectations and learn to ask and find what you’re looking for. 5.2.1 Resources This lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan’s materials. I definitely recommend reading through the original lessons and using them as reference: Julie Lowndes’ Data Science Training for the Ocean Health Index R, RStudio, RMarkdown Programming in R Jenny Bryan’s lectures from STAT545 at UBC R basics, workspace and working directory, RStudio projects Basic care and feeding of data in R RStudio has great resources as well: webinars cheatsheets Finally, Hadley Wickham’s book R for Data Science is a great resource to get more in depth. -R for Data Science Other resources: LaTeX Equation Formatting Base R Cheatsheet RMarkdown Reference Guide MATLAB/R Translation Cheat Sheet 5.3 R at the console Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. An important first question: where are we? If you’ve just opened RStudio for the first time, you’ll be in your Home directory. This is noted by the ~/ at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won’t change where you are: even as you click through you’ll still be Home: ~/. OK let’s go into the Console, where we interact with the live R process. We use R to calculate things for us, so let’s do some simple math. 3*4 ## [1] 12 You can assign the value of that mathematic operation to a variable, or object, in R. You do this using the assignment operator, &lt;-. Make an assignment and then inspect the object you just created. x &lt;- 3 * 4 x ## [1] 12 In my head I hear, e.g., “x gets 12”. All R statements where you create objects – “assignments” – have this form: objectName &lt;- value. I’ll write it in the console with a hash #, which is the way R comments so it won’t be evaluated. ## objectName &lt;- value ## This is also how you write notes in your code to explain what you are doing. Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. # i_use_snake_case # other.people.use.periods # evenOthersUseCamelCase Make an assignment this_is_a_really_long_name &lt;- 2.5 To inspect this variable, instead of typing it, we can press the up arrow key and call your command history, with the most recent commands first. Let’s do that, and then delete the assignment: this_is_a_really_long_name ## [1] 2.5 Another way to inspect this variable is to begin typing this_…and RStudio will automagically have suggested completions for you that you can select by hitting the tab key, then press return. One more: science_rocks &lt;- &quot;yes it does!&quot; You can see that we can assign an object to be a word, not a number. In R, this is called a “string”, and R knows it’s a word and not a number because it has quotes &quot; &quot;. You can work with strings in your data in R pretty easily, thanks to the stringr and tidytext packages. We won’t talk about strings very much specifically, but know that R can handle text, and it can work with text and numbers together. Strings and numbers lead us to an important concept in programming: that there are different “classes” or types of objects. An object is a variable, function, data structure, or method that you have written to your environment. You can see what objects you have loaded by looking in the “environment” pane in RStudio. The operations you can do with an object will depend on what type of object it is. This makes sense! Just like you wouldn’t do certain things with your car (like use it to eat soup), you won’t do certain operations with character objects (strings), for example. Try running the following line in your console: &quot;Hello world!&quot; * 3 What happened? Why? You may have noticed that when assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name: weight_kg &lt;- 55 # doesn&#39;t print anything (weight_kg &lt;- 55) # but putting parenthesis around the call prints the value of `weight_kg` ## [1] 55 weight_kg # and so does typing the name of the object ## [1] 55 Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg): 2.2 * weight_kg ## [1] 121 We can also change a variable’s value by assigning it a new one: weight_kg &lt;- 57.5 2.2 * weight_kg ## [1] 126.5 This means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a new variable, weight_lb: weight_lb &lt;- 2.2 * weight_kg and then change weight_kg to 100. weight_kg &lt;- 100 What do you think is the current content of the object weight_lb? 126.5 or 220? Why? You can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is the function c(), which stands for combine or concatenate. Here we will create a vector of weights in kilograms, and convert them to pounds, saving the weight in pounds as a new object. weight_kg &lt;- c(55, 25, 12) weight_kg ## [1] 55 25 12 weight_lb &lt;- weight_kg * 2.2 weight_lb ## [1] 121.0 55.0 26.4 5.3.1 Error messages are your friends Implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type. Remember that this is a language, not unsimilar to English! There are times you aren’t understood – it’s going to happen. There are different ways this can happen. Sometimes you’ll get an error. This is like someone saying ‘What?’ or ‘Pardon’? Error messages can also be more useful, like when they say ‘I didn’t understand this specific part of what you said, I was expecting something else’. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. And also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I’m telling a story about tables and you’re picturing where you eat breakfast and I’m talking about data. This can leave me thinking I’ve gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused… So write clean code and check your work as you go to minimize these circumstances! 5.3.2 Logical operators and expressions A moment about logical operators and expressions. We can ask questions about the objects we just made. == means ‘is equal to’ != means ‘is not equal to’ &lt; means ` is less than’ &gt; means ` is greater than’ &lt;= means ` is less than or equal to’ &gt;= means ` is greater than or equal to’ weight_kg == 2 ## [1] FALSE FALSE FALSE weight_kg &gt;= 30 ## [1] TRUE FALSE FALSE weight_kg != 5 ## [1] TRUE TRUE TRUE Shortcuts You will make lots of assignments and the operator &lt;- is a pain to type. Don’t be lazy and use =, although it would work, because it will just sow confusion later. Instead, utilize RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automagically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. 5.3.3 Clearing the environment Now look at the objects in your environment (workspace) – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with a few different R commands: objects() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; ls() ## [1] &quot;science_rocks&quot; &quot;this_is_a_really_long_name&quot; ## [3] &quot;weight_kg&quot; &quot;weight_lb&quot; ## [5] &quot;x&quot; If you want to remove the object named weight_kg, you can do this: rm(weight_kg) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 5.4 RMarkdown Now that we know some basic R syntax, let’s learn a little about RMarkdown. You will drive yourself crazy (and fail to generate a reproducible workflow!) running code directly in the console. RMarkdown is really key for collaborative research, so we’re going to get started with it early and then use it for the rest of the course. An RMarkdown file will allow us to weave markdown text with chunks of R code to be evaluated and output content like tables and plots. File -&gt; New File -&gt; RMarkdown… -&gt; Document of output format HTML, OK. You can give it a Title like “My Project”. Then click OK. OK, first off: by opening a file, we are seeing the 4th pane of the RStudio console, which is essentially a text editor. This lets us organize our files within RStudio instead of having a bunch of different windows open. Let’s have a look at this file — it’s not blank; there is some initial text is already provided for you. Notice a few things about it: There are white and grey sections. R code is in grey sections, and other text is in white. Let’s go ahead and “Knit HTML” by clicking the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Create a new folder for it somewhere that you will be able to find again (such as your Desktop or Documents), and name that folder something you’ll remember (like arctic_training_files). What do you notice between the two? Notice how the grey R code chunks are surrounded by 3 backticks and {r LABEL}. These are evaluated and return the output text in the case of summary(cars) and the output plot in the case of plot(pressure). Notice how the code plot(pressure) is not shown in the HTML output because of the R code chunk option echo=FALSE. More details… This RMarkdown file has 2 different languages within it: R and Markdown. We don’t know that much R yet, but you can see that we are taking a summary of some data called ‘cars’, and then plotting. There’s a lot more to learn about R, and we’ll get into it for the next few days. The second language is Markdown. This is a formatting language for plain text, and there are only about 15 rules to know. Notice the syntax for: headers get rendered at multiple levels: #, ## bold: **word** There are some good cheatsheets to get you started, and here is one built into RStudio: Go to Help &gt; Markdown Quick Reference Important: note that the hashtag # is used differently in Markdown and in R: in R, a hashtag indicates a comment that will not be evaluated. You can use as many as you want: # is equivalent to ######. It’s just a matter of style. in Markdown, a hashtag indicates a level of a header. And the number you use matters: # is a “level one header”, meaning the biggest font and the top of the hierarchy. ### is a level three header, and will show up nested below the # and ## headers. Learn more: http://rmarkdown.rstudio.com/ 5.4.1 Your Turn In Markdown, Write some italic text, and make a numbered list. And add a few subheaders. Use the Markdown Quick Reference (in the menu bar: Help &gt; Markdown Quick Reference). Reknit your html file. 5.4.2 Code chunks OK. Now let’s practice with some of those commands. Create a new chunk in your RMarkdown first in one of these ways: click “Insert &gt; R” at the top of the editor pane type by hand ```{r} ``` if you haven’t deleted a chunk that came with the new file, edit that one Now, let’s write some R code. x &lt;- 4*3 x Now, hitting return does not execute this command; remember, it’s just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let’s do each of them): copy-paste this line into the console. select the line (or simply put the cursor there), and click ‘Run’. This is available from the bar above the file (green arrow) the menu bar: Code &gt; Run Selected Line(s) keyboard shortcut: command-return click the green arrow at the right of the code chunk 5.4.3 Your turn Add a few more commands to your file. Execute them by trying the three ways above. Then, save your R Markdown file. 5.5 R functions, help pages So far we’ve learned some of the basic syntax and concepts of R programming, how to navigate RStudio, and RMarkdown, but we haven’t done any complicated or interesting programming processes yet. This is where functions come in! A function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are “calling” a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions. All functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. The pieces of information that the function needs to do its job are called arguments. So the syntax will look something like: result_value &lt;- function_name(argument1 = value1, argument2 = value2, ...). 5.5.1 A simple example To take a very simple example, let’s look at the mean() function. As you might expect, this is a function that will take the mean of a set of numbers. Very convenient! Let’s create our vector of weights again: weight_kg &lt;- c(55, 25, 12) and use the mean function to calculate the mean weight. mean(weight_kg) ## [1] 30.66667 5.5.2 Getting help What if you know the name of the function that you want to use, but don’t know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions. To access the help page for mean, enter the following into your console: ?mean The help pane will show up in the lower right hand corner of your RStudio. The help page is broken down into sections: Description: An extended description of what the function does. Usage: The arguments of the function(s) and their default values. Arguments: An explanation of the data each argument is expecting. Details: Any important details to be aware of. Value: The data the function returns. See Also: Any related functions you might find useful. Examples: Some examples for how to use the function. 5.5.3 Your turn Exercise: Talk to your neighbor(s) and look up the help file for a function that you know or expect to exist. Here are some ideas: ?getwd(), ?plot(), min(), max(), ?log()). And there’s also help for when you only sort of remember the function name: double-questionmark: ??install Not all functions have (or require) arguments: date() ## [1] &quot;Fri Feb 21 06:39:23 2020&quot; 5.5.4 Use a function to read a file into R So far we have learned how to assign values to objects in R, and what a function is, but we haven’t quite put it all together yet with real data yet. To do this, we will introduce the function read.csv, which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R. Since this is our irst time using this function, first access the help page for read.csv. This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let’s get a file! 5.5.4.1 Download a file from the Arctic Data Center Navigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X., and download the first csv file called “BGchem2008data.csv” Move this file from your Downloads folder into a place you can more easily find it. I recommend creating a folder called data in your previously-created directory arctic_training_files, and putting the file there. Now we have to tell read.csv how to find the file. We do this using the file argument which you can see in usage section in the help page. In RMarkdown, you can either use absolute paths (which will start with your home directory ~/) or paths relative to the location of the RMarkdown. RStudio and RMarkdown have some great autocomplete capabilities when using relative paths, so we will go that route. Assuming you have moved your file to a folder within arctic_training_files called data, your read.csv call will look like this: bg_chem &lt;- read.csv(&quot;data/BGchem2008data.csv&quot;) You should now have an object of the class data.frame in your environment called bg_chem. Check your environment pane to ensure this is true. Note that in the help page there are a whole bunch of arguments that we didn’t use in the call above. Some of the arguments in function calls are optional, and some are required. Optional arguments will be shown in the usage section with a name = value pair, with the default value shown. If you do not specify a name = value pair for that argument in your function call, the function will assume the default value (example: header = TRUE for read.csv). Required arguments will only show the name of the argument, without a value. Note that the only required argument for read.csv is file. You can always specify arguments in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want file = &quot;data/BGchem2008data.csv&quot;, since file is the first argument. If we wanted to add another argument, say stringsAsFactors, we need to specify it explicitly using the name = value pair, since the second argument is header. For functions I call often, I use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Many R users (including myself) will override the default stringsAsFactors argument using the following call: bg_chem &lt;- read.csv(&quot;data/BGchem2008data.csv&quot;, stringsAsFactors = FALSE) 5.6 Using data.frames A data.frame is a two dimensional data structure in R that mimics spreadsheet behavior. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents a measurement of that variable. When we ran read.csv, the object bg_chem that we created is a data.frame. There are a a bunch of ways R and RStudio help you explore data frames. Here are a few, give them each a try: click on the word bg_chem in the environment pane click on the arrow next to bg_chem in the environment pane execute head(bg_chem) in the console execute View(bg_chem) in the console Usually we will want to run functions on individual columns in a data.frame. To call a specific column, we use the list subset operator $. Say you want to look at the first few rows of the Date column only. This would do the trick: head(bg_chem$Date) ## [1] &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; &quot;2008-03-21&quot; ## [6] &quot;2008-03-22&quot; How about calculating the mean temperature of all the CTD samples? mean(bg_chem$CTD_Temperature) ## [1] -0.9646915 Or, if we want to save this to a variable to use later: mean_temp &lt;- mean(bg_chem$CTD_Temperature) You can also create basic plots using the list subset operator. plot(bg_chem$CTD_Depth, bg_chem$CTD_Temperature) There are many more advancted tools and functions in R that will enable you to make better plots using cleaner syntax, we will cover some of these later in the course. 5.6.1 Your Turn Exercise: Spend a few minutes exploring this dataset. Try out different functions on columns using the list subset operator and experiment with different plots. 5.7 Troubleshooting 5.7.1 My RMarkdown won’t knit to PDF If you get an error when trying to knit to PDF that says your computer doesn’t have a LaTeX installation, one of two things is likely happening: Your computer doesn’t have LaTeX installed You have an installation of LaTeX but RStudio cannot find it (it is not on the path) If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn’t covered here. If you fall in the first category - you are sure you don’t have LaTeX installed - can use the R package tinytex to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer. To install tinytex run: install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() If you get an error that looks like destination /usr/local/bin not writable, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal: sudo chown -R `whoami`:admin /usr/local/bin and then try the above install instructions again. More information about tinytex can be found here 5.7.2 I just entered a command and nothing is happening It may be because you didn’t complete a command: is there a little + in your console? R is saying that it is waiting for you to finish. In the example below, I need to close that parenthesis. &gt; x &lt;- seq(1, 10 + You can either just type the closing parentheses here and push return, or push the esc button twice. 5.7.3 R says my object is not found New users will frequently see errors that look like this: Error in mean(myobject) : object 'myobject' not found This means that you do not have an object called myobject saved in your environment. The common reasons for this are: typo: make sure your object name is spelled exactly like what shows up in the console. Remember R is case sensitive. not writing to a variable: note that the object is only saved in the environment if you use the assignment operator, eg: myobject &lt;- read.csv(...) not executing the line in your RMarkdown: remember that writing a line of code in RMarkdown is not the same as writing in the console, you have to execute the line of code using command + enter, running the chunk, or one of the other ways outlined in the RMarkdown section of this training 5.8 Literate Analysis RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. Here is an example of a real analysis workflow written using RMarkdown. 5.9 Exercise Create an RMarkdown document with some of your own data. If you don’t have a good dataset handy, use the example dataset here: Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X. Your document should contain the following sections: Introduction to your dataset Include an external link Simple analysis Presentation of a result A plot In-line description of results "],
["version-control-with-git-and-github.html", "6 Version Control with git and GitHub 6.1 Learning Objectives 6.2 The problem with “save_as” 6.3 git 6.4 First Repository 6.5 Getting a Local Copy of a Repository 6.6 Tracking File Changes with git 6.7 Collaboration and conflict free workflows 6.8 Exercise 6.9 Advanced topics 6.10 References", " 6 Version Control with git and GitHub 6.1 Learning Objectives In this lesson, you will learn: Why git is useful for reproducible analysis How to use git to track changes to your work over time How to use GitHub to collaborate with others How to structure your commits so your changes are clear to others How to write effective commit messages 6.2 The problem with “save_as” Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control. Version control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In other words, version control is a system that helps you to manage the different versions of your files in an organized manner. It will help you to never have to duplicate files using save as as a way to keep different versions of a file (see below). Version control help you to create a timeline of snapshots containing the different versions of a file. At any point in time, you will be able to roll back to a specific version. Bonus: you can add a short description (commit message) to remember what each specific version is about. What is the difference between git and GitHub? git: version control software used to track files in a folder (a repository) git creates the versioned history of a repository GitHub: web site that allows users to store their git repositories and share them with others 6.3 git This training material focuses on the code versioning system called Git. Note that there are others, such as Mercurial or svn for example. Git is a free and open source distributed version control system. It has many functionalities and was originally geared towards software development and production environment. In fact, Git was initially designed and developed in 2005 by Linux kernel developers (including Linus Torvalds) to track the development of the Linux kernel. Here is a fun video of Linus Torvalds touting Git to Google. How does it work? Git can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information). What git is not: Git is not a backup per se Git is not good at versioning large files (there are workarounds) =&gt; not meant for data 6.4 First Repository Git can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information). Although there many ways to start a new repository, GitHub (or any other cloud solutions, such as GitLab) provide among the most convenient way of starting a repository. 6.4.1 GitHub GitHub is a company that hosts git repositories online and provides several collaboration features (among which forking). GitHub fosters a great user community and has built a nice web interface to git, also adding great visualization/rendering capacities of your data. GitHub.com: https://github.com A user account: https://github.com/brunj7 An organization account: https://github.com/nceas NCEAS GitHub instance: https://github.nceas.ucsb.edu/ 6.4.2 Let’s look at a repository on GitHub This screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes. If we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July: And finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file: Tracking these changes, and seeing how they relate to released versions of software and files is exactly what Git and GitHub are good for. We will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow. 6.4.3 Creating a Repository on GitHub We are going to create a new repository on your GitHub account. If you do not have an account yet, it is free to create one here: https://github.com/join?source=header-home To create a new repository follow these steps: Click on Enter a descriptive name for your new repository, training-repo (avoid upper case and use - instead of spaces or _) Write a 1-sentence description about the repository content Choose “Public” (as of January 2019, GitHub now offers unlimited free private repositories with a maximum of 3 collaborators) Check “Initialize this repository with a README” Add a .gitignore file (optional). As the name suggest, the gitignore file is used to specify the file format that git should not track. GitHub offers pre-written gitignore files for commodity Add a license file (optional) Here is a website to look for more pre-written.gitignore files: https://github.com/github/gitignore =&gt; Here it is, you now have a repository in the cloud!! 6.4.4 First commit For simple changes to text files, you can make edits right in the GitHub web interface. For example, navigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add text, and when done, add a commit message, and hit the Commit changes button. Congratulations, you’ve now authored your first versioned commit. If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file. Let’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the blue header above the file listing shows the most recent commit, along with its commit message, and its SHA identifer. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (ad96b85), it will display the set of changes made in that particular commit. In the next section we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio. 6.5 Getting a Local Copy of a Repository The next step is going to get a local copy of this repository to your personal computer. In git jargon, creating an exact copy of a repository on your local computer is called cloning. RStudio can help us to clone a repository. Since RStudio Projects also work at the folder/directory level, it is the “unit” that is going to be used to link a repository to RStudio. You can create a new RStudio Project from the upper-right corner of the RStudio IDE window, choosing New Project Choose Version Control Select git Go back to your web browser and from the GitHub repository page click on the green clone or download button and copy the URL to your repository. Note: The URL should start with “https://”. If it starts with “git@github.com”, click “Use HTTPS” above the URL. Paste this URL in the first box and leave the second box empty. Finally select a location on your HD where the repository will be cloned to. Click Create Project =&gt; Congratulations!! you have cloned the repository to your computer and created a RStudio project out of it. You can also use your computer file browser to look at the files in the repository. You have two files: The training-repo.Rproj file for the RStudio Project you just created. Note that because we left the second box empty on step 5, the name of the repository was used to name the RStudio project. This file will be what you open to begin working on your R and RMarkdown scripts. The Rproj file will save your settings and open tabs when you close the project, and will restore these settings the next time you open it. The README.md file that was automatically generated by GitHub when creating the repository If you look again at your repository page on GitHub you will noticed that the .Rproj file is not there. It is because this file was created by RStudio on your local machine and you have not yet tried to synchronize the files between your local copy and the one in the cloud (remote copy in git jargon). Note also that the .gitignore file is not showing up in the Finder view. It is because files with a name starting with a dot are considered “hidden”. By default most of OS will not show those files. However if you use the Files panel in RStudio, you can see the .gitignore file. We are going to edit the README.md file, adding more information about the repository (purpose of the this file). You can directly edit this file in RStudio. You can open the file by clicking on its name from the Files tab in the lower-right panel. 6.6 Tracking File Changes with git 6.6.1 Basic Workflow Overview You modify files in your working directory and save them as usual You add snapshots of your changed files to your staging area You do a commit, which takes the files as they are in the staging area and permanently stores them as snapshots to your Git directory. We can make an analogy with taking a family picture, where each family member would represent a file. Adding files (to the staging area), is like deciding which family member(s) are going to be on your next picture Committing is like taking the picture These 2-step process enables you to flexibly group files into a specific commit. These steps are repeated for every version you want to keep (every time you would like to use save as). Every time you commit, you create a new snapshot, you add the new version of the file to the git database, while keeping all the previous versions in the database. It creates an history of the content of your repository that is like a graph that you can navigate: 6.6.2 Using git from RStudio 6.6.2.1 Tracking changes RStudio provides a great interface to git helping you navigating the git workflow and getting information about the state of your repository through nice icons and visualizations of the information. If you click on the Git tab in your RStudio upper-right panel, you should see the following information The RStudio Git pane lists every file that’s been added, modified or deleted. The icon describes the change: In our case, it means that: the .gitignorefile has been modified since the last commit the .Rproj file has never been tracked by git (remember RStudio just created this project file for us) Note also that the README.md file is not listed, but it exists (see Filespane). It is because files with no modifications since last commit are not listed. GitHub has created the .gitignore file for us and we have not modified it since. So why is it listed as modified? We can check this by clicking on the Diff button (upper-left on the Git pane). We can see that a new line (in green) has been added at the end of the .gitignore file. In fact, RStudio did that when creating the project to make sure that some temporary files are not tracked by git. Let us improve the content of the README.md file as below to make it more descriptive. As soon as you saved your changes, you should see the README.md file listed as modified in the git pane. Let us look at the diff of the README.md file. As you can see, the original lines are in red, in other words for git those lines have been deleted. The new lines that we just typed are in green, which indicates that these lines have been added for git. Note the line numbers in the left margin that help you to track which line have been removed and added. 6.6.2.2 Keeping Changes as Snapshots Now we would like to save a snapshot of this version of the README.md file. Here are the steps we will need to do: Add the file to the next commit by checking the box in front of the file name in the git pane. Note that the icon M will move to the left to show you that this file is now staged to be part of the next commit Commit: Click the Commit button at the top of the git pane Write a short but descriptive commit message in the new window Click on the he Commit button to save this version of the file in the git database Close the windows to get back to the main RStudio window Once done, add both the .gitignore and the training-repo.Rproj and commit those files together. Note that the icons at the top of the git pane have been organized in sequence from left to right to match the git workflow. 6.6.2.3 Good Commit Message Tips Clearly, good documentation of what you’ve done is critical to making the version history of your repository meaningful and helpful. Its tempting to skip the commit message altogether, or to add some stock blurb like ‘Updates’. Its better to use messages that will be helpful to your future self in deducing not just what you did, but why you did it. Also, commit messages are best understood if they follow the active verb convention. For example, you can see that my commit messages all started with a past tense verb, and then explained what was changed. While some of the changes we illustrated here were simple and easily explained in a short phrase, for more complex changes, its best to provide a more complete message. The convention, however, is to always have a short, terse first sentence, followed by a more verbose explanation of the details and rationale for the change. This keeps the high level details readable in the version log. I can’t count the number of times I’ve looked at the commit log from 2, 3, or 10 years prior and been so grateful for diligence of my past self and collaborators. 6.6.3 Looking at the Repository History We have done 2 new commits at this point. Let us look at the commit timeline we have created so far. You can click on the Clock icon at the top to visualize the history. You can see that there has been 3 commits so far. The first one has been done by GitHub when we created the repository and the 2 commits we just did. The most recent commit is at the top. 6.6.4 Sending changes back to GitHub Now that we have created these two commits on our local machine, our local version of the repository is different from the version on GitHub. RStudio communicate this information to you. If you look below the icons on the git pane, you will see the warning message: “Your branch is ahead of ‘origin/master’ by two commits”. This can be translated as you have two additional commits on your local machine that you never shared back to the remote repository on GitHub. Open your favorite web browser and look at the content of your repository on GitHub. You will see the old version of the README.md and .gitignore file and no trace of the .Rproj file. There are two git commands to exchange between a local and remote versions of a repository: - pull: git will get the latest remote version and try to merge it with your local version - push: git will send your local version to the remote version of the repository (in our case GitHub) Before sending your local version to the remote, you should always get the latest remote version first. In other words, you should pull first and push second. This is the way git protects the remote version against incompatibilities with the local version. You always deal with potential problems on your local machine. Therefore your sequence will always be: pull push Of course RStudio have icons for that on top of the git pane, with the blue arrow down being for pull and the green arrow up being for push. Remember the icons are organized in sequence! Let us do the pull and push to synchronized the remote repositories. We have now synchronized the local (our computer) and remote (on GitHub) versions of our repository. You can now look at the page of your repository on GitHub, you should see 3 files with the exact same version that you have on your local! 6.7 Collaboration and conflict free workflows Up to now, we have been focused on using Git and GitHub for yourself, which is a great use. But equally powerful is to share a GitHib repository with other researchers so that you can work on code, analyses, and models together. When working together, you will need to pay careful attention to the state of the remote repository to avoid and handle merge conflicts. A merge conflict occurs when two collaborators make two separate commits that change the same lines of the same file. When this happens, git can’t merge the changes together automatically, and will give you back an error asking you to resolve the conflict. Don’t be afraid of merge conflicts, they are pretty easy to handle. and there are some great guides. That said, its truly painless if you can avoid merge conflicts in the first place. You can minimize conflicts by: Ensure that you pull down changes just before you commit Ensures that you have the most recent changes But you may have to fix your code if conflict would have occurred Coordinate with your collaborators on who is touching which files You still need to comunicate to collaborate 6.8 Exercise Use RStudio to add a new RMarkdown file to your training-repo repository, add some code, and then save it. Next, stage and commit the file locally, and push it up to GitHub. 6.9 Advanced topics There’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as: Using git on the command line Resolving conflicts Branching and merging Pull requests versus direct contributions for collaboration Using .gitignore to protect sensitive data GitHub Issues and why they are useful and much, much more 6.10 References Using RStudio: Happy Git and GitHub for the useR: http://happygitwithr.com/ R packages - Git and GitHub: http://r-pkgs.had.co.nz/git.html#git-init Mainly from the command line: Interactive git 101: https://try.github.io/ Very good tutorial about git: https://www.atlassian.com/git/tutorials/what-is-version-control Software Carpentry Version Control with Git Git tutorial geared towards scientists: http://nyuccl.org/pages/gittutorial/ Short intro to git basics: https://github.com/mbjones/gitbasics Git documentation about the basics: http://gitref.org/basic/ Git documentation - the basics: https://git-scm.com/book/en/v2/Getting-Started-Git-Basics Git terminology: https://www.atlassian.com/git/glossary/terminology In trouble, guide to know what to do: http://justinhileman.info/article/git-pretty/git-pretty.png Want to undo something? https://github.com/blog/2019-how-to-undo-almost-anything-with-git Git terminology: https://www.atlassian.com/git/glossary/terminology 8 tips to work better with git: https://about.gitlab.com/2015/02/19/8-tips-to-help-you-work-better-with-git/ GitPro book (2nd edition): https://git-scm.com/book/en/v2 GitHub Workflow GitHub: guides on how to use GitHub: https://guides.github.com/ GitHub from RStudio: http://r-pkgs.had.co.nz/git.html#git-pull Forking: https://help.github.com/articles/fork-a-repo/ https://guides.github.com/activities/forking/ Comparison of git repository host services: https://www.git-tower.com/blog/git-hosting-services-compared/ Branches interactive tutorial on branches: http://learngitbranching.js.org/ using git in a collaborative environment: https://www.atlassian.com/git/tutorials/comparing-workflows/centralized-workflow "],
["git-collaboration-and-conflict-management.html", "7 Git: Collaboration and Conflict Management 7.1 Learning Objectives 7.2 Collaborating with Git 7.3 Merge conflicts 7.4 How to resolve a conflict 7.5 Workflows to avoid merge conflicts 7.6 Other Collaborative Workflows with GitHub 7.7 Working with branches", " 7 Git: Collaboration and Conflict Management 7.1 Learning Objectives In this lesson, you will learn: How to use Git and GitHub to collaborate with colleagues on code What typically causes conflicts when collaborating Workflows to avoid conflicts How to resolve a conflict 7.2 Collaborating with Git Git is a great tool for working on your own, but even better for working with friends and colleagues. Git allows you to work with confidence on your own local copy of files with the confidence that you will be able to successfully synchronize your changes with the changes made by others. The simplest way to collaborate with Git is to use a shared repository on a hosting service such as GitHub, and use this shared repository as the mechanism to move changes from one collaborator to another. While there are other more advanced ways to sync git repositories, this “hub and spoke” model works really well due to its simplicity. 7.2.1 Activity: Collaborating with a trusted colleague Settings. Working in pairs, choose one person as the ‘Owner’ and one as the ‘Collaborator’. Then, have the Owner visit their training-repo repository created earlier,, and visit the Settings page, and select the Collaborators screen, and add the username of your Collaborator in the box. Once the collaborator has been added, they should check their email for an invitation from GitHub, and click on the acceptance link, which will enable them to collaborate on the repository. Collaborator clone. To be able to contribute to a repository, the collaborator must clone the repository from the Owner’s GitHub account. To do this, the Collaborator should visit the GitHub page for the Owner’s repository, and then copy the clone URL. In R Studio, the Collaborator will create a new project from version control by pasting this clone URL into the appropriate dialog (see the earlier chapter introducing GitHub). Collaborator Edits. With a clone copied locally, the Collaborator can now make changes to the index.Rmd file in the repository, adding a line or statement somewhere noticeable near the top. Save your changes. Collaborator commit and push. To sync changes, the collaborator will need to add, commit, and push their changes to the Owner’s repository. But before doing so, its good practice to pull immediately before committing to ensure you have the most recent changes from the owner. So, in R Studio’s Git tab, first click the “Diff” button to open the git window, and then press the green “Pull” down arrow button. This will fetch any recent changes from the origin repository and merge them. Next, add the changed index.Rmd file to be committed by cocking the check box next to it, type in a commit message, and click ‘Commit.’ Once that finishes, then the collaborator can immediately click ‘Push’ to send the commits to the Owner’s GitHub repository. Owner pull. Now, the owner can open their local working copy of the code in RStudio, and pull those changes down to their local copy. Congrats, the owner now has your changes! Owner edits, commit, and push. Next, the owner should do the same. Make changes to a file in the repository, save it, pull to make sure no new changes have been made while editing, and then add, commit, and push the Owner changes to GitHub. Collaborator pull. The collaborator can now pull down those owner changes, and all copies are once again fully synced. And you’re off to collaborating. 7.3 Merge conflicts A merge conflict occurs when both the owner and collaborator change the same lines in the same file without first pulling the changes that the other has made. This is most easily avoided by good communication about who is working on various sections of each file, and trying to avoid overlaps. But sometimes it happens, and git is there to warn you about potential problems. And git will not allow you to overwrite one person’s changes to a file with another’s changes to the same file if they were based on the same version. The main problem with merge conflicts is that, when the Owner and Collaborator both make changes to the same line of a file, git doesn’t know whose changes take precedence. You have to tell git whose changes to use for that line. 7.4 How to resolve a conflict 7.4.1 Abort, abort, abort… Sometimes you just made a mistake. When you get a merge conflict, the repository is placed in a ‘Merging’ state until you resolve it. There’s a command line command to abort doing the merge altogether: git merge --abort Of course, after doing that you still haven’t synced with your collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine. 7.4.2 Checkout The simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the command line git program to tell git to use either your changes (the person doing the merge), or their changes (the other collaborator). keep your collaborators file: git checkout --theirs conflicted_file.Rmd keep your own file: git checkout --ours conflicted_file.Rmd Once you have run that command, then run add, commit, and push the changes as normal. 7.4.3 Pull and edit the file But that requires the command line. If you want to resolve from RStudio, or if you want to pick and choose some of your changes and some of your collaborator’s, then instead you can manually edit and fix the file. When you pulled the file with a conflict, git notices that there is a conflict and modifies the file to show both your own changes and your collaborator’s changes in the file. It also shows the file in the Git tab with an orange U icon, which indicates that the file is Unmerged, and therefore awaiting you help to resolve the conflict. It delimits these blocks with a series of less than and greater than signs, so they are easy to find: To resolve the conflicts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your collaborators lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. Once you have made those changes, you simply add, commit, and push the files to resolve the conflict. 7.5 Workflows to avoid merge conflicts Communicate often. Tell each other what you are working on. Pull -&gt; Edit -&gt; Add -&gt; Pull -&gt; Commit -&gt; Push Pull before every commit, and commit often in small chunks. 7.6 Other Collaborative Workflows with GitHub In our exercise we experience a workflow that is suitable for a small team working together on a project. However if the frequency of the merge conflicts increase, it is the sign that you might want o use another type of workflow involving branches and pull requests. Branches in git enable you to create a separate and siloed version of the content of your repository. You can work on this branch / copy without disrupting the work of others. When you are done, you can ask to merge back your branches to the original version via a pull request. Sometimes, you want external people to the project to be able to contribute. However if you would like to control more what they are contributing, you can use this type of workflow where you will have to review and accept every changes that are submitted by this external collaborator. A fork is a copy of a repository that will be stored under your user account. Forking a repository allows you to freely experiment with changes without affecting the original project. We can create a fork on GitHub by clicking the “fork” button in the top right corner of our repository webpage. 7.6.1 Collaborating through Forking, aka the GitHub workflow Most commonly, forks are used to either propose changes to someone else’s project or to use someone else’s project as a starting point for your own idea. When you are satisfied with your work, you can initiate a Pull Request to initiate discussion about your modifications and requesting to integrate your changes to the main repository. Your commit history allows the original repository administrators to see exactly what changes would be merged if they accept your request. Do this by going to the original repository and clicking the “New pull request” button Next, click “compare across forks”, and use the dropdown menus to select your fork as the “head fork” and the original repository as the “base fork”. Then type a title and description for the changes you would like to make. By using GitHub’s @mention syntax in your Pull Request message, you can ask for feedback from specific people or teams. This workflow is recommended when you do not have push/write access to a repository, such as contributing to a open source software or R package, or if you are heavily changing a project. 7.6.2 Collaborating through write / push access When you collaborate closely and actively with colleagues, you do not want necessarily to have to review all their changes through pull requests. You can then give them write access (git push) to your repository to allow them to directly edit and contribute to its content. This is the workflow we will recommend to use within your working group. You still add collaborators with write access to the repository, like we did previously. The new part of the workflow is the use of branches 7.6.2.1 Branches adapted from https://www.atlassian.com/git/tutorials/git-merge What are branches? Well in fact nothing new, as the master is a branch. A branch represents an independent line of development, parallel to the master (branch). Why should you use branches? For 2 main reasons: We want the master to only keep a version of the code that is working We want to version the code we are developing to add/test new features (for now we mostly talk about feature branch) in our script without altering the version on the master. 7.7 Working with branches 7.7.1 Creating a new branch In RStudio, you can create a branch using the git tab. Click on the branch button Fill the branch name in the new branch window; in this example, we are going to use test for the name; leave the other options as default and click create you will be directly creating a local and remote branch and switch to it Congratulations you just created your first branch! Let us check on GitHub: As you can see, now there are two branches on our remote repository: - master - test 7.7.2 Using a branch Here there is nothing new. The workflow is exactly the same as we did before, excepts our commit will be created on the test branch instead of the master branch. "],
["linux-and-the-command-line.html", "8 Linux and the Command Line 8.1 Introduction to UNIX and its siblings 8.2 The Command Line Interface (CLI) 8.3 Navigating and managing files/directories in *NIX 8.4 General command syntax 8.5 Getting things done 8.6 Uploading Files 8.7 Advanced Topics: 8.8 Online resources", " 8 Linux and the Command Line 8.1 Introduction to UNIX and its siblings UNIX Originally developed at AT&amp;T Bell Labs circa 1970. Has experienced a long, multi-branched evolutionary path POSIX (Portable Operating System Interface) a set of specifications of what an OS needs to qualify as “a Unix”, to enhance interoperability among all the “Unix” variants 8.1.1 Various Unices The unix family tree OS X is a Unix! Linux is not fully POSIX-compliant, but certainly can be regarded as functionally Unix 8.1.2 Some Unix hallmarks Supports multi-users, multi-processes Highly modular: many small tools that do one thing well, and can be combined Culture of text files and streams Primary OS on HPC (High Performance Computing Systems) Main OS on which Internet was built 8.2 The Command Line Interface (CLI) The CLI provides a direct way to interact with the Operating System, by typing in commands. 8.2.1 Why the CLI is worth learning Typically much more extensive access to features, commands, options Command statements can be written down, saved (scripts!) Easier automation Much “cheaper” to do work on a remote system (no need to transmit all the graphical stuff over the network) 8.2.2 Connecting to a remote server via ssh From the gitbash (MS Windows) or the terminal (Mac) type: ssh aurora.nceas.ucsb.edu You will be prompted for your username and password. aurora_ssh You can also directly add your username: ssh brun@aurora.nceas.ucsb.edu In this case, you will be only asked for your password as you already specified which user you want to connect with. ** You can also use the terminal from RStudio!!** 8.3 Navigating and managing files/directories in *NIX pwd: Know where you are ls: List the content of the directory cd: Go inside a directory ~ : Home directory . : Here (current directory) ..: Up one level (upper directory) go to my “Home” directory: cd ~ go up one directory level: cd .. list the content: ls list the content showing hidden files: ls -a note that -a is referred as an option (modifies the command) More files/directories manipulations: mkdir: Create a directory cp: Copy a file mv: Move a file rm /rmdir: Remove a file / directory use those carefully, there is no return / Trash!! 8.3.1 Permissions File permissions 8.3.2 Exercise Navigate to the repo we created and inspect its content using the CLI Note: typing is not your thing? the &lt;tab&gt; key is your friend! One hit it will auto-complete the file/directory/path name for you. If there are many options, hit it twice to see the options. 8.4 General command syntax $ command [options] [arguments] where command must be an executable file on your PATH * echo $PATH and options can usually take two forms * short form: -a * long form: --all You can combine the options: ls -ltrh What do these options do? man ls 8.4.1 find Show me my Rmarkdown files! find . -iname &#39;*.Rmd&#39; Which files are larger than 1GB? find . -size +1G With more details about the files: find . -size +1G -ls 8.5 Getting things done 8.5.1 Some useful, special commands using the Control key Cancel (abort) a command: Ctrl-c Stop (suspend) a command: Ctrl-z Ctrl-z can be used to suspend, then background a process 8.5.2 Process management Like Windows Task Manager, OSX Activity Monitor top, ps, jobs (hit q to get out!) kill to delete an unwanted job or process Foreground and background: &amp; 8.5.3 What about “space” How much storage is available on this system? df -h How much storage am “I” using overall? du -hs &lt;folder&gt; How much storage am “I” using, by subdirectory? du -h &lt;folder&gt; 8.6 Uploading Files You have several options to upload files to the server. Some are more convenient if you have few files, like RStudio interface, some are more built for uploading a lot of files at one, like specific software… and you guessed it the CLI :) 8.6.1 RStudio You can only upload one file at the time (you can zip a folder to trick it): 8.6.2 sFTP Software An efficient protocol to upload files is FTP (File Transfer Protocol). The s stands for secured. Any software supporting those protocols will work to transfer files. We recommend the following free software: Mac users: cyberduck Windows: WinSCP 8.6.3 scp The scp command is another convenient way to transfer a single file or directory using the CLI. You can run it from Aurora or from your local computer. Here is the basic syntax: scp &lt;/source/path&gt; &lt;hostname:/path/to/destination/&gt; Here is an example of my uploading the file 10min-loop.R to Aurora from my laptop. The destination directory on Aurora is /home/brun/github_com/NCEAS/nceas-training/materials/files: scp 10min-loop.R brun@aurora.nceas.ucsb.edu:/home/brun/github_com/NCEAS/nceas-training/materials/files If you want to upload an entire folder, you can add the -r option to the command. The general syntax is: scp -r /path/to/source-folder user@server:/path/to/destination-folder/ Here is an example uploading all the images in the myplot folder scp -r myplots brun@aurora.nceas.ucsb.edu:/home/brun/github_com/NCEAS/nceas-training/materials/images 8.7 Advanced Topics: 8.7.1 Unix systems are multi-user Who else is logged into this machine? who Who is logged into “this shell”? whoami 8.7.2 A sampling of simple commands for dealing with files wc count lines, words, and/or characters diff compare two files for differences sort sort lines in a file uniq report or filter out repeated lines in a file 8.7.3 All files have permissions and ownership Change permissions: chmod Change ownership: chown List files showing ownership and permissions: ls -l schild@aurora:~/postdoc-training/data$ ls -l total 1136 -rw----r-- 1 schild scientist 1062050 May 29 2007 AT_85_to_89.csv -rwxrwxr-x 1 schild scientist 16200 Jun 26 11:20 env.csv -rwxr-xr-x 1 schild scientist 23358 Jun 26 11:20 locale.csv -rwxrwx--- 1 schild scientist 7543 Jun 26 11:20 refrens.csv -rwx------ 1 schild scientist 46653 Jun 26 11:20 sample.csv Clear contents in terminal window: clear 8.7.4 Getting help &lt;command&gt; -h, &lt;command&gt; --help man, info, apropos, whereis Search the web! 8.7.5 History See your command history: history Re-run last command: !! (pronounced “bang-bang”) Re-run 32th command: !32 Re-run 5th from last command: !-5 Re-run last command that started with ‘c’: !c 8.7.6 Get into the flow, with pipes stdin, stdout, stderr $ ls *.png | wc -l $ ls *.png | wc -l &gt; pngcount.txt $ diff &lt;(sort file1.txt) &lt;(sort file2.txt) $ ls foo 2&gt;/dev/null note use of * as character wildcard for zero or more matches (same in Mac and Windows); % is equivalent wildcard match in SQL queries ? matches single character; _ is SQL query equivalent 8.7.7 Text editing 8.7.7.1 Some editors vim emacs nano $ nano .bashrc 8.7.7.2 Let’s look at our text file cat print file(s) head print first few lines of file(s) tail print last few lines of file(s) less “pager” – view file interactively (type q to quit command)qqqbf od --t “octal dump” – to view file’s underlying binary/octal/hexadecimal/ASCII format $ shild@aurora:~/data$ head -3 env.csv EnvID,LocID,MinDate,MaxDate,AnnPPT,MAT,MaxAT,MinAT,WeatherS,Comments 1,*Loc ID,-888,-888,-888,-888,-888,-888,-888,-888 1,10101,-888,-888,-888,-888,-888,-888,-888,-888 $ shild@aurora:~/data$ head -3 env.csv | od -cx 0000000 E n v I D , L o c I D , M i n D 6e45 4976 2c44 6f4c 4963 2c44 694d 446e 0000020 a t e , M a x D a t e , A n n P 7461 2c65 614d 4478 7461 2c65 6e41 506e 0000040 P T , M A T , M a x A T , M i n 5450 4d2c 5441 4d2c 7861 5441 4d2c 6e69 0000060 A T , W e a t h e r S , C o m m 5441 572c 6165 6874 7265 2c53 6f43 6d6d 0000100 e n t s \\r \\n 1 , * L o c I D , 6e65 7374 0a0d 2c31 4c2a 636f 4920 2c44 0000120 - 8 8 8 , - 8 8 8 , - 8 8 8 , - 382d 3838 2d2c 3838 2c38 382d 3838 2d2c 0000140 8 8 8 , - 8 8 8 , - 8 8 8 , - 8 3838 2c38 382d 3838 2d2c 3838 2c38 382d 0000160 8 8 , - 8 8 8 \\r \\n 1 , 1 0 1 0 1 3838 2d2c 3838 0d38 310a 312c 3130 3130 0000200 , - 8 8 8 , - 8 8 8 , - 8 8 8 , 2d2c 3838 2c38 382d 3838 2d2c 3838 2c38 0000220 - 8 8 8 , - 8 8 8 , - 8 8 8 , - 382d 3838 2d2c 3838 2c38 382d 3838 2d2c 0000240 8 8 8 , - 8 8 8 \\r \\n 3838 2c38 382d 3838 0a0d od is especially useful in searching for hidden characters in your data watch for carriage return \\r and new line \\n\\ dos2unix and unix2dos 8.7.8 Create custom commands with “alias” alias lwc=’ls *.jpg | wc -l’ You can create a number of custom aliases that are available whenever you login, by putting commands such as the above in your shell start-up file, e.g. .bashrc 8.7.9 A sampling of more advanced utilities grep search files for text sed filter and transform text find advanced search for files/directories 8.7.9.1 grep Show all lines containing “bug” in my R scripts $ grep bug *.R Now count the number of occurrences per file $ grep -c bug *.R Print the names of files that contain bug $ grep -l bug *.R Print the lines of files that don’t contain bug $ grep -v bug *.R Print “hidden” dot-files in current directory $ ls -a | grep &#39;^\\.&#39; 8.7.9.2 sed Remove all lines containing “bug”! $ sed &#39;/bug/d&#39; myscript.R Call them buglets, not bugs! $ sed &#39;s/bug/buglet/g&#39; myscript.R Actually, only do this on lines starting with # $ sed &#39;/#/ s/bug/buglet/g&#39; myscript.R 8.8 Online resources Above are just a few of the most useful Linux &amp; Unix commands based on our experience. There are many more, and they comprise a rich set, that will serve you for years. They can be used in combination, and run from scripts. They can empower you when using high-end analytical servers, or doing repetitive tasks! http://linuxcommand.org/ http://www.linfo.org/command_line_lesson_1.html Free book!: http://linuxcommand.org/tlcl.php More about byobu: https://help.ubuntu.com/community/Byobu "],
["data-modeling-tidy-data.html", "9 Data Modeling &amp; Tidy Data 9.1 Learning Objectives 9.2 Benefits of relational data systems 9.3 Data Organization 9.4 Multiple tables 9.5 Inconsistent observations 9.6 Inconsistent variables 9.7 Marginal sums and statistics 9.8 Good enough data modeling 9.9 Primary and Foreign Keys 9.10 Entity-Relationship Model (ER) 9.11 Merging data 9.12 Simple Guidelines for Effective Data 9.13 Data modeling exercise 9.14 Related resources", " 9 Data Modeling &amp; Tidy Data 9.1 Learning Objectives Understand basics of relational data models aka tidy data Learn how to design and create effective data tables 9.2 Benefits of relational data systems Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates 9.3 Data Organization 9.4 Multiple tables 9.5 Inconsistent observations 9.6 Inconsistent variables 9.7 Marginal sums and statistics 9.8 Good enough data modeling 9.8.1 Denormalized data Observations about different entities combined In the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individuals of possibly different species found at that site. This is not normalized data. People often refer to this as wide format, because the observations are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we wold have to add new columns to the table. This is difficult to analyze, understand, and maintain. 9.8.2 Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observed entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Each column represents either an identifying variable or a measured variable Here’s an example of tidy (normalized) data in which the top table is the collection of observations about individuals of several species, and the bottom table are the observations containing properties of the sites at which the species occurred. 9.9 Primary and Foreign Keys When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) For example, in the second table below, the site column is the primary key of that table, because it uniquely identifies each row of the table as a unique observation of a site. Inthe first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku. 9.10 Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each site in the SITES table must have one or more observations in the PLOTOBS table, whereas each PLOTOBS has one and only one SITE. 9.11 Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of rows that have matches in both the left table and the right table: this is called an INNER JOIN. Other types of join are possible as well. A LEFT JOIN takes all of the rows from the left table, and merges on the data from matching rows in the right table. Keys that don’t match from the left table are still provided with a missing value (na) from the right table. A RIGHT JOIN is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a FULL OUTER JOIN includes all data from all rows in both tables, and includes missing values wherever necessary. Sometimes people represent these as Venn diagrams showing which parts of the left and right tables are included in the results for each join. These however, miss part of the story related to where the missing value come from in each result. In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 9.12 Simple Guidelines for Effective Data Design to add rows, not columns Each column one type Eliminate redundancy Uncorrected data file Header line Nonproprietary formats Descriptive names No spaces Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. 9.13 Data modeling exercise Break into groups, 1 per table To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. That site includes metadata describing the full data set, including column definitions. Here’s the first catch table: And here’s the region_defs table: Draw an ER model for the tables Indicate the primary and foreign keys Is the catch table in normal (aka tidy) form? If so, what single type of entity was observed? If not, how might you restructure the data table to make it tidy? Draw a new ER diatram showing this re-designed data structure 9.14 Related resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. White et al. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. Software Carpentry SQL tutorial Tidy Data "],
["data-cleaning-and-manipulation.html", "10 Data Cleaning and Manipulation 10.1 Learning Objectives 10.2 Introduction 10.3 Setup 10.4 About the pipe (%&gt;%) operator 10.5 Selecting/removing columns: select() 10.6 Quality Check 10.7 Changing column content: mutate() 10.8 Changing shape: pivot_longer() and pivot_wider() 10.9 Renaming columns with rename() 10.10 Adding columns: mutate() 10.11 group_by and summarise 10.12 Filtering rows: filter() 10.13 Sorting your data: arrange() 10.14 Joins in dplyr 10.15 separate() and unite() 10.16 Summary", " 10 Data Cleaning and Manipulation 10.1 Learning Objectives In this lesson, you will learn: What the Split-Apply-Combine strategy is and how it applies to data The difference between wide vs. tall table formats and how to convert between them How to use dplyr and tidyr to clean and manipulate data for analysis How to join multiple data.frames together using dplyr 10.2 Introduction The data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst. Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps: Split: Split the data into logical groups (e.g., area, stock, year) Apply: Calculate some summary statistic on each group (e.g. mean total length by year) Combine: Combine the groups back together into a single table Figure 1: diagram of the split apply combine strategy As shown above (Figure 1), our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table. dplyr provides a fast and powerful way to express this. Let’s look at a simple example of how this is done: Assuming our length data is already loaded in a data.frame called length_data: year length_cm 1991 5.673318 1991 3.081224 1991 4.592696 1992 4.381523 1992 5.597777 1992 4.900052 1992 4.139282 1992 5.422823 1992 5.905247 1992 5.098922 We can do this calculation using dplyr like this: length_data %&gt;% group_by(year) %&gt;% summarise(mean_length_cm = mean(length_cm)) Another exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format: site 1990 1991 … 1993 gold 100 118 … 112 lake 100 118 … 112 … … … … … dredge 100 118 … 112 You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run: lm(length ~ year) But this won’t work on our wide data because lm needs length and year to be columns in our table. Or how would we make a separate plot for each year? We could call plot one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset. The tidyr package allows us to quickly switch between wide format and what is called tall format using the pivot_longer function: site_data %&gt;% pivot_longer(-site, names_to = &quot;year&quot;, values_to = &quot;length&quot;) site year length gold 1990 101 lake 1990 104 dredge 1990 144 … … … dredge 1993 145 In this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages: dplyr mutate() group_by() summarise() select() filter() arrange() left_join() rename() tidyr pivot_longer() pivot_wider() extract() separate() 10.3 Setup Let’s start going over the most common functions you’ll use from the dplyr package. To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. First, let’s load dplyr and tidyr: library(dplyr) library(tidyr) Then let’s read in the data and take a look at it: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) head(catch_original) ## Region Year Chinook Sockeye Coho Pink Chum All notesRegCode ## 1 SSE 1886 0 5 0 0 0 5 ## 2 SSE 1887 0 155 0 0 0 155 ## 3 SSE 1888 0 224 16 0 0 240 ## 4 SSE 1889 0 182 11 92 0 285 ## 5 SSE 1890 0 251 42 0 0 292 ## 6 SSE 1891 0 274 24 0 0 298 Note: I copied the URL from the Download button on https://knb.ecoinformatics.org/#view/df35b.304.2 This dataset is relatively clean and easy to interpret as-is. But while it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first. 10.4 About the pipe (%&gt;%) operator Before we jump into learning tidyr and dplyr, we first need to explain the %&gt;%. Both the tidyr and the dplyr packages use the pipe operator - %&gt;%, which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement. Say you want to both filter out rows of a dataset, and select certain columns. Instead of writing df_filtered &lt;- filter(df, ...) df_selected &lt;- select(df_filtered, ...) You can write df_cleaned &lt;- df %&gt;% filter(...) %&gt;% select(...) If you think of the assignment operator (&lt;-) as reading like “gets”, then the pipe operator would read like “then.” So you might think of the above chunk being translated as: The cleaned dataframe gets the original data, and then a filter (of the original data), and then a select (of the filtered data). The benefits to using pipes are that you don’t have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it. RStudio has a keyboard shortcut for %&gt;% : Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). 10.5 Selecting/removing columns: select() The first issue is the extra columns All and notesRegCode. Let’s select only the columns we want, and assign this to a variable called catch_data. catch_data &lt;- catch_original %&gt;% select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum) head(catch_data) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Much better! select also allows you to say which columns you don’t want, by passing unquoted column names preceded by minus (-) signs: catch_data &lt;- catch_original %&gt;% select(-All, -notesRegCode) head(catch_data) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 10.6 Quality Check Now that we have the data we are interested in using, we should do a little quality check to see that it seems as expected. One nice way of doing this is the summary function. summary(catch_data) ## Region Year Chinook Sockeye ## Length:1708 Min. :1878 Length:1708 Min. : 0.00 ## Class :character 1st Qu.:1922 Class :character 1st Qu.: 6.75 ## Mode :character Median :1947 Mode :character Median : 330.50 ## Mean :1946 Mean : 1401.09 ## 3rd Qu.:1972 3rd Qu.: 995.50 ## Max. :1997 Max. :44269.00 ## Coho Pink Chum ## Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 ## Median : 41.5 Median : 34.5 Median : 63.0 ## Mean : 150.4 Mean : 2357.8 Mean : 422.0 ## 3rd Qu.: 175.0 3rd Qu.: 1622.5 3rd Qu.: 507.5 ## Max. :3220.0 Max. :53676.0 Max. :10459.0 Notice something seems a bit off? The Chinook catch data are character class. Let’s fix it using the function mutate before moving on. 10.7 Changing column content: mutate() We can use the mutate function to change a column, or to create a new column. First Let’s try to just convert the Chinook catch values to numeric type using the as.numeric() function, and overwrite the old Chinook column. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = as.numeric(Chinook)) ## Warning: NAs introduced by coercion head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 We get a warning “NAs introduced by coercion” which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning datasets and it’s important to have the skills to deal with it when it crops up. To investigate, let’s isolate the issue. We can find out which values are NAs with a combination of is.na() and which(), and save that to a variable called i. i &lt;- which(is.na(catch_clean$Chinook)) i ## [1] 401 It looks like there is only one problem row, lets have a look at it in the original data. catch_data[i,] ## Region Year Chinook Sockeye Coho Pink Chum ## 401 GSE 1955 I 66 0 0 1 Well that’s odd: The value in catch_thousands is I. It turns out that this dataset is from a PDF which was automatically converted into a CSV and this value of I is actually a 1. Let’s fix it by incorporating the ifelse function to our mutate call, which will change the value of the Chinook column to 1 if the value is equal to I, otherwise it will leave the column as the same value. catch_clean &lt;- catch_data %&gt;% mutate(Chinook = ifelse(Chinook == &quot;I&quot;, 1, Chinook)) %&gt;% mutate(Chinook = as.integer(Chinook)) head(catch_clean) ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 10.8 Changing shape: pivot_longer() and pivot_wider() The next issue is that the data are in a wide format and, we want the data in a tall format instead. pivot_longer() from the tidyr package helps us do just this conversion: catch_long &lt;- catch_clean %&gt;% pivot_longer(cols = -c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) head(catch_long) ## # A tibble: 6 x 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 The syntax we used above for pivot_longer() might be a bit confusing so let’s walk though it. The first argument to pivot_longer is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or the names of the columns you are not pivoting over. The names_to argument takes the name of the column that you are creating from the column names you are pivoting over. The values_to argument takes the name of the column that you are creating from the values in the columns you are pivoting over. The opposite of pivot_longer(), pivot_wider(), works in a similar declarative fashion: catch_wide &lt;- catch_long %&gt;% pivot_wider(names_from = species, values_from = catch) head(catch_wide) ## # A tibble: 6 x 7 ## Region Year Chinook Sockeye Coho Pink Chum ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 10.9 Renaming columns with rename() If you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish so let’s convert it before moving on. Let’s first rename the catch column to be called catch_thousands: catch_long &lt;- catch_long %&gt;% rename(catch_thousands = catch) head(catch_long) ## # A tibble: 6 x 4 ## Region Year species catch_thousands ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 10.10 Adding columns: mutate() Now let’s use mutate again to create a new column called catch with units of fish (instead of thousands of fish). catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) head(catch_long) Now let’s remove the catch_thousands column for now since we don’t need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the catch_long data.frame. catch_long &lt;- catch_long %&gt;% mutate(catch = catch_thousands * 1000) %&gt;% select(-catch_thousands) head(catch_long) ## # A tibble: 6 x 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 We’re now ready to start analyzing the data. 10.11 group_by and summarise As I outlined in the Introduction, dplyr lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the group_by() and summarise() functions: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(catch_mean = mean(catch)) head(mean_region) ## # A tibble: 6 x 2 ## Region catch_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 ALU 40384. ## 2 BER 16373. ## 3 BRB 2709796. ## 4 CHG 315487. ## 5 CKI 683571. ## 6 COP 179223. Exercise: Find another grouping and statistic to calculate for each group. Exercise: Find out if you can group by multiple variables. Another common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n(). n_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarize(n = n()) head(n_region) ## # A tibble: 6 x 2 ## Region n ## &lt;chr&gt; &lt;int&gt; ## 1 ALU 435 ## 2 BER 510 ## 3 BRB 570 ## 4 CHG 550 ## 5 CKI 525 ## 6 COP 470 10.12 Filtering rows: filter() filter() is the verb we use to filter our data.frame to rows matching some condition. It’s similar to subset() from base R. Let’s go back to our original data.frame and do some filter()ing: SSE_catch &lt;- catch_long %&gt;% filter(Region == &quot;SSE&quot;) head(SSE_catch) ## # A tibble: 6 x 4 ## Region Year species catch ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 1886 Chinook 0 ## 2 SSE 1886 Sockeye 5000 ## 3 SSE 1886 Coho 0 ## 4 SSE 1886 Pink 0 ## 5 SSE 1886 Chum 0 ## 6 SSE 1887 Chinook 0 Exercise: Filter to just catches of over one million fish. Exercise: Filter to just SSE Chinook 10.13 Sorting your data: arrange() arrange() is how we sort the rows of a data.frame. In my experience, I use arrange() in two common cases: When I want to calculate a cumulative sum (with cumsum()) so row order matters When I want to display a table (like in an .Rmd document) in sorted order Let’s re-calculate mean catch by region, and then arrange() the output by mean catch: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(mean_catch) head(mean_region) ## # A tibble: 6 x 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 BER 16373. ## 2 KTZ 18836. ## 3 ALU 40384. ## 4 NRS 51503. ## 5 KSK 67642. ## 6 YUK 68646. The default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function: mean_region &lt;- catch_long %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(desc(mean_catch)) head(mean_region) ## # A tibble: 6 x 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 3184661. ## 2 BRB 2709796. ## 3 NSE 1825021. ## 4 KOD 1528350 ## 5 PWS 1419237. ## 6 SOP 1110942. 10.14 Joins in dplyr So now that we’re awesome at manipulating a single data.frame, where do we go from here? Manipulating more than one data.frame. If you’ve ever used a database, you may have heard of or used what’s called a “join”, which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two. We’ve already covered joins in Data Modeling &amp; Tidy Data so let’s see how it’s done with dplyr. The dataset we’re working with, https://knb.ecoinformatics.org/#view/df35b.304.2, contains a second CSV which has the definition of each Region code. This is a really common way of storing auxiliary information about our dataset of interest (catch) but, for analylitcal purposes, we often want them in the same data.frame. Joins let us do that easily. Let’s look at a preview of what our join will do by looking at a simplified version of our data: Visualisation of our left_join First, let’s read in the region definitions data table and select only the columns we want. Note that I have piped my read.csv result into a select call, creating a tidy chunk that reads and selects the data that we need. region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) %&gt;% select(code, mgmtArea) head(region_defs) ## code mgmtArea ## 1 GSE Unallocated Southeast Alaska ## 2 NSE Northern Southeast Alaska ## 3 SSE Southern Southeast Alaska ## 4 YAK Yakutat ## 5 PWSmgmt Prince William Sound Management Area ## 6 BER Bering River Subarea Copper River Subarea If you examine the region_defs data.frame, you’ll see that the column names don’t exactly match the image above. If the names of the key columns are not the same, you can explicitly specify which are the key columns in the left and right side as shown below: catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(catch_joined) ## # A tibble: 6 x 5 ## Region Year species catch mgmtArea ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 SSE 1886 Chinook 0 Southern Southeast Alaska ## 2 SSE 1886 Sockeye 5000 Southern Southeast Alaska ## 3 SSE 1886 Coho 0 Southern Southeast Alaska ## 4 SSE 1886 Pink 0 Southern Southeast Alaska ## 5 SSE 1886 Chum 0 Southern Southeast Alaska ## 6 SSE 1887 Chinook 0 Southern Southeast Alaska Notice that I have deviated from our usual pipe syntax (although it does work here!) because I prefer to see the data.frames that I am joining side by side in the syntax. Another way you can do this join is to use rename to change the column name code to Region in the region_defs data.frame, and run the left_join this way: region_defs &lt;- region_defs %&gt;% rename(Region = code, Region_Name = mgmtArea) catch_joined &lt;- left_join(catch_long, region_defs, by = c(&quot;Region&quot;)) head(catch_joined) Now our catches have the auxiliary information from the region definitions file alongside them. Note: dplyr provides a complete set of joins: inner, left, right, full, semi, anti, not just left_join. 10.15 separate() and unite() separate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single). This can come in really handle when we need to split a column into two pieces by a consistent separator (like a dash). Let’s make a new data.frame with fake data to illustrate this. Here we have a set of site identification codes. with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information. sites_df &lt;- data.frame(site = c(&quot;HAW-101&quot;, &quot;HAW-103&quot;, &quot;OAH-320&quot;, &quot;OAH-219&quot;, &quot;MAI-039&quot;), stringsAsFactors = FALSE) sites_df %&gt;% separate(site, c(&quot;island&quot;, &quot;site_number&quot;), &quot;-&quot;) ## island site_number ## 1 HAW 101 ## 2 HAW 103 ## 3 OAH 320 ## 4 OAH 219 ## 5 MAI 039 Exercise: Split the city column in the following data.frame into city and state_code columns: cities_df &lt;- data.frame(city = c(&quot;Juneau AK&quot;, &quot;Sitka AK&quot;, &quot;Anchorage AK&quot;), stringsAsFactors = FALSE) # Write your solution here unite() does just the reverse of separate(). If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column. dates_df &lt;- data.frame(year = c(&quot;1930&quot;, &quot;1930&quot;, &quot;1930&quot;), month = c(&quot;12&quot;, &quot;12&quot;, &quot;12&quot;), day = c(&quot;14&quot;, &quot;15&quot;, &quot;16&quot;), stringsAsFactors = FALSE) dates_df %&gt;% unite(date, year, month, day, sep = &quot;-&quot;) ## date ## 1 1930-12-14 ## 2 1930-12-15 ## 3 1930-12-16 Exercise: Use unite() on your solution above to combine the cities_df back to its original form with just one column, city: # Write your solution here 10.16 Summary We just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now: catch_original &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) %&gt;% select(code, mgmtArea) mean_region &lt;- catch_original %&gt;% select(-All, -notesRegCode) %&gt;% mutate(Chinook = ifelse(Chinook == &quot;I&quot;, 1, Chinook)) %&gt;% mutate(Chinook = as.numeric(Chinook)) %&gt;% pivot_longer(-c(Region, Year), names_to = &quot;species&quot;, values_to = &quot;catch&quot;) %&gt;% mutate(catch = catch*1000) %&gt;% group_by(Region) %&gt;% summarize(mean_catch = mean(catch)) %&gt;% left_join(region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) head(mean_region) ## # A tibble: 6 x 3 ## Region mean_catch mgmtArea ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ALU 40384. Aleutian Islands Subarea ## 2 BER 16373. Bering River Subarea Copper River Subarea ## 3 BRB 2709796. Bristol Bay Management Area ## 4 CHG 315487. Chignik Management Area ## 5 CKI 683571. Cook Inlet Management Area ## 6 COP 179223. Copper River Subarea "],
["collaboration-designing-and-facilitating-effective-meetings.html", "11 Collaboration: Designing and facilitating effective meetings 11.1 Learning Objectives 11.2 What makes meetings effective? 11.3 Designing for successful team science meetings 11.4 Sharing the load of effective meeting facilitation 11.5 Creating the conditions for quality participation 11.6 Techniques for democratizing participation 11.7 The Groan Zone (aka the Diamond Participation Model) 11.8 Techniques to support divergent and emergent thinking 11.9 Making thinking visible 11.10 Techniques to support convergent thinking (i.e., getting to agreement) 11.11 Virtual meeting best practices 11.12 Resources", " 11 Collaboration: Designing and facilitating effective meetings 11.1 Learning Objectives In this lesson, you will learn: How to design creative, productive, inclusive team science meetings How to facilitate more effective in person team science meetings How to facilitate more effective virtual team science meetings 11.2 What makes meetings effective? As humans, we are constantly coming together in groups for some common purpose. At our core, we are social animals, and our exceptional abilities to connect and collaborate may be among the main things that distinguish us as a species. So if meetings are just opportunities for groups of humans to connect and collaborate, why do so many of us find them “soul-sucking and painful?” Probably we are going about this the wrong way! In general… Effective meetings have a clear purpose / goal. They have clearly defined objectives (intended outcomes and/or outputs). The right people have been convened. Those people understand the purpose and objectives of the meeting. They understand their role, and they are ready to engage in the way that will best serve the collective purpose. The agenda for the meeting has been designed with the outcomes and outputs in mind. The format, facilities, and activities suit the agenda and purpose of the meeting. The format, facilities, and activities encourage and empower people to contribute fully and openly. People listen, participate, and take responsibility for leadership in ways that support other participants, the process, and progress. Actions and next steps are clearly described (including roles, responsibilities, timelines, definition of success) and taken up by participants. Progress (during and after the meeting) is reviewed, tracked, and communicated. Everyone is invited to review progress / success and feed learning back in to improve future meetings and ongoing work of the team. 11.3 Designing for successful team science meetings What are we trying to achieve when we design and facilitate a team science meeting? First, we are working to build and sustain a coalition of the willing. Second, we want to foster creativity. When faced with complex problems without obvious solutions, we need to invite new ideas from all quarters and create the conditions where innovation and unconventional thinking are welcomed. We cannot know ahead of time where good ideas will come from. Third, we want results, so we need to foster a really productive environment. The key to that productivity is good communication. And the style of communication that is most effective for interdisciplinary problem-solving is dialogue (versus debate). Dialogue will be covered in more depth down below. Finally, in most team science endeavors, we can and should also be designing for future collaborative potential. These people are part of your professional network and could become lifelong collaborators. How do you build and sustain a pool of collaborators who can become part of your future coalitions of the willing for new projects? The value of planning To design a successful meeting: Give yourself enough time - at least 2x the meeting length Determine who needs to be involved in planning Make sure all the key aspects of meeting planning are being handled by someone. Use your team and share the load! Purpose and objectives - Getting these clear is the most important thing you can do to plan a good meeting! Participant list Scheduling Venue, accommodations (if applicable) and logistics A/V and remote participation technology Invitations Agenda design Process design Assigning pre-work - What can be accomplished ahead of the meeting to prepare for success? Facilitation team - Who will lead the meeting? Who will facilitate? Who will take notes? Who can troubleshoot technology? Recreation (field trips, opportunities for fun) Food and beverages (group meals, catered working lunches, snacks) Materials Pre- and post-meeting communication with participants 11.4 Sharing the load of effective meeting facilitation Shared leadership is a critical element of creating a culture of collaboration. The agenda design should articulate specific roles and who is going to play them, which may include: Meeting chair – keeps an eye on the overall vision and progress of the meeting; this role may be shared among the working group leaders Discussion leader(s) – each session Process facilitator – sets the tone and pace, mediates conflicts, and ensures all voices are being heard, interpersonal dynamics are positive/effective, and group is staying on task Timekeeper – may also be the chair or facilitator Notetaker – captures action items and notes, often in a google doc that can be viewed and added to by others; this role should be rotated and shared evenly among genders and career stage; could also produce a meeting summary Scribe – captures important points that can be seen in real time by the whole group, usually on a whiteboard or flipchart Spotter – keeps a running list of who is waiting to speak (especially in large groups/intense discussions) Relationship monitor – tracks group dynamics and actively works to help everyone feel included and engaged on personal and social levels Participation monitor – engineers opportunities for participation, quells interrupters, amplifies and credits the messages of quieter participants. As you get to know your team members, you can start to match people to these different roles based on their skills and recruit them to help. 11.5 Creating the conditions for quality participation Collaborative working groups convene diverse collections of participants not otherwise collaborating to catalyze novel insights and solutions. One primary method of catalyzing novel ideas is to allow the flow of dialogue during a working group meeting. In contrast to debate or discussion, dialogue allows groups to recognize the limits on their own and others’ individual perspectives and to strive for more coherent thought. This process can take working groups in directions not imagined or planned. In discussion, different views are presented and defended, and this may provide a useful analysis of the whole situation. In dialogue, all views are treated as equally valid and different views are presented as a means toward discovering a new view. In a skillful discussion, decisions are made. In a dialogue, complex issues are explored. Both are critical to the working group process, and the more artfully a group can move between these two forms of discourse (and out of less productive debate and polite discussion) according to what is needed, the more effective the group will be. Adapted from the work of Peter Senge Agreements or principles can help enable dialogue. (See list of potential agreements in the resources section). Opportunities to build social cohesion and human connection also support dialogue. These could take the form of icebreakers or other exercises that invite people to connect on a personal level. Or they could be informal activities like coffee breaks, field trips, games or group dinners that offer opportunities for people to get to know each other and build connections. 11.6 Techniques for democratizing participation Remember that your goal is to enable the full participation of all group members so you can tap their diverse perspectives and catalyze creative problem-solving. To do this, you want to thoughtfully work to democratize participation so that a few voices don’t dominate discussion. A few simple techniques can help: Use tent cards to indicate when one wants to speak and have someone track the order they go up and call on people in order Allow time for silent reflection before inviting discussion Mix up the discussion format throughout the meeting - pairs, small groups, plenary Provide opportunities for small groups to go off and work on something and then report refined ideas back to the group Invite, amplify and credit the contributions of quieter participants 11.7 The Groan Zone (aka the Diamond Participation Model) 11.8 Techniques to support divergent and emergent thinking Be creative and empathetic when you design your agenda. Think about your participants and what is going to help all of them participate fully and creatively. Here are a few techniques and microstructures we have found useful. Round robins, e.g. to get starting positions out on the table and hear from everyone 1,2,4,all to allow everyone’s participation and elevate themes and key insights (format goes from individual to small group to whole group discussion) Sticky note brainstorming + clustering Rotating stations World Cafe conversations Panel discussion or User Experience Fishbowl to explicitly draw out and contrast different expert perspectives Parallel breakout groups Scribing to capture participant’s viewpoints (use their own words) Encouraging and drawing out people, mirroring and validating what they say See other ideas in the Creative Problem Solving (CPS): Divergent Tools Quick Reference from Omni Skills linked below in Resources 11.9 Making thinking visible Remember that there are many different ways that people learn. Encourage group members to synthesize and feed back the information that is being discussed in different ways to enhance understanding and learning. Visual tools - including written notes and graphics or drawings - can be a helpful complement to oral discussion. Consider using: Shared notes, e.g. in google docs Scribing on a flipchart or whiteboard Grids to organize information and compare Conceptual models to articulate shared understanding of complex systems Manifestos, abstracts and other written collateral to distill ideas 11.10 Techniques to support convergent thinking (i.e., getting to agreement) Coming to a clear decision can sometimes be the hardest work of a collaboration. Use tools and frameworks to help your group converge in its thinking and come to clear decisions. Gradient of agreement Kaner Gradient of Agreement-Adapted (TRG, 2017) and (Hughes, 2017) Feasibility / impact matrix Top five Dot voting Gut check See other ideas in the Creative Problem Solving (CPS): Convergent Tools Quick Reference from Omni Skills linked below in Resources 11.11 Virtual meeting best practices Running a virtual meeting is much like an in person meeting. Use all the tools and planning guidelines above to design an effective virtual meeting. To get the most out of virtual collaboration (list adapted from Young 2009, facilitate.com): Each virtual meeting should have the same level of planning as an in person meeting, i.e., twice the planning time as duration Match the virtual technology to efficiently accomplish each objective Test your technology with participants in advance Prepare participants by circulating an agenda and pre-work in advance and hold them accountable for coming prepared Establish mutual agreement on virtual conferencing best practices, e.g., using a headset, video, muting when not talking, and remind participants of them at the outset To maximize engagement, actively request participants to remove distractions, take the pulse of the group frequently, create an interactive format, and stick to meeting start and end times Use facilitation to build trust and social capital Keep shared goals in sight, track progress, check in frequently, and celebrate achievements See also the guidance in Hampton et al. 2017 in the Resource list below. 11.12 Resources Bohm, David. 2004. On Dialogue. Routledge Classics Hampton, Stephanie et al. 2017. Best Practices for Virtual Participation in Meetings: Experiences from Synthesis Centers.Bulletin of the Ecological Society of America IDEO Design Kit Kaner, Sam. 2014. Facilitator’s Guide to Participatory Decision-Making. Jossey-Bass Kappel, Carrie. 2019. From groan zone to growth zone. Integration and Implementation Insights Liberating Structures Omni Skills. Creative Problem Solving (CPS): Convergent Tools Quick Reference Omni Skills. Creative Problem Solving (CPS): Divergent Tools Quick Reference Wilkie, David. 2019. The secret sauce for environmental problem-solving.Scientific American "],
["publication-graphics.html", "12 Publication Graphics 12.1 Learning Objectives 12.2 Overview 12.3 Setup 12.4 Static figures using ggplot2 12.5 Interactive visualization using leaflet and DT 12.6 Resources", " 12 Publication Graphics 12.1 Learning Objectives In this lesson, you will learn: The basics of the ggplot2 package to create static plots How to use ggplot2’s theming abilities to create publication-grade graphics The basics of the leaflet package to create interactive maps 12.2 Overview ggplot2 is a popular package for visualizing data in R. From the home page: ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. It’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). This lesson will introduce you to the basic components of working with ggplot2. 12.2.1 ggplot vs base vs lattice vs XYZ… R provides many ways to get your data into a plot. Three common ones are, “base graphics” (plot(), hist(), etc`) lattice ggplot2 All of them work! I use base graphics for simple, quick and dirty plots. I use ggplot2 for most everything else. ggplot2 excels at making complicated plots easy and easy plots simple enough. 12.3 Setup First, let’s load the packages we’ll need: library(leaflet) library(dplyr) library(tidyr) library(ggplot2) library(DT) library(scales) # install.packages(&quot;scales&quot;) library(patchwork) # install.packages(&quot;patchwork&quot;) 12.3.1 Load salmon escapement data You can load the data table directly from the KNB Data Repository, if it isn’t already present on your local computer. This technique only downloads the file if you need it. data_url &lt;- &quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e&quot; esc &lt;- tryCatch( read.csv(&quot;data/escapement.csv&quot;, stringsAsFactors = FALSE), error=function(cond) { message(paste(&quot;Escapement file does not seem to exist, so get it from the KNB.&quot;)) esc &lt;- read.csv(url(data_url, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) return(esc) } ) head(esc) Now that we have the data loaded, let’s calculate annual escapement by species and region: annual_esc &lt;- esc %&gt;% separate(sampleDate, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;), sep = &quot;-&quot;) %&gt;% mutate(Year = as.numeric(Year)) %&gt;% group_by(Species, SASAP.Region, Year) %&gt;% summarize(escapement = sum(DailyCount)) %&gt;% filter(Species %in% c(&quot;Chinook&quot;, &quot;Sockeye&quot;, &quot;Chum&quot;, &quot;Coho&quot;, &quot;Pink&quot;)) head(annual_esc) ## # A tibble: 6 x 4 ## # Groups: Species, SASAP.Region [1] ## Species SASAP.Region Year escapement ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Chinook Alaska Peninsula and Aleutian Islands 1974 1092 ## 2 Chinook Alaska Peninsula and Aleutian Islands 1975 1917 ## 3 Chinook Alaska Peninsula and Aleutian Islands 1976 3045 ## 4 Chinook Alaska Peninsula and Aleutian Islands 1977 4844 ## 5 Chinook Alaska Peninsula and Aleutian Islands 1978 3901 ## 6 Chinook Alaska Peninsula and Aleutian Islands 1979 10463 That command used a lot of the dplyr commands that we’ve used, and some that are new. The separate function is used to divide the sampleDate column up into Year, Month, and Day columns, and then we use group_by to indicate that we want to calculate our results for the unique combinations of species, region, and year. We next use summarize to calculate an escapement value for each of these groups. Finally, we use a filter and the %in% operator to select only the salmon species. 12.4 Static figures using ggplot2 Every graphic you make in ggplot2 will have at least one aesthetic and at least one geom (layer). The aesthetic maps your data to your geometry (layer). Your geometry specifies the type of plot we’re making (point, bar, etc.). Now, let’s plot our results using ggplot. ggplot uses a mapping aesthetic (set using aes()) and a geometry to create your plot. Additional geometries/aesthetics and theme elements can be added to a ggplot object using +. ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col() What if we want our bars to be blue instad of gray? You might think we could run this: ggplot(annual_esc, aes(x = Species, y = escapement, fill = &quot;blue&quot;)) + geom_col() Why did that happen? Notice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word “blue” in our dataframe, and then mapped it to the fill aesthetic, which then chose the default fill color of red. What we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the geom_bar function, outside of the mapping aesthetics function call. ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col(fill = &quot;blue&quot;) What if we did want to map the color of the bars to a variable, such as region. ggplot is really powerful because we can easily get this plot to visualize more aspects of our data. ggplot(annual_esc, aes(x = Species, y = escapement, fill = SASAP.Region)) + geom_col() Just like in dplyr and tidyr, we can also pipe a data.frame directly into the first argument of the ggplot function using the %&gt;% operator. Let’s look at an example using a different geometry. Here, we use the pipe operator to pass in a filtered version of annual_esc, and make a line plot with points at each observation. annual_esc %&gt;% filter(SASAP.Region == &quot;Kodiak&quot;) %&gt;% ggplot(aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() This can certainly be convenient, especially for cases like the above, but use it carefully! Combining too many data-tidying or subsetting operations with your ggplot call can make your code more difficult to debug and understand. 12.4.1 Setting ggplot themes Now let’s work on making this plot look a bit nicer. Add a title using ggtitle(), adjust labels using ylab(), and include a built in theme using theme_bw(). There are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete theme_ &lt;TAB&gt; to view a list of theme functions. For clarity in the next section, I’ll save the filtered version of the annual escapement data.frame to it’s own object. kodiak_esc &lt;- annual_esc %&gt;% filter(SASAP.Region == &quot;Kodiak&quot;) ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + theme_bw() You can see that the theme_bw() function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. The built in theme functions change the default settings for many elements that can also be changed invididually using thetheme() function. The theme() function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at ?theme). Luckily there are many great ggplot resources online so we don’t have to remember all of these, just google “ggplot cheatsheet” and find one you like. Let’s look at an example of a theme call, where we change the position of our plot above from the right side to the bottom, and remove the title from the legend. ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) Note that the theme() call needs to come after any built in themes like theme_bw() are used. Otherwise, theme_bw() will likely override any theme elements that you changed using theme(). You can also save the result of a series of theme() function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again! my_theme &lt;- theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + my_theme 12.4.2 Smarter tick labels using scales Fixing tick labels in ggplot can be really hard! The y-axis labels in the plot above don’t look great. We could manually fix them, but it would likely be tedious and error prone. The scales package provides some nice helper functions to easily rescale and relabel your plots. Here, we use scale_y_continuous from ggplot2, with the argument labels, which is assigned to the function name comma, from the scales package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers. ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + scale_y_continuous(labels = comma) + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + my_theme 12.4.3 Creating multiple plots What if we wanted to generate a plot for every region? A fast way to do this uses the function facet_wrap(). This function takes a mapping to a variable using the syntax ~variable_name. The ~ (tilde) is a model operator which tells facet_wrap to model each unique value within variable_name to a facet in the plot. The default behaviour of facet wrap is to put all facets on the same x and y scale. You can use the scales argument to specify whether to allow different scales between facet plots. Here, we free the y scale. You can also specify the number of columns using the n_col argument. ggplot(annual_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + scale_y_continuous(labels = comma) + facet_wrap(~SASAP.Region, scales = &quot;free_y&quot;, ncol = 2) + ylab(&quot;Escapement&quot;) + my_theme 12.4.4 Several plots, one figure Sometimes you want to combine separate ggplots into the same Figure. The patchwork package helps you to do this in a very easy manner: # Plot 1 p1 &lt;- ggplot(annual_esc, aes(x = Species, y = escapement)) + geom_col() + my_theme # Plo2 p2 &lt;- ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + geom_line() + geom_point() + ylab(&quot;Escapement&quot;) + ggtitle(&quot;Kodiak Salmon Escapement&quot;) + my_theme # Put the plots to gether in one figure p1 + p2 12.5 Interactive visualization using leaflet and DT 12.5.1 Tables Now that we know how to make great static visualizations, lets introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub pages, so at the end of this lesson we will publish our figures to the website created earlier in the week during this lesson. First let’s show an interactive table of unique sampling locations using DT. Write a data.frame containing unique sampling locations with no missing values using two new functions from dplyr and tidyr: distinct() and drop_na(). locations &lt;- esc %&gt;% distinct(Location, Latitude, Longitude) %&gt;% drop_na() And display it as an interactive table using datatable() from the DT package. datatable(locations) 12.5.2 Maps Similar to ggplot2, you can make a basic leaflet map using just a couple lines of code. Note that unlike ggplot2, the leaflet package uses pipe operators (%&gt;%) and not the additive operator (+). The addTiles() function without arguments will add base tiles to your map from OpenStreetMap. addMarkers() will add a marker at each location specified by the latitude and longitude arguments. Note that the ~ symbol is used here to model the coordinates to the map (similar to facet_wrap in ggplot). leaflet(locations) %&gt;% addTiles() %&gt;% addMarkers(lng = ~Longitude, lat = ~Latitude, popup = ~ Location) You can also use leaflet to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans (GEBCO) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments.. leaflet(locations) %&gt;% addWMSTiles(&quot;https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?&quot;, layers = &#39;GEBCO_LATEST&#39;, attribution = &quot;Imagery reproduced from the GEBCO_2014 Grid, version 20150318, www.gebco.net&quot;) %&gt;% addCircleMarkers(lng = ~Longitude, lat = ~Latitude, popup = ~ Location, radius = 5, # set fill properties fillColor = &quot;salmon&quot;, fillOpacity = 1, # set stroke properties stroke = T, weight = 0.5, color = &quot;white&quot;, opacity = 1) Leaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. Here is an example of some we created as part of the SASAP project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of RMarkdown and GitHub pages can be. 12.6 Resources Lisa Charlotte Rost. (2018) Why not to use two axes, and what to use instead: The case against dual axis charts ggplot: https://ggplot2.tidyverse.org/index.html patchwork: https://patchwork.data-imaginist.com/articles/guides/layout.html rleaflet: https://rstudio.github.io/leaflet/ "],
["getting-the-most-out-of-collaborative-science.html", "13 Getting the Most out of Collaborative Science 13.1 Learning objectives 13.2 Aligning around shared purpose and a clear work plan 13.3 Culture and leadership 13.4 Traditional and network mindsets 13.5 Resources", " 13 Getting the Most out of Collaborative Science 13.1 Learning objectives In this lesson you will learn What conditions enable successful interdisciplinary and intersectoral collaboration How to strengthen the structures, processes, and mindsets that support team science collaboration How to build capacity within your teams and the broader system for more effective collaboration, facilitation, and principles-based convening Characteristics of effective teams Right size Right people Aligned purpose and incentives Effective organizational structure Strong individual contributions Supportive team processes See Micken and Rodger 2000 in the Resources below for more detail. 13.2 Aligning around shared purpose and a clear work plan Coming to a shared definition of success and understanding of the project scope is critical for success. It may be useful to write a short manifesto that articulates what you are doing, why and how. Once the purpose is clear, then you need a clear work plan and timeline that articulates the group’s work streams and the roles, responsibilities and tasks of group members. Tools that can help support this include: Responsibility assignment matrix (RACI) Gantt Chart There are many free online templates available for these, as well as various project management software platforms. Whatever you use, make it accessible to all team members so everyone can record and track progress. What is most important is to select a system that the majority of your team members are likely to use. If there is already momentum toward a particular tool or platform, your best bet is probably to go with that momentum rather than try to institute a new practice or tool. 13.3 Culture and leadership The success of a working group or other team science endeavor hinges on a good collaborative culture emerging within the group, and interactions early on help set expectations and direction. The meeting chair and/or facilitator can use signposting language (e.g., “We want your input on our goals and work plan so we can get to a shared vision of where we’re headed.”) and structures for interaction (e.g., those that invite participation by everyone) to set the tone. Some elements of this culture include: Mutual respect, trust, and opportunities for human connection Clear norms and expectations, e.g. for how conflicts will be handled and how credit will be shared and attributed Clear data sharing and authorship guidelines: If your group spans diverse fields and/or sectors, members may have divergent perspectives, so it’s important to talk about this openly early in the process Equal opportunities for participation that welcome all voices and contributions Sharing facilitation responsibilities: Each activity should have an objective. The duty of keeping the group on track toward those objectives can be rotated. 13.4 Traditional and network mindsets Collaborative work requires us to move in and out of what is known as a network mindset (versus a traditional mindset, which might apply, for example in a traditional lab setting). It is the network mindset that sees the potential for greater than the sum of the parts solutions. Orienting from a network mindset we lean into uncertainty, trusting that useful outcomes may emerge from the collective intelligence of the network. Those outcomes include enhanced learning, new ideas, and new opportunities for collaborative research. It’s important to note, though, that there is a place for both the traditional and the network mindset. Neither is better than the other. The skill comes in knowing when one or the other is required and moving easily between them. Traditional Mindset Network Mindset Hierarchical Horizontal Firmly controlled and planned Loosely controlled and emergent Centralized authority Distributed authority Task Oriented Relationship Oriented Production of products and services Cultivation of learning and activity Strengthening individual efforts Weaving connections and building networks Proprietary information and learning Open information and learning Decision making concentrated Decision making shared / transparent Individual intelligence and achievement Collective intelligence and action Effectiveness linked to concrete outputs Effectiveness also linked to intangibles (trusting relationships, information flows) Credit claimed by individual or single organization Credit shared with partners Conflict averse Conflict acknowledged and addressed Adapted from Monitor Institute &amp; Rockefeller Foundation, ENGAGE 13.5 Resources Micken and Rodgers. 2000. Characteristics of effective teams: A literature review. Australian Health Reviews "],
["navigating-challenges-in-collaborative-science.html", "14 Navigating Challenges in Collaborative Science 14.1 Learning objectives 14.2 Authorship and credit 14.3 Data sharing 14.4 Resources", " 14 Navigating Challenges in Collaborative Science 14.1 Learning objectives In this lesson you will learn Best practices for navigating authorship, credit, and power dynamics in team science collaborations Best practices for data sharing in team science collaborations 14.2 Authorship and credit Most people who have participated in group projects will recognize that maintaining clear channels of communication among team members can be a big challenge. At NCEAS we encourage scientists to discuss authorship, credit and data sharing early and often. Many publications resulting from NCEAS work are authored by large groups of individuals. These individuals may not know each other well at the beginning of the project, and they may represent fields which have diverse views on authorship and data sharing. Check the “Publication” section of the Ecological Society of America’s Code of Ethics as a starting point for discussions about co-authorship. NCEAS endorses these principles in establishing authorship. You might also wish to check guidelines published by the journal(s) to which you anticipate submitting your work. We recommend you write an authorship agreement for your group early in the project and refer to it for each product. A template is provided in the Resources section below. 14.3 Data sharing NCEAS is committed to making ecological data available to the broader scientific community, and we expect that NCEAS projects adhere to our data policy, which can be found here. As with authorship, we also encourage groups to engage in open, honest conversation about data sharing early in their collaboration. In addition, there may be legal or policy considerations related to data sharing for your project. You can learn more here. We recommend you write a data sharing agreement for your group early on and revisit it periodically throughout the project. A template is provided in the Resources section below. 14.4 Resources Authorship template Cheruvelil, Kendra S., et al. “Creating and maintaining high‐performing collaborative research teams: the importance of diversity and interpersonal skills.” Frontiers in Ecology and the Environment 12.1 (2014): 31-38. DataOne Best Practices Database Data sharing template Frassl, Marieke A et al. “Ten simple rules for collaboratively writing a multi-authored paper.” PLoS computational biology vol. 14,11 e1006508. 15 Nov. 2018, doi:10.1371/journal.pcbi.1006508 Goring, Simon J., et al. “Improving the culture of interdisciplinary collaboration in ecology by expanding measures of success.” Frontiers in Ecology and the Environment 12.1 (2014): 39-47. "],
["coding-best-practices-and-tips.html", "15 Coding best practices and tips 15.1 Scripting languages 15.2 Structure of a script 15.3 Functions 15.4 Scope: Global vs. Local Environment 15.5 Note about loading libraries in R 15.6 Solutions to function challenge 15.7 References", " 15 Coding best practices and tips 15.1 Scripting languages Compared to other programming languages (such as C), scripting languages are not required to be compiled to be executable. One consequence is that generally scripts will execute more slowly than a compiled executable because they need an interpreter. However, the more natural language oriented syntax makes them easier to learn and use. 15.1.1 Don’t start coding without planning! It is important to stress that scientists write scripts to help them to investigate scientific question(s). Therefore it is important that scripting does not drive our analysis and thinking. We strongly recommend you take the time to plan the steps you need to accomplish to conduct your analysis. Developing a scientific workflow of your analyis will then allow you to develop a pseudo code that will help you to narrow down the tasks that need to be accomplished to move forward your analaysis. 15.2 Structure of a script A script can be divided into several sections. Each scripting language has its own syntax and style, but these main components are generally accepted: From the top to the bottom of your script: Comment explaining the purpose of the script Attribution: authors, contributors, date of last update, contact info Import of external modules / packages Constant definitions Function definitions (respecting the order in which they are called) Main code calling the different functions 15.2.1 Few important principles Comment your code. It will allow you to document what your code does both for your collaborators and your future self Use variables and constants instead of hard links Choose intuitives names for your variables and functions, not generic. If you store a list of files, do not use x for the variable name, use instead files. Even better use input_files if you are listing the files you are importing. Be consistent in terms of style you use to name your variables, functions, … keep it simple, stupid (KISS). Do not create overly complicated statements. Implement your tasks in several simple lines instead of embedding a lot of executions in one complicated line. It will save you time while debugging and make your code more readable to others Go modular! Break down tasks into small code fragments such as functions. It will make your code reusable for you and others (if well documented). Keep functions simple; they should only implement one or few (related) tasks Don’t Repeat Yourself (DRY). If you start copy/pasting part of your code changing few parameters =&gt; write a function and call it several times with different parameters. Add flow control such as loops and conditions. It will be easier to debug, change and maintain Test your code. Test your code against values you would expect or computed with another software. Try hedge cases, such as NA, negative values, …. and try to handle errors in your code Iterate with small steps, implement few changes at a time to your code. Test, fix and move forward! 15.3 Functions One of the fundamental ways of making your code modular and reusable is to create functions that you can call to do a task. 15.3.1 Defining a function It is important to note that the statements in a function definition are NOT executed as the interpreter first passes over the lines. The function needs to be CALLED for these statements to be exectuted. In other words, if you have a bug in your function statements, but your main code never calls this function, the script will run just fine. Only when you call the function you will discover the problem. Keep that in my mind while debugging. 15.3.2 R function definition my_function_name &lt;- function(argument1, argument2, opt_arg=default_value) { statements return(object) # not always necessary } Note it is specific to R that functions are defined using the regular assignment operator &lt;-. 15.3.2.1 Python function definition def my_function_name(argument1, argument2, optional_arg=default_value): statements return object Python uses the def keyword to signal the definiton of a function 15.3.3 Calling a function 15.3.3.1 R my_results &lt;- my_function_name(1,2) # want to call the fucntion with parameters out of order? my_results &lt;- my_function_name(argument2=2, argument1=1) 15.3.3.2 Python my_results = my_function_name(1,2) # want to call the fucntion with parameters out of order? my_results = my_function_name(argument2=2, argument1=1) 15.3.4 Challenge Write a function that computes the percentage of a number: n*p/100 Make the ratio factor an argument so we can also use it to compute 1/1000 On the same script write a second function to compute a2-b2 Modify your function to compute the square root: sqrt(a2-b2). Find potential values that could make the function to crash and add necessary error handling Comment your functions 15.4 Scope: Global vs. Local Environment All variables in a program may not be accessible at all locations in that program. This depends on where you have declared a variable. Variables that are defined inside a function body have a local scope, and those defined outside have a global scope. This means that local variables can be only be accessed inside the function in which they are declared. Thus, any ordinary assignments done within the function are local and temporary and are lost after exit from the function; whereas global variables can be accessed throughout the program body by all functions. 15.4.1 Challenge Try to predict what will be outputted by the following: foo &lt;- function() { bar &lt;- 1 } bar &lt;- 2 foo &lt;- function() { bar &lt;- 1 } foo &lt;- function() { bar &lt;&lt;- 1 #hein?! } Although &lt;&lt;- is a specificity of R, the general concept of global and local scope is valid for Python and others. 15.4.2 One last one foo &lt;- function() { bar &lt;- bar + 1 } This also means that there is one more reason to use functions!! All the temporary variables you define in a function are automatically deleted when you exit the function, freeing up the computer memory. 15.5 Note about loading libraries in R First: is a package a library? Not until it is installed on your system using (install.packages(“this_awsome_package_I_need”)! see here for more details. 15.5.1 Namespace In R you use the command library(mypackage) to load a library. Each library has its own namespace. For example, if you use dplyr::union dplyr is the namespace and union is the function. Namespace attribution is a great way to be sure that functions from different packages sharing the same name will not overwrite each other. However when you load a library, you might have noticed/ignored similar messages: library(dplyr) 15.5.2 Challenge What does it means? Does the order you use to load library matter in R? How can you check what you are using? 15.6 Solutions to function challenge 15.6.1 First function task # function to calculate n*p/some ratio factor percent_function &lt;- function(n,p,ratio_factor) { #n &lt;- first value #p &lt;- second value #ratio_factor &lt;- denominator; value by which n*p is divided pct &lt;- (n*p)/ratio_factor return(pct) } # test function test &lt;- percent_function(n=10,p=10,ratio_factor=1000) test 0.1 15.6.2 Second function task # function to calculate a^2-b^2 a2b2_function &lt;- function(a,b) { #a &lt;- first value #b &lt;- second value a2b2 &lt;- a^2 - b^2 return(a2b2) } # test function test &lt;- a2b2_function(a=3,b=2) test 5 15.6.3 Third function task # function to calculate sqrt(a^2-b^2) sqrt_a2b2_function &lt;- function(a,b) { #a &lt;- first value #b &lt;- second value a2b2 &lt;- a^2 - b^2 squared &lt;- sqrt(a2b2) return(squared) #produces NaN if b^2 &lt; a^2 } # test function test &lt;- sqrt_a2b2_function(a=3,b=2) test 2.236068 15.7 References Functional programming in R: https://adv-r.hadley.nz/fp.html https://www.quora.com/What-is-the-difference-between-programming-languages-markup-languages-and-scripting-languages http://stackoverflow.com/questions/17253545/scripting-language-vs-programming-language More advanced information about scope and closure in R: https://darrenjw.wordpress.com/2011/11/23/lexical-scope-and-function-closures-in-r/ R styling: A good overview by H. Wickham of the best practices regarding best syntax to us in R from Hadley Wickham. Standardized comments: https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html Python styling: the Hitchhiker’s guide try this from the Python console: import this "],
["best-practices-data-and-metadata.html", "16 Best Practices: Data and Metadata 16.1 Learning Objectives 16.2 Preserving computational workflows 16.3 Best Practices: Overview 16.4 Organizing Data: Best Practices", " 16 Best Practices: Data and Metadata 16.1 Learning Objectives In this lesson, you will learn: Why preserving computational workflows is important How to acheive practical reproducibility What are some best practices for data and metadata management Download slides: Best Practices: Data and Metadata 16.2 Preserving computational workflows Preservation enables: Understanding Evaluation Reuse All for Future You! And your collaborators and colleagues across disciplines. Figure 16.1: Scientific products that need to be preserved from computational workflows include data, software, metadata, and other products like graphs and figures. While the Arctic Data Center, Knowledge Network for Biocomplexity and similar repositories do focus on preserving data, we really set our sights much more broadly on preserving entire computational workflows that are instrumental to advances in science. A computational workflow represents the sequence of computational tasks that are performed from raw data acquisition through data quality control, integration, analysis, modeling, and visualization. Figure 16.2: Computational steps can be organized as a workflow streaming raw data through to derived products. In addition, these workflows are often not executed all at once, but rather are divided into multiple workflows, each with its own purpose. For example, a data acquistion and cleaning workflow often creates a derived and integrated data product that is then picked up and used by multiple downstream analytical workflows that produce specific scientific findings. These workflows can each be archived as distinct data packages, with the output of the first workflow becoming the input of the second and subsequent workflows. Figure 16.3: Computational workflows can be archived and preserved in multiple data packages that are linked by their shared components, in this case an intermediate data file. 16.3 Best Practices: Overview Who Must Submit? Organizing Data File Formats Large Data Packages Metadata Data Identifiers Provenance Licensing and Distribution 16.4 Organizing Data: Best Practices Both (Borer et al. 2009) and (White et al. 2013) provide some practical guidelines for organizing and structuring data. Critical aspects of their recommendations include: Write scripts for all data manipulation Uncorrected raw data file Document processing in scripts Design to add rows, not columns Each column one variable Each row one observation Use nonproprietary file formats Descriptive names, no spaces Header line References "],
["spatial-vector-analysis-using-sf.html", "17 Spatial vector analysis using sf 17.1 Learning Objectives 17.2 Introduction 17.3 Reading a shapefile 17.4 sf &amp; the Tidyverse 17.5 Visualize with ggplot 17.6 Incorporate base maps into static maps using ggmap 17.7 Visualize sf objects with leaflet", " 17 Spatial vector analysis using sf 17.1 Learning Objectives In this lesson, you will learn: How to use the sf package to analyze geospatial data Static mapping with ggplot interactive mapping with leaflet 17.2 Introduction From the sf vignette: Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them. The sf package is an R implementation of Simple Features. This package incorporates: a new spatial data class system in R functions for reading and writing data tools for spatial operations on vectors Most of the functions in this package starts with prefix st_ which stands for spatial and temporal. In this tutorial, our goal is to use a shapefile of Alaska regions and data on population in Alaska by community to create a map that looks like this: The data we will be using to create the map are: Alaska regional boundaries Community locations and population Alaksa rivers 17.3 Reading a shapefile All of the data used in this tutorial are simplified versions of real datasets available on the KNB. I’ve simplified the original high-resolution geospatial datasets to ease the processing burden on your computers while learning how to do the analysis. These simplified versions of the datasets may contain topological errors. The original version of the datasets are indicated throughout the chapter. 17.3.1 Setup For convience, I’ve hosted a zipped copy of all of the files on our test site. Go to this dataset, right click the download button, and select copy link address From the terminal, make and navigate to a directory called data in your project directory mkdir data, cd data Use wget to download the zip into the data directory wget -O data.zip https://dev.nceas.ucsb.edu/knb/d1/mn/v2/object/urn%3Auuid%3Aaceaecb2-1ce0-4d41-a839-d3607d32bb58 Unzip the file using unzip unzip data.zip The first file we will use is a shapefile of regional boundaries in alaska derived from: Jared Kibele and Jeanette Clark. 2018. State of Alaska’s Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. doi:10.5063/F1125QWP. First load the libraries library(sf) library(dplyr) library(ggplot2) library(leaflet) library(scales) library(ggmap) Read in the data and look at a plot of it. ## Read in shapefile using sf ak_regions &lt;- read_sf(&quot;data/shapefiles/ak_regions_simp.shp&quot;) plot(ak_regions) We can also examine it’s class. class(ak_regions) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; sf objects usually have two types - sf and data.frame. Two main differences comparing to a regular data.frame object are spatial metadata (geometry type, dimension, bbox, epsg (SRID), proj4string) and additional column - typically named geometry. Since our shapefile object has the data.frame class, viewing the contents of the object using the head function shows similar results to data we read in using read.csv. head(ak_regions) ## Simple feature collection with 6 features and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.2296 ymin: 51.15702 xmax: 179.8567 ymax: 71.43957 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 6 x 4 ## region_id region mgmt_area geometry ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 1 Aleutian I… 3 (((-171.1345 52.44974, -171.1686 52.41744, -1… ## 2 2 Arctic 4 (((-139.9552 68.70597, -139.9893 68.70516, -1… ## 3 3 Bristol Bay 3 (((-159.8745 58.62778, -159.8654 58.61376, -1… ## 4 4 Chignik 3 (((-155.8282 55.84638, -155.8049 55.86557, -1… ## 5 5 Copper Riv… 2 (((-143.8874 59.93931, -143.9165 59.94034, -1… ## 6 6 Kodiak 3 (((-151.9997 58.83077, -152.0358 58.82714, -1… 17.3.2 Coordinate Reference System Every sf object needs a coordinate reference system (or crs) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are “unprojected” (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984. You can view what crs is set by using the function st_crs st_crs(ak_regions) ## Coordinate Reference System: ## EPSG: 4326 ## proj4string: &quot;+proj=longlat +datum=WGS84 +no_defs&quot; This is pretty confusing looking. Without getting into the details, that long string says that this data has a greographic coordinate system (WGS84) with no projection. A convenient way to reference crs quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes here. We will use several EPSG codes in this lesson. Here they are, along with their more readable names: 3338: Alaska Albers 4326: WGS84 (World Geodetic System 1984), used in GPS 3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI You will often need to transform your geospatial data from one coordinate system to another. The st_transform function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of 3338. ak_regions_3338 &lt;- ak_regions %&gt;% st_transform(crs = 3338) st_crs(ak_regions_3338) ## Coordinate Reference System: ## EPSG: 3338 ## proj4string: &quot;+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; plot(ak_regions_3338) Much better! 17.3.3 Attributes sf objects can be used as a regular data.frame object in many operations. We already saw the results of plot and head. Here are a couple more: nrow(ak_regions_3338) ## [1] 13 ncol(ak_regions_3338) ## [1] 4 summary(ak_regions_3338) ## region_id region mgmt_area geometry ## Min. : 1 Length:13 Min. :1 MULTIPOLYGON :13 ## 1st Qu.: 4 Class :character 1st Qu.:2 epsg:3338 : 0 ## Median : 7 Mode :character Median :3 +proj=aea ...: 0 ## Mean : 7 Mean :3 ## 3rd Qu.:10 3rd Qu.:4 ## Max. :13 Max. :4 17.4 sf &amp; the Tidyverse Since sf objects are dataframes, they play nicely with packages in the tidyverse. Here are a couple of simple examples: select() ak_regions_3338 %&gt;% select(region) ## Simple feature collection with 13 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -2175328 ymin: 405653.9 xmax: 1579226 ymax: 2383770 ## epsg (SRID): 3338 ## proj4string: +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## # A tibble: 13 x 2 ## region geometry ## &lt;chr&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 Aleutian Islands (((-1156666 420855.1, -1159837 417990.3, -1161898 416944.4,… ## 2 Arctic (((571289.9 2143072, 569941.5 2142691, 569158.2 2142146, 56… ## 3 Bristol Bay (((-339688.6 973904.9, -339302 972297.3, -339229.2 971037.4… ## 4 Chignik (((-114381.9 649966.8, -112866.8 652065.8, -108836.8 654303… ## 5 Copper River (((561012 1148301, 559393.7 1148169, 557797.7 1148492, 5559… ## 6 Kodiak (((115112.5 983293, 113051.3 982825.9, 110801.3 983211.6, 1… ## 7 Kotzebue (((-678815.3 1819519, -677555.2 1820698, -675557.8 1821561,… ## 8 Kuskokwim (((-1030125 1281198, -1029858 1282333, -1028980 1284032, -1… ## 9 Cook Inlet (((35214.98 1002457, 36660.3 1002038, 36953.11 1001186, 367… ## 10 Norton Sound (((-848357 1636692, -846510 1635203, -840513.7 1632225, -83… ## 11 Prince William … (((426007.1 1087250, 426562.5 1088591, 427711.6 1089991, 42… ## 12 Southeast (((1287777 744574.1, 1290183 745970.8, 1292940 746262.7, 12… ## 13 Yukon (((-375318 1473998, -373723.9 1473487, -373064.8 1473930, -… Note the sticky geometry column! The geometry column will stay with your sf object even if it is not called explicitly. filter() ak_regions_3338 %&gt;% filter(region == &quot;Southeast&quot;) ## Simple feature collection with 1 feature and 3 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 559475.7 ymin: 722450 xmax: 1579226 ymax: 1410576 ## epsg (SRID): 3338 ## proj4string: +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## # A tibble: 1 x 4 ## region_id region mgmt_area geometry ## * &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 12 Southea… 1 (((1287777 744574.1, 1290183 745970.8, 1292940 7… 17.4.1 Joins You can also use the sf package to create spatial joins, useful for when you want to utilize two datasets together. As an example, let’s ask a question: how many people live in each of these Alaska regions? We have some population data, but it gives the number of people by city, not by region. To determine the number of people per region we will need to: read in the city data from a csv and turn it into an sf object use a spatial join (st_join) to assign each city to a region use group_by and summarize to calculate the total population by region First, read in the population data as a regular data.frame. This data is derived from: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. doi:10.5063/F11G0JHX. Unnecessary columns were removed and the most recent year of data was selected. pop &lt;- read.csv(&quot;data/shapefiles/alaska_population.csv&quot;) The st_join function is a spatial left join. The arguments for both the left and right tables are objects of class sf which means we will first need to turn our population data.frame with latitude and longitude coordinates into an sf object. We can do this easily using the st_as_sf function, which takes as arguments the coordinates and the crs. The remove = F specification here ensures that when we create our geometry column, we retain our original lat lng columns, which we will need later for plotting. Although it isn’t said anywhere explicitly in the file, let’s assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a crs number of 4236. pop_4326 &lt;- st_as_sf(pop, coords = c(&#39;lng&#39;, &#39;lat&#39;), crs = 4326, remove = F) head(pop_4326) ## Simple feature collection with 6 features and 5 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -176.6581 ymin: 51.88 xmax: -154.1703 ymax: 62.68889 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## year city lat lng population geometry ## 1 2015 Adak 51.88000 -176.6581 122 POINT (-176.6581 51.88) ## 2 2015 Akhiok 56.94556 -154.1703 84 POINT (-154.1703 56.94556) ## 3 2015 Akiachak 60.90944 -161.4314 562 POINT (-161.4314 60.90944) ## 4 2015 Akiak 60.91222 -161.2139 399 POINT (-161.2139 60.91222) ## 5 2015 Akutan 54.13556 -165.7731 899 POINT (-165.7731 54.13556) ## 6 2015 Alakanuk 62.68889 -164.6153 777 POINT (-164.6153 62.68889) Now we can do our spatial join! You can specify what geometry function the join uses (st_intersects, st_within, st_crosses, st_is_within_distance, …) in the join argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles. In this case, we want to find what region each city falls within, so we will use st_within. pop_joined &lt;- st_join(pop_4326, ak_regions_3338, join = st_within) This gives an error! Error: st_crs(x) == st_crs(y) is not TRUE Turns out, this won’t work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using st_transform, and projecting our population object into Alaska Albers. pop_3338 &lt;- st_transform(pop_4326, crs = 3338) pop_joined &lt;- st_join(pop_3338, ak_regions_3338, join = st_within) head(pop_joined) ## Simple feature collection with 6 features and 8 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -1537925 ymin: 472627.8 xmax: -10340.71 ymax: 1456223 ## epsg (SRID): 3338 ## proj4string: +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## year city lat lng population region_id region ## 1 2015 Adak 51.88000 -176.6581 122 1 Aleutian Islands ## 2 2015 Akhiok 56.94556 -154.1703 84 6 Kodiak ## 3 2015 Akiachak 60.90944 -161.4314 562 8 Kuskokwim ## 4 2015 Akiak 60.91222 -161.2139 399 8 Kuskokwim ## 5 2015 Akutan 54.13556 -165.7731 899 1 Aleutian Islands ## 6 2015 Alakanuk 62.68889 -164.6153 777 13 Yukon ## mgmt_area geometry ## 1 3 POINT (-1537925 472627.8) ## 2 3 POINT (-10340.71 770998.4) ## 3 4 POINT (-400885.5 1236460) ## 4 4 POINT (-389165.7 1235475) ## 5 3 POINT (-766425.7 526057.8) ## 6 4 POINT (-539724.9 1456223) 17.4.2 Group and summarize Next we compute the total population for each region. In this case, we want to do a group_by and summarise as this were a regular data.frame - otherwise all of our point geometries would be included in the aggreation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using as.data.frame, on the advice of the sf::tidyverse help page. pop_region &lt;- pop_joined %&gt;% as.data.frame() %&gt;% group_by(region) %&gt;% summarise(total_pop = sum(population)) head(pop_region) ## # A tibble: 6 x 2 ## region total_pop ## &lt;chr&gt; &lt;int&gt; ## 1 Aleutian Islands 8840 ## 2 Arctic 8419 ## 3 Bristol Bay 6947 ## 4 Chignik 311 ## 5 Cook Inlet 408254 ## 6 Copper River 2294 And use a regular left_join to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps. pop_region_3338 &lt;- left_join(ak_regions_3338, pop_region) ## Joining, by = &quot;region&quot; #plot to check plot(pop_region_3338[&quot;total_pop&quot;]) So far, we have learned how to use sf and dplyr to use a spatial join on two datasets and calculate a summary metric from the result of that join. The group_by and summarize functions can also be used on sf objects to summarize within a dataset and combine geometries. Many of the tidyverse functions have methods specific for sf objects, some of which have additional arguments that wouldn’t be relevant to the data.frame methods. You can run ?sf::tidyverse to get documentation on the tidyverse sf methods. Let’s try some out. Say we want to calculate the population by Alaska management area, as opposed to region. pop_mgmt_338 &lt;- pop_region_3338 %&gt;% group_by(mgmt_area) %&gt;% summarize(total_pop = sum(total_pop)) plot(pop_mgmt_338[&quot;total_pop&quot;]) Notice that the region geometries were combined into a single polygon for each management area. If we don’t want to combine geometries, we can specifcy do_union = F as an argument. pop_mgmt_3338 &lt;- pop_region_3338 %&gt;% group_by(mgmt_area) %&gt;% summarize(total_pop = sum(total_pop), do_union = F) plot(pop_mgmt_3338[&quot;total_pop&quot;]) 17.4.3 Save Save the spatial object to disk using write_sf() and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver driver, but there are many other format options available. write_sf(pop_region_3338, &quot;data/shapefiles/ak_regions_population.shp&quot;, delete_layer = TRUE) 17.5 Visualize with ggplot ggplot2 now has integrated functionality to plot sf objects using geom_sf(). We can plot sf objects just like regular data.frames using geom_sf. ggplot(pop_region_3338) + geom_sf(aes(fill = total_pop)) + theme_bw() + labs(fill = &quot;Total Population&quot;) + scale_fill_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) We can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the crs to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings). The rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW. rivers_3338 &lt;- read_sf(&quot;data/shapefiles/ak_rivers_simp.shp&quot;) st_crs(rivers_3338) ## Coordinate Reference System: ## No EPSG code ## proj4string: &quot;+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs&quot; Note that although no EPSG code is set explicitly, with some sluething we can determine that this is EPSG:3338. This site is helpful for looking up EPSG codes. ggplot() + geom_sf(data = pop_region_3338, aes(fill = total_pop)) + geom_sf(data = rivers_3338, aes(size = StrOrder), color = &quot;black&quot;) + geom_sf(data = pop_3338, aes(), size = .5) + scale_size(range = c(0.01, 0.2), guide = F) + theme_bw() + labs(fill = &quot;Total Population&quot;) + scale_fill_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) 17.6 Incorporate base maps into static maps using ggmap The ggmap package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others. We’ll need to transform our shapefile with population data by community to EPSG:3857 which is the CRS used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others. pop_3857 &lt;- pop_3338 %&gt;% st_transform(crs = 3857) Next, let’s grab a base map from the Stamen map tile server covering the region of interest. First we include a function that transforms the bounding box (which starts in EPSG:4326) to also be in the EPSG:3857 CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with ggmap described in more detail here # Define a function to fix the bbox to be in EPSG:3857 # See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208 ggmap_bbox_to_3857 &lt;- function(map) { if (!inherits(map, &quot;ggmap&quot;)) stop(&quot;map must be a ggmap object&quot;) # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector, # and set the names to what sf::st_bbox expects: map_bbox &lt;- setNames(unlist(attr(map, &quot;bb&quot;)), c(&quot;ymin&quot;, &quot;xmin&quot;, &quot;ymax&quot;, &quot;xmax&quot;)) # Coonvert the bbox to an sf polygon, transform it to 3857, # and convert back to a bbox (convoluted, but it works) bbox_3857 &lt;- st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857)) # Overwrite the bbox of the ggmap object with the transformed coordinates attr(map, &quot;bb&quot;)$ll.lat &lt;- bbox_3857[&quot;ymin&quot;] attr(map, &quot;bb&quot;)$ll.lon &lt;- bbox_3857[&quot;xmin&quot;] attr(map, &quot;bb&quot;)$ur.lat &lt;- bbox_3857[&quot;ymax&quot;] attr(map, &quot;bb&quot;)$ur.lon &lt;- bbox_3857[&quot;xmax&quot;] map } Next, we define the bounding box of interest, and use get_stamenmap() to get the basemap. Then we run our function defined above on the result of the get_stamenmap() call. bbox &lt;- c(-170, 52, -130, 64) # This is roughly southern Alaska ak_map &lt;- get_stamenmap(bbox, zoom = 4) ak_map_3857 &lt;- ggmap_bbox_to_3857(ak_map) Finally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857): ggmap(ak_map_3857) + geom_sf(data = pop_3857, aes(color = population), inherit.aes = F) + scale_color_continuous(low = &quot;khaki&quot;, high = &quot;firebrick&quot;, labels = comma) 17.7 Visualize sf objects with leaflet We can also make an interactive map from our data above using leaflet. Leaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what crs you have set using st_crs. Here we define a leaflet projection for Alaska Albers, and save it as a variable to use later. epsg3338 &lt;- leaflet::leafletCRS( crsClass = &quot;L.Proj.CRS&quot;, code = &quot;EPSG:3338&quot;, proj4def = &quot;+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;, resolutions = 2^(16:7)) You might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the crs of our shapefile: st_crs(pop_region_3338) ## Coordinate Reference System: ## EPSG: 3338 ## proj4string: &quot;+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; Since leaflet requires that we use an unprojected coordinate system, let’s use st_transform yet again to get back to WGS84. pop_region_4326 &lt;- pop_region_3338 %&gt;% st_transform(crs = 4326) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = &quot;gray&quot;, weight = 1) m We can add labels, legends, and a color scale. pal &lt;- colorNumeric(palette = &quot;Reds&quot;, domain = pop_region_4326$total_pop) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = ~pal(total_pop), weight = 1, color = &quot;black&quot;, fillOpacity = 1, label = ~region) %&gt;% addLegend(position = &quot;bottomleft&quot;, pal = pal, values = range(pop_region_4326$total_pop), title = &quot;Total Population&quot;) m We can also add the individual communities, with popup labels showing their population, on top of that! pal &lt;- colorNumeric(palette = &quot;Reds&quot;, domain = pop_region_4326$total_pop) m &lt;- leaflet(options = leafletOptions(crs = epsg3338)) %&gt;% addPolygons(data = pop_region_4326, fillColor = ~pal(total_pop), weight = 1, color = &quot;black&quot;, fillOpacity = 1) %&gt;% addCircleMarkers(data = pop_4326, lat = ~lat, lng = ~lng, radius = ~log(population/500), # arbitrary scaling fillColor = &quot;gray&quot;, fillOpacity = 1, weight = 0.25, color = &quot;black&quot;, label = ~paste0(pop_4326$city, &quot;, population &quot;, comma(pop_4326$population))) %&gt;% addLegend(position = &quot;bottomleft&quot;, pal = pal, values = range(pop_region_4326$total_pop), title = &quot;Total Population&quot;) m There is a lot more functionality to sf including the ability to intersect polygons, calculate distance, create a buffer, and more. Here are some more great resources and tutorials for a deeper dive into this great package: Spatial analysis in R with the sf package Intro to Spatial Analysis sf github repo Tidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf mapping-fall-foliage-with-sf Geocomputation with R "],
["raster-analysis.html", "18 Raster Analysis 18.1 Learning Objectives 18.2 Introduction 18.3 The raster Package 18.4 Other Thoughts 18.5 References", " 18 Raster Analysis 18.1 Learning Objectives In this lesson, you will learn: How to use the raster package to import geospatial data how to combine raster and vector to conduct geospatial analysis in R 18.2 Introduction We just have seen how to conduct spatial analysis in R using vector data. There is a second categories of geospatial data: raster data. In a nutshell, raster data is a matrix of cells (or pixels) organized into rows and columns. Each cell / pixels stores a value. Satellite imagery, aerial imagery and radar from Earth Observation Systems are a great source of environmental data. As discussed in the previous section, raster will have a Coordinate Reference System (CRS) defining the mathematical formula used to used to define the position of the raster pixels on earth. Few extra terms you might hear when talking about raster: The spatial resolution of a raster refers to the size of a pixel. It is often defined as one number (e.g. 30m) assuming the pixels are squares; However it is not a requirement and pixels can be rectangular (have edges of different length). The extent of a raster refers to the area on Earth that is covered by the entire raster. It be defined in many ways, but it will often be defined using a specific corner, the pixel size, and the number of pixels along the x and y dimensions. The number of bands of a raster refers to a set of images over the same area but with value corresponding to different “bands” of the electromagnetic spectrum. For example, a typical RGB image will be made of 3: Red, Green, Blue. Often you want to stack those images to conduct an analysis as the area of observation matched perfectly. Satellite and other sensors can of course capture information far beyond the visible part of the electromagnetic spectrum. 18.2.1 Raster File Formats As vector data, raster data comes in various file formats: 18.2.1.1 TIFF/GeoTIFF TIFF/GeoTIFF are probably the most common raster formats. GeoTIFFs consist of one file with a specific header that encapsulates the geospatial information. There is a second way to add geospatial information to a TIFF file by adding an extra “world” file .tfw with the same name as the TIFF file. This .tfw is a simple text file that defines various parameters to define the raster geospatial information. 18.2.1.2 IMG This is a proprietary form from the remote sensing software ERDAS Imagine. It has the advantage to be able to store multiple bands in one file and also to have great compression capacity. 18.2.1.3 NetCDF The Network Common Data Form, or netCDF, is an interface to a library of data access functions for storing and retrieving data in the form of arrays. A Subset of this format, the Hierarchical data format (HDF4 or HDF5), is a format that is often used by satellite imagery data providers. Through its great handling of multidimensional data, it can be used to store both space and time information. One word on ESRI geospatial database storing raster data: it is unfortunately to sue R to read this data format. R relies on open source libraries to import and process geospatial data (GDAL, GEOS, PROJ, …), and ESRI has not yet opened this data format to those libraries. 18.3 The raster Package The raster package is THE package in R to import and process raster data. It can handle many of the formats mentioned above and more. To import a raster in R, you will use the raster function for a single band raster. For multi-band raster, you can use the brick() function instead. For this example, we are going to read a land cover map of Alaska from the The Multi-Resolution Land Characteristics (MRLC) consortium. The original file was quite large quite large (~8GB), so we have already resampled to a lower spatial resolution of 50m and saved it in a shared directory on the server. library(dplyr) library(sf) library(raster) library(ggplot2) library(curl) library(scales) # House keeping to setup data from the vector chapter ak_regions &lt;- read_sf(&quot;data/shapefiles/ak_regions_simp.shp&quot;) ak_regions_3338 &lt;- ak_regions %&gt;% st_transform(crs = 3338) pop &lt;- read.csv(&quot;data/shapefiles/alaska_population.csv&quot;) pop_4326 &lt;- st_as_sf(pop, coords = c(&#39;lng&#39;, &#39;lat&#39;), crs = 4326, remove = F) pop_3338 &lt;- st_transform(pop_4326, crs = 3338) pop_joined &lt;- st_join(pop_3338, ak_regions_3338, join = st_within) Let us now read this raster into R: lc_3338 &lt;- raster(&quot;/home/shares/scientist/ds-workshop/ak_nlcd_2011_landcover_50m_3338.tif&quot;) Look at the raster: lc_3338 ## class : RasterLayer ## dimensions : 40716, 74552, 3035459232 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : -2232595, 1495005, 344575, 2380375 (xmin, xmax, ymin, ymax) ## crs : +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## source : /tmp/RtmpakeRBa/file49ed3b556455 ## names : file49ed3b556455 ## values : 0, 95 (min, max) It is always good to check if everything aligns well: plot(lc_3338) plot(pop_3338, add=TRUE) ### Extracting information from raster OK, everything looks good. Now we want to find the most frequent land cover within a radius of 500m. To do so we can use the extract function. This function can handle, the buffering and even have a modal function that will let us find the mode. # Compute the most frequent land cover type around the population raster_points &lt;- extract(lc_3338, pop_3338, buffer=500, fun=modal) # Add those values to our data pop_joined$land_cover &lt;- raster_points Note that I am using 500 for the buffer radius because we know the unit is meter from the projection definition: crs : +proj=aea +lat_0=50 +lon_0=-154 +lat_1=55 +lat_2=65 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 **+units=m**. At this point, we only have the values for this raster and do not know yet what are the corresponding land covers. Luckily we can look at the metadata here: https://www.mrlc.gov/data/legends/national-land-cover-database-2011-nlcd2011-legend This not a super friendly format. We have already processed this information (see the end of this chapter to learn how) and stored it in a csv: ## ID COUNT Red Green Blue Land.Cover.Type ## 1 0 6539917102 0 0 0 Unclassified ## 2 11 267506887 71 107 161 Open Water ## 3 12 77582023 209 222 250 Perennial Ice/Snow ## 4 21 383719 222 202 202 Developed, Open Space ## 5 22 1080686 217 148 130 Developed, Low Intensity ## 6 23 145477 238 0 0 Developed, Medium Intensity ## 7 24 52478 171 0 0 Developed, High Intensity ## 8 31 146969306 179 174 163 Barren Land ## 9 41 61258881 104 171 99 Deciduous Forest ## 10 42 259612828 28 99 48 Evergreen Forest ## 11 43 61771547 181 202 143 Mixed Forest ## 12 51 322951293 166 140 48 Dwarf Shrub ## 13 52 426246570 204 186 125 Shrub/Scrub ## 14 71 32455056 227 227 194 Grassland/Herbaceous ## 15 72 107796396 202 202 120 Sedge/Herbaceous ## 16 74 582776 120 174 148 Moss ## 17 81 51213 220 217 61 Pasture/Hay ## 18 82 319495 171 112 40 Cultivated Crops ## 19 90 65732442 186 217 235 Woody Wetlands ## 20 95 56251009 112 163 186 Emergent Herbaceous Wetlands # Read the land cover legend Legend_lc &lt;- read.csv(&quot;/home/shares/scientist/ds-workshop/legend_ak_nlcd_2011.csv&quot;, stringsAsFactors = FALSE) Legend_lc Now we can join this information to our main data frame and since the categories are pretty detailed, we will also aggregate them to a coarser level of information: # It is a lot of categories, let us consolidate this Legend_lc &lt;- Legend_lc %&gt;% mutate(main_lc = ifelse(ID %in% 40:49, &quot;Forest&quot;, ifelse(ID %in% 20:29, &quot;Urban&quot;, ifelse(ID %in% 50:59, &quot;Shrub&quot;, ifelse(ID %in% 70:79, &quot;Grass&quot;, ifelse(ID %in% 80:89, &quot;Crops&quot;, ifelse(ID %in% 90:99, &quot;Wetland&quot;, Land.Cover.Type) ) ) ) ) ) ) # Join the LC categories to the population data pop_3338_cover &lt;- left_join(pop_joined, Legend_lc, by=c(&quot;land_cover&quot;=&quot;ID&quot;)) %&gt;% dplyr::select(-Red, -Green, -Blue, -Land.Cover.Type) 18.3.1 Plotting # Create color palette by keeping last color of each group pal &lt;- Legend_lc %&gt;% group_by(main_lc) %&gt;% slice(n()) %&gt;% # Keeping the last color of the groups ungroup %&gt;% arrange(ID) %&gt;% mutate(color_hex = rgb(Red, Green, Blue, max = 255)) %&gt;% dplyr::select(main_lc, color_hex) # turn pallete into a list for plotting pal_list &lt;- pal$color_hex names(pal_list) &lt;- pal$main_lc # Plot by region ggplot(pop_3338_cover, aes(region, population, fill = main_lc)) + geom_col() + scale_y_continuous(labels = comma) + scale_fill_manual(values = pal_list) + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle(&quot;Land cover type by region and population&quot;) + labs(fill = &quot;&quot;, y = &quot;Population&quot;, x = &quot;&quot;) ggplot() + geom_sf(data = ak_regions_3338, aes(), color = &quot;black&quot;) + geom_sf(data = pop_3338_cover, aes(color = main_lc, size = population), show.legend = &quot;point&quot;) + scale_size_continuous(guide = F) + scale_color_manual(values = pal_list) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) 18.3.2 Calculation with raster A Very useful function of the raster package is the function calc. As the name suggest this function helps you to do computation on raster. Continuing our example above, we want to compute a percentage of area that is forest. Let us first select a region of interest: copper_region_3338 &lt;- ak_regions_3338 %&gt;% filter(region == &quot;Copper River&quot;) Crop the raster using the region: copper_lc_3338 &lt;- crop(lc_3338, copper_region_3338) plot(copper_lc_3338) The way calc works is that you need to create a function to be applied to the raster using calc. Here is the function we will use create a binary mask for forest: # Function to create a binary mask for forest forest_masker &lt;- function(x){ x[x&lt;40 | x&gt;49] &lt;- 0 x[x&gt;=40 &amp; x&lt;=49] &lt;- 1 x } Now we can use the calc function to apply this function to every pixels: copper_forested &lt;- calc(copper_lc_3338, forest_masker) plot(copper_forested) As previously, we can now use the extract function to count the number of forested pixels: # Filter the population data for the copper region copper_pop_3338 &lt;- pop_3338_cover %&gt;% filter(region == &quot;Copper River&quot;) # Use those locations to extract the number of pixels wth forest forested_count &lt;- extract(copper_forested, copper_pop_3338, buffer=500, fun=sum) Adding the values back to the main data set: copper_pop_3338$forest_cov &lt;- 100 * forested_count / (20*20) # 20 pixels within the diameter head(copper_pop_3338) ## Simple feature collection with 6 features and 12 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 356847.6 ymin: 1318282 xmax: 505439 ymax: 1433557 ## epsg (SRID): 3338 ## proj4string: +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## year city lat lng population region_id region ## 1 2015 Chistochina 62.57178 -144.6542 59 5 Copper River ## 2 2015 Chitina 61.51583 -144.4369 80 5 Copper River ## 3 2015 Copper Center 61.97696 -145.3297 405 5 Copper River ## 4 2015 Eureka Roadhouse 61.93861 -147.1681 0 5 Copper River ## 5 2015 Gakona 62.30194 -145.3019 205 5 Copper River ## 6 2015 Glennallen 62.10917 -145.5464 366 5 Copper River ## mgmt_area land_cover COUNT main_lc geometry forest_cov ## 1 2 90 65732442 Wetland POINT (477473 1433557) 5.25 ## 2 2 41 61258881 Forest POINT (505439 1318282) 49.75 ## 3 2 42 259612828 Forest POINT (451821.3 1362933) 65.00 ## 4 2 52 426246570 Shrub POINT (356847.6 1347480) 13.00 ## 5 2 41 61258881 Forest POINT (448515.1 1399150) 43.50 ## 6 2 21 383719 Urban POINT (438715.9 1376141) 33.50 Plotting the percentage of forested area in the 1 km^2 surround of the population centers: ggplot() + geom_sf(data = copper_region_3338, aes(), color = &quot;black&quot;) + geom_sf(data = copper_pop_3338, aes(color = forest_cov, size = population)) + scale_size_continuous(guide = F) + scale_colour_gradientn(colours = terrain.colors(10, rev=TRUE)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + labs(color = &quot;Percent Forest Cover&quot;) 18.4 Other Thoughts 18.4.1 Raster processing can be slow Processing raster in R is not fast and raster processing can quickly involve large data set. This is especially true when reprojecting large raster. It is therefore also good to learn how to use other tools to do such tasks. gdal at the command line is a great one (see gdalwrap). It is also very important to process raster the correct way. Before starting the processing of large raster, we strongly recommend you read this information: https://rspatial.org/raster/appendix1.html 18.4.2 Preprocessing we did Actually because the original Land Cover raster was using a img format, it was possible to store more information then the pixel values. This format can handle categorical data and attach extra information, we can get the colors as specified by the data creator as well as the categories corresponding to a specific values. The the raster object in R use a S4 format that allows it to store specific information into slots. You can access these slots using the @. Here how we created the legend csv file you used: # Read the original raster lc &lt;- raster(&quot;/home/shares/scientist/ds-workshop/LC30/ak_nlcd_2011_landcover_1_15_15.img&quot;) # Look at the table attached lc@data@attributes # Remove the value without any pixel legend_lc &lt;- lc@data@attributes[[1]] %&gt;% filter(COUNT!=0) %&gt;% dplyr::select(-Opacity) # Write the csv write_csv(legend_lc, &quot;/home/shares/scientist/ds-workshop/legend_ak_nlcd_2011.csv&quot;) This is how we reprojected to original data and resampled it to a 50m resolution instead of the 30m. Note that the use the Nearest Neighbor method ngb to compute the new raster as this algorithm keeps the original values lc_3338 &lt;- projectRaster(lc, crs=crs(ak_regions_3338), res=50, method = &#39;ngb&#39;, filename=&quot;/home/shares/scientist/ds-workshop/ak_nlcd_2011_landcover_50m_3338.tif&quot;) 18.5 References Modern Geospatial Data Analysis in R Geocomputation with R Raster analysis in R Spatial data science "],
["finding-help.html", "19 Finding help 19.1 From within R 19.2 Task views 19.3 RStudio Website 19.4 Solving problems 19.5 Communities 19.6 NCEAS", " 19 Finding help 19.1 From within R ?..., help(...) : help for a specific function (you know the name of it) ??..., help.search(...) (or help.search(...,agrep=FALSE) to avoid partial matching): includes packages and partial matching 19.2 Task views Task views (or this view ) Environmetrics Phylogeny/comparative methods High-performance Spatial 19.3 RStudio Website Cheat Sheets: https://rstudio.com/resources/cheatsheets/ (Also from RStudio IDE -&gt; Help -&gt; Cheatsheets) Books: https://rstudio.com/resources/books/ Webinar series: https://resources.rstudio.com/ 19.4 Solving problems There are 2 big categories of errors: The code will not run (aka R is complaining… like a lot) The code is running, but the results are erroneous (the hardest one to solve) 19.4.1 Do it yourself Few strategies: try to reproduce the problem break it down to narrow the problematic piece of code (the degugger can help you) try with a small sample dataset try with a test dataset you built to test edge cases 19.4.2 Search Forums, blogs, … Google :) StackExchange https://stackexchange.com/ and the more known StackOverflow https://stackoverflow.com/ RStudio https://education.rstudio.com/ and its community website https://community.rstudio.com/ RopenSci https://ropensci.org/about/ r-bloggers Twitter: #rstats 19.4.3 Posting a question It is recommended to provide a reproducible example when posting a question on a forum. reprex is a great R package to help you to do so. Most of the time, creating a reproducible example on a data sample will help you to solve your problem on your own 19.5 Communities Friends and colleagues Local user groups: R User Groups, R-ladies on Meetup Universities (e.g. Software Carpentry workshops) 19.6 NCEAS NCEAS computing team is here to advise and support your working group! For data science questions: SciComp@nceas.ucsb.edu For technical questions: help@nceas.ucsb.edu For KNB questions: knb-help@nceas.ucsb.edu "]
]
