--- 
title: "Reproducible Research Techniques for Synthesis"
date: "November 4-8, 2019"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
description: ""
always_allow_html: yes
---

# Reproducible Research Techniques for Synthesis


The research landscape is changing. Researchers are increasingly engaging in collaboration across networks; open science includes not just open publication but also open data, software, and workflows; and technology is evolving in support of this new paradigm. This five-day workshop is designed to help researchers stay abreast of current best practices and initiatives and get started on acquiring good data science skills to maximize their productivity, share their data with the scientific community effectively and efficiently, and benefit from the re-use of their data by others.

## Schedule

```{r schedule, echo=FALSE, out.width = '100%', fig.align = 'center'}
knitr::include_graphics("images/schedule.png")
```

### Code of Conduct

Please note that by participating in an NCEAS activity you agree to abide by our [Code of Conduct](https://www.nceas.ucsb.edu/files/NCEAS_Code-of-Conduct_2019.pdf)



```{r footer_logo, echo=FALSE, out.width = '100%', fig.align = 'center'}
knitr::include_graphics("images/nceas-logo.png")
```


<!--chapter:end:index.Rmd-->


# Thinking preferences

## Learning Objectives

An ice-breaker activity that will provide: 

- Opportunity to get to know fellow participants and trainers 
- An introduction to variation in thinking preferences

## About the Whole Brain Thinking System

Everyone thinks differently. The way individuals think guide the way they work, and the way groups of individuals think quide how teams work. Understanding thinking preferences facilitates effective collaboration and team work.

The Whole Brain Model, developed by Ned Herrmann, builds builds upon our understanding of brain functioning. For example, the left and right hemispheres are associated with different types of information processing and our neocortex and limbic system regulate different functions and behaviours.

![](images/Brain.jpg)

The Herrmann Brain Dominance Instrument (HBDI) provides insight into dominant characteristics based on thinking preferences. There are four major thinking styles that reflect the left cerebral, left limbic, right cerebral and right limbic.

- Analytical (Blue)
- Practical (Green)
- Relational (Red)
- Experimental (Yellow)

![](images/WholeBrain.jpg)

These four thinking styles are characterized by different traits. Those in the BLUE quadrant have a strong logical and rational side. They analyze information and may be technical in their approach to problems. They are interested in the 'what' of a situation. Those in the GREEN quadrant have a strong organizational and sequential side. They like to plan details and are methodical in their approach. They are interested in the 'when' of a situation.  The RED quadrant includes those that are feelings-based in their apporach. They have strong interpersonal skills and are good communicators. They are interested in the 'who' of a situation. Those in the YELLOW quadrant are ideas people. They are imaginative, conceptual thinkers that explore outside the box. Yellows are interested in the 'why' of a situation.  

![](images/WholeBrainTraits.jpg)

Most of us identify with thinking styles in more than one quadrant and these different thinking preferences reflect a complex self made up of our rational, theoretical self; our ordered, safekeeping self; our emotional, interpersonal self; and our imaginitive, experimental self.

![](images/ComplexSelf.jpg)

Undertsanding the complexity of how people think and process information helps us understand not only our own approach to problem solving, but also how individuals within a team can contribute. There is great value in diversity of thinking styles within collaborative teams, each type bringing stengths to different aspects of project development.

![](images/WorkingStyles.jpg)

<!--chapter:end:collaboration-thinking-preferences.Rmd-->

# Best Practices: Data and Metadata

## Learning Objectives

In this lesson, you will learn:

- Why preserving computational workflows is important
- How to acheive practical reproducibility
- What are some best practices for data and metadata management

Download slides: [Best Practices: Data and Metadata](files/metadata-data-best-practices.pdf)

## Preserving computational workflows

Preservation enables:

- Understanding
- Evaluation
- Reuse

All for `Future You`!  And your collaborators and colleagues across disciplines.

```{r components01, echo=FALSE, out.width = '50%', fig.align = 'center', fig.cap = 'Scientific products that need to be preserved from computational workflows include data, software, metadata, and other products like graphs and figures.'}
knitr::include_graphics("images/comp-repro.png")
```

While the [Arctic Data Center](https://arcticdata.io), [Knowledge Network for Biocomplexity](https://knb.ecoinformatiocs.org) and similar repositories do
focus on preserving data, we really set our sights much more broadly on preserving
entire computational workflows that are instrumental to advances in science.  A 
computational workflow represents the sequence of computational tasks that are
performed from raw data acquisition through data quality control, integration, 
analysis, modeling, and visualization.

```{r compworkflow01, echo=FALSE, out.width = '100%', fig.align = 'center', fig.cap = 'Computational steps can be organized as a workflow streaming raw data through to derived products.'}
knitr::include_graphics("images/comp-workflow-1.png")
```

In addition, these workflows are often not executed all at once, but rather are
divided into multiple workflows, earch with its own purpose.  For example, a data
acquistion and cleaning workflow often creates a derived and integrated data product
that is then picked up and used by multiple downstream analytical workflows that
produce specific scientific findings.  These workflows can each be archived as 
distinct data packages, with the output of the first workflow becoming the input
of the second and subsequent workflows.

```{r compworkflow02, echo=FALSE, out.width = '100%', fig.align = 'center', fig.cap = 'Computational workflows can be archived and preserved in multiple data packages that are linked by their shared components, in this case an intermediate data file.'}
knitr::include_graphics("images/comp-workflow-2.png")
```

## Best Practices: Overview

- Who Must Submit?
- Organizing Data
- File Formats
- Large Data Packages
- Metadata
- Data Identifiers
- Provenance
- Licensing and Distribution


```{r fjord, echo=FALSE, out.width = '100%', fig.align = 'center'}
knitr::include_graphics("images/fjord.png")
```

## Organizing Data: Best Practices

Both [@borer_simple_2009] and [@white_nine_2013] provide some practical guidelines
for organizing and structuring data.  Critical aspects of their recommendations
include:

- Write scripts for all data manipulation
    - Uncorrected raw data file
    - Document processing in scripts
- Design to add rows, not columns
    - Each column one variable
    - Each row one observation
- Use nonproprietary file formats
    - Descriptive names, no spaces
    - Header line
    

<!--chapter:end:metadata-data-best-practices-intro.Rmd-->

# RStudio and Git/GitHub Setup and Motivation

## Learning Objectives

In this lesson, you will learn:

- What computational reproducibility is and why it is useful
- How version control can increase computational reproducibility
- How to check to make sure your RStudio environment is set up properly for analysis
- How to set up git

## Reproducible Research
Reproducibility is the hallmark of science, which is based on empirical observations 
coupled with explanatory models.  While reproducibility encompasses 
the full science lifecycle, and includes issues such as methodological consistency and
treatment of bias, in this course we will focus on **computational reproducibility**: 
the ability to document data, analyses, and models sufficiently for other researchers 
to be able to understand and ideally re-execute the computations that led to 
scientific results and conclusions.

### What is needed for computational reproducibility?

The first step towards addressing these issues is to be able to evaluate the data,
analyses, and models on which conclusions are drawn.  Under current practice, 
this can be difficult because data are typically unavailable, the method sections
of papers do not detail the computational approaches used, and analyses and models
are often conducted in graphical programs, or, when scripted analyses are employed,
the code is not available.

And yet, this is easily remedied.  Researchers can achieve computational 
reproducibility through open science approaches, including straightforward steps 
for archiving data and code openly along with the scientific workflows describing 
the provenance of scientific results (e.g., @hampton_tao_2015, @munafo_manifesto_2017).

### Conceptualizing workflows

Scientific workflows encapsulate all of the steps from data acquisition, cleaning,
transformation, integration, analysis, and visualization.  

![](images/workflow.png)

Workflows can range in detail from simple flowcharts 
to fully executable scripts. R scripts and python scripts are a textual form 
of a workflow, and when researchers publish specific versions of the scripts and 
data used in an analysis, it becomes far easier to repeat their computations and 
understand the provenance of their conclusions.

## Why use git?

### The problem with filenames

![](images/phd_comics_final.png)

Every file in the scientific process changes.  Manuscripts are edited.
Figures get revised.  Code gets fixed when problems are discovered.  Data files
get combined together, then errors are fixed, and then they are split and 
combined again. In the course of a single analysis, one can expect thousands of
changes to files.  And yet, all we use to track this are simplistic *filenames*.  
You might think there is a better way, and you'd be right: __version control__.

Version control systems help you track all of the changes to your files, without
the spaghetti mess that ensues from simple file renaming.  In version control systems
like `git`, the system tracks not just the name of the file, but also its contents,
so that when contents change, it can tell you which pieces went where.  It tracks
which version of a file a new version came from.  So its easy to draw a graph
showing all of the versions of a file, like this one:

![](images/version-graph.png)

Version control systems assign an identifier to every version of every file, and 
track their relationships. They also allow branches in those versions, and merging
those branches back into the main line of work.  They also support having 
*multiple copies* on multiple computers for backup, and for collaboration.
And finally, they let you tag particular versions, such that it is easy to return 
to a set of files exactly as they were when you tagged them.  For example, the 
exact versions of data, code, and narrative that were used when a manuscript was 
submitted might be `R2` in the graph above.

## Checking the RStudio environment

### R Version

We will use R version 3.6.1, which you can download and install from [CRAN](https://cran.rstudio.com). To check your version, run this in your RStudio console:

```{r rversion, eval=FALSE}
R.version$version.string
```

### RStudio Version

We will be using RStudio version 1.2.500 or later, which you can download and install [here](https://www.rstudio.com/products/rstudio/download/) To check your RStudio version, run the following in your RStudio console:

```{r rstudio-version, eval=FALSE}
RStudio.Version()$version
```

If the output of this does not say `1.2.500` or higher, you should update your RStudio. Do this by selecting Help -> Check for Updates and follow the prompts.

### Package installation

Run the following lines to check that all of the packages we need for the training are installed on your computer.

```{r package-install, eval = FALSE}
packages <- c("DT", "dataone", "datapack", "devtools", "dplyr", "EML", "ggmap", "ggplot2", "leaflet", "readxl", "tidyr", "scales", "sf", "rmarkdown", "roxygen2", "usethis", "broom", "captioner")

for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } }

rm(packages) #remove variable from workspace

```

If you haven't installed all of the packages, this will automatically start installing them. If they are installed, it won't do anything.

Next, create a new R Markdown (File -> New File -> R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes.

At this point, RStudio and R should be all set up.

## Setting up git

If you haven't already, go to [github.com](http://github.com) and create an account. If you haven't downloaded git already, you can download it [here](https://git-scm.com/downloads).

Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -> Terminal -> New Terminal.

A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter:

```{sh git-name, eval=FALSE}
git config --global user.name "Matt Jones"
```

Next, enter the following line, with the email address you used when you created your account on github.com:

```{sh git-email, eval=FALSE}
git config --global user.email "gitcode@magisa.org"
```

Note that these lines need to be run one at a time.

Finally, check to make sure everything looks correct by entering this line, which will return the options that you have set.

```{sh git-list, eval=FALSE}
git config --global user.name
git config --global user.email
```

### Note for Windows Users

If you get "command not found" (or similar) when you try these steps through the RStudio terminal tab, you may need to set the type of terminal that gets launched by RStudio. Under some git install senerios, the git executable may not be available to the default terminal type.

In addition, some versions of windows have difficults with the command line if you are using an account name with spaces in it (such as "Matt Jones", rather than something like "mbjones").  You may need to use an account name without spaces.

### Updating a previous R installation

**This is useful for users who already have R with some packages installed and need to upgrade R, but don't want to lose packages.** If you have never installed R or any R packages before, you can skip this section.

If you already have R installed, but need to update, and don't want to lose your packages, these two R functions can help you. The first will save all of your packages to a file. The second loads the packages from the file and installs packages that are missing.

Save this script to a file (eg `package_update.R`).

```{r, eval = F}
#' Save R packages to a file. Useful when updating R version
#'
#' @param path path to rda file to save packages to. eg: installed_old.rda
save_packages <- function(path){
    tmp <- installed.packages()
    installedpkgs <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
    save(installedpkgs, file = path)
}

#' Update packages from a file. Useful when updating R version
#' 
#' @param path path to rda file where packages were saved
update_packages <- function(path){
    tmp <- new.env()
    installedpkgs <- load(file = path, envir = tmp)
    installedpkgs <- tmp[[ls(tmp)[1]]]
    tmp <- installed.packages()

    installedpkgs.new <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
    missing <- setdiff(installedpkgs, installedpkgs.new)
    install.packages(missing)
    update.packages()
}
```

Source the file that you saved above (eg: `source(package_update.R)`). Then, run the `save_packages` function.

```{r, eval = F}
save_packages("installed.rda")
```

Then quit R, go to [CRAN](https://cran.rstudio.com), and install the latest version of R. 

Source the R script that you saved above again, and then run:

```{r, eval = F}
update_packages("installed.rda")
```

This should install all of your R packages that you had before you upgraded.


<!--chapter:end:r-intro-rstudio-git-setup-motivation.Rmd-->

# Introduction to R and RMarkdown

## Learning Objectives

In this lesson we will:

- get oriented to the RStudio interface
- work with R in the console
- explore RMarkdown
- be introduced to built-in R functions
- learn to use the help pages

## Introduction and Motivation

There is a vibrant community out there that is collectively developing increasingly easy to use and powerful open source programming tools. The changing landscape of programming is making learning how to code easier than it ever has been. Incorporating programming into analysis workflows not only makes science more efficient, but also more computationally reproducible. In this course, we will use the programming language R, and the accompanying integrated development environment (IDE) RStudio. R is a great language to learn for data-oriented programming because it is widely adopted, user-friendly, and (most importantly) open source!

So what is the difference between R and RStudio? Here is an analogy to start us off. **If you were a chef, R is a knife.** You have food to prepare, and the knife is one of the tools that you'll use to accomplish your task.

And **if R were a knife, RStudio is the kitchen**. RStudio provides a place to do your work! Other tools, communication, community, it makes your life as a chef easier. RStudio makes your life as a researcher easier by bringing together other tools you need to do your work efficiently - like a file browser, data viewer, help pages, terminal, community, support, the list goes on. So it's not just the infrastructure (the user interface or IDE), although it is a great way to learn and interact with your variables, files, and interact directly with git. It's also data science philosophy, R packages, community, and more. So although you can prepare food without a kitchen and we could learn R without RStudio, that's not what we're going to do. We are going to take advantage of the great RStudio support, and learn R and RStudio together.

Something else to start us off is to mention that you are learning a new language here. It's an ongoing process, it takes time, you'll make mistakes, it can be frustrating, but it will be overwhelmingly awesome in the long run. We all speak at least one language; it's a similar process, really. And no matter how fluent you are, you'll always be learning, you'll be trying things in new contexts, learning words that mean the same as others, etc, just like everybody else. And just like any form of communication, there will be miscommunications that can be frustrating, but hands down we are all better off because of it. 

While language is a familiar concept, programming languages are in a different context from spoken languages, but you will get to know this context with time. For example: you have a concept that there is a first meal of the day, and there is a name for that: in English it's "breakfast". So if you're learning Spanish, you could expect there is a word for this concept of a first meal. (And you'd be right: 'desayuno'). **We will get you to expect that programming languages also have words (called functions in R) for concepts as well**. You'll soon expect that there is a way to order values numerically. Or alphabetically. Or search for patterns in text. Or calculate the median. Or reorganize columns to rows. Or subset exactly what you want. We will get you increase your expectations and learn to ask and find what you're looking for.



### Resources

This lesson is a combination of excellent lessons by others. Huge thanks to Julie Lowndes for writing most of this content and letting us build on her material, which in turn was built on Jenny Bryan's materials. I definitely recommend reading through the original lessons and using them as reference:   

[Julie Lowndes' Data Science Training for the Ocean Health Index](http://ohi-science.org/data-science-training/)

- [R, RStudio, RMarkdown](http://ohi-science.org/data-science-training/rstudio.html)
- [Programming in R](http://ohi-science.org/data-science-training/programming.html)


[Jenny Bryan's lectures from STAT545 at UBC](https://stat545-ubc.github.io/)

- [R basics, workspace and working directory, RStudio projects](http://stat545-ubc.github.io/block002_hello-r-workspace-wd-project.html)
- [Basic care and feeding of data in R](http://stat545-ubc.github.io/block006_care-feeding-data.html)

RStudio has great resources as well: 

- [webinars](https://www.rstudio.com/resources/webinars/) 
- [cheatsheets](https://www.rstudio.com/resources/cheatsheets/)

Finally, Hadley Wickham's book R for Data Science is a great resource to get more in depth. 

-[R for Data Science](http://r4ds.had.co.nz/)

Other resources:

- [LaTeX Equation Formatting](https://www.caam.rice.edu/~heinken/latex/symbols.pdf)
- [Base R Cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/05/base-r.pdf)
- [RMarkdown Reference Guide](https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)
- [MATLAB/R Translation Cheat Sheet](http://mathesaurus.sourceforge.net/octave-r.html)


## R at the console

Launch RStudio/R.

![](images/RStudio_IDE.png)

Notice the default panes:

  * Console (entire left)
  * Environment/History (tabbed in upper right)
  * Files/Plots/Packages/Help (tabbed in lower right)

FYI: you can change the default location of the panes, among many other things: [Customizing RStudio](https://support.rstudio.com/hc/en-us/articles/200549016-Customizing-RStudio). 


An important first question: **where are we?** 

If you've just opened RStudio for the first time, you'll be in your Home directory. This is noted by the `~/` at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won't change where you are: even as you click through you'll still be Home: `~/`. 

![](images/RStudio_IDE_homedir.png)


OK let's go into the Console, where we interact with the live R process.

We use R to calculate things for us, so let's do some simple math.

```{r}
3*4
```

You can assign the value of that mathematic operation to a variable, or object, in R. You do this using the assignment operator, `<-`.

Make an assignment and then inspect the object you just created.

```{r}
x <- 3 * 4
x
```
In my head I hear, e.g., "x gets 12".

All R statements where you create objects -- "assignments" -- have this form: `objectName <- value`.  

I'll write it in the console with a hash `#`, which is the way R comments so it won't be evaluated. 
```{r eval = FALSE}
## objectName <- value

## This is also how you write notes in your code to explain what you are doing.
```

Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a [convention for demarcating words](http://en.wikipedia.org/wiki/Snake_case) in names.

```{r}
# i_use_snake_case
# other.people.use.periods
# evenOthersUseCamelCase
```

Make an assignment
```{r}
this_is_a_really_long_name <- 2.5
```

To inspect this variable, instead of typing it, we can press the up arrow key and call your command history, with the most recent commands first. Let's do that, and then delete the assignment: 

```{r}
this_is_a_really_long_name
```

Another way to inspect this variable is to begin typing `this_`...and RStudio will automagically have suggested completions for you that you can select by hitting the tab key, then press return. 

One more:
```{r}
science_rocks <- "yes it does!"
```

You can see that we can assign an object to be a word, not a number. In R, this is called a "string", and R knows it's a word and not a number because it has quotes `" "`. You can work with strings in your data in R pretty easily, thanks to the [`stringr`](http://stringr.tidyverse.org/) and [`tidytext`](https://github.com/juliasilge/tidytext#tidytext-text-mining-using-dplyr-ggplot2-and-other-tidy-tools) packages. We won't talk about strings very much specifically, but know that R can handle text, and it can work with text and numbers together. 

Strings and numbers lead us to an important concept in programming: that there are different "classes" or types of objects. An object is a variable, function, data structure, or method that you have written to your environment. You can see what objects you have loaded by looking in the "environment" pane in RStudio. The operations you can do with an object will depend on what type of object it is. This makes sense! Just like you wouldn't do certain things with your car (like use it to eat soup), you won't do certain operations with character objects (strings), for example.

Try running the following line in your console:

```{r, eval = F}
"Hello world!" * 3
```

What happened? Why?




You may have noticed that when assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:

```{r, purl=FALSE}
weight_kg <- 55    # doesn't print anything
(weight_kg <- 55)  # but putting parenthesis around the call prints the value of `weight_kg`
weight_kg          # and so does typing the name of the object
```

Now that R has `weight_kg` in memory, we can do arithmetic with it. For
instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):

```{r}
2.2 * weight_kg
```

We can also change a variable's value by assigning it a new one:

```{r}
weight_kg <- 57.5
2.2 * weight_kg
```

This means that assigning a value to one variable does not change the values of
other variables.  For example, let's store the animal's weight in pounds in a new
variable, `weight_lb`:

```{r}
weight_lb <- 2.2 * weight_kg
```

and then change `weight_kg` to 100.

```{r}
weight_kg <- 100
```

What do you think is the current content of the object `weight_lb`? 126.5 or 220? Why?

You can also store more than one value in a single object. Storing a series of weights in a single object is a convenient way to perform the same operation on multiple values at the same time. One way to create such an object is the function `c()`, which stands for combine or concatenate.

Here we will create a _vector_ of weights in kilograms, and convert them to pounds, saving the weight in pounds as a new object.

```{r}
weight_kg <- c(55, 25, 12)
weight_kg
```

```{r}
weight_lb <- weight_kg * 2.2
weight_lb
```


### Error messages are your friends

Implicit contract with the computer/scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type.

Remember that this is a language, not unsimilar to English! There are times you aren't understood -- it's going to happen. There are different ways this can happen. Sometimes you'll get an error. This is like someone saying 'What?' or 'Pardon'? Error messages can also be more useful, like when they say 'I didn't understand this specific part of what you said, I was expecting something else'. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. 

<div style="width:400px">
![](images/practicalDev_googleErrorMessage.jpg)
</div>

And also know that there are errors that can creep in more subtly, without an error message right away, when you are giving information that is understood, but not in the way you meant. Like if I'm telling a story about tables and you're picturing where you eat breakfast and I'm talking about data. This can leave me thinking I've gotten something across that the listener (or R) interpreted very differently. And as I continue telling my story you get more and more confused... So write clean code and check your work as you go to minimize these circumstances!

### Logical operators and expressions

A moment about **logical operators and expressions**. We can ask questions about the objects we just made. 

- `==` means 'is equal to'
- `!=` means 'is not equal to'
- `<` means ` is less than'
- `>` means ` is greater than'
- `<=` means ` is less than or equal to'
- `>=` means ` is greater than or equal to'
```{r}
weight_kg == 2
weight_kg >= 30
weight_kg != 5
```

> Shortcuts
You will make lots of assignments and the operator `<-` is a pain to type. Don't be lazy and use `=`, although it would work, because it will just sow confusion later. Instead, utilize **RStudio's keyboard shortcut: Alt + - (the minus sign)**.
Notice that RStudio automagically surrounds `<-` with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces.
RStudio offers many handy [keyboard shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts). Also, Alt+Shift+K brings up a keyboard shortcut reference card.


### Clearing the environment
Now look at the objects in your environment (workspace) -- in the upper right pane. The workspace is where user-defined objects accumulate. 

![](images/RStudio_IDE_env.png)

You can also get a listing of these objects with a few different R commands:

```{r}
objects()
ls()
```

If you want to remove the object named `weight_kg`, you can do this:

```{r}
rm(weight_kg)
```

To remove everything:

```{r}
rm(list = ls())
```

or click the broom in RStudio's Environment pane.

## RMarkdown

Now that we know some basic R syntax, let's learn a little about RMarkdown. You will drive yourself crazy (and fail to generate a reproducible workflow!) running code directly in the console. RMarkdown is really key for collaborative research, so we're going to get started with it early and then use it for the rest of the course. 

An RMarkdown file will allow us to weave markdown text with chunks of R code to be evaluated and output content like tables and plots.

File -> New File -> RMarkdown... -> Document of output format HTML, OK.

<div style="width:300px">
![](images/rstudio_new-rmd-doc-html.png)
</div>

You can give it a Title like "My Project". Then click OK. 

OK, first off: by opening a file, we are seeing the 4th pane of the RStudio console, which is essentially a text editor. This lets us organize our files within RStudio instead of having a bunch of different windows open.

Let's have a look at this file — it's not blank; there is some initial text is already provided for you. Notice a few things about it: 

- There are white and grey sections. R code is in grey sections, and other text is in white. 

![](images/rmarkdown.png)

<br>

Let's go ahead and "Knit HTML" by clicking the blue yarn at the top of the RMarkdown file. When you first click this button, RStudio will prompt you to save this file. Create a new folder for it somewhere that you will be able to find again (such as your Desktop or Documents), and name that folder something you'll remember (like `arctic_training_files`).

<br>

![](images/rmarkdown_side_by_side.png)

What do you notice between the two? 

Notice how the grey **R code chunks** are surrounded by 3 backticks and `{r LABEL}`. These are evaluated and return the output text in the case of `summary(cars)` and the output plot in the case of `plot(pressure)`.

Notice how the code `plot(pressure)` is not shown in the HTML output because of the R code chunk option `echo=FALSE`.

More details...

This RMarkdown file has 2 different languages within it: **R** and **Markdown**. 

We don't know that much R yet, but you can see that we are taking a summary of some data called 'cars', and then plotting. There's a lot more to learn about R, and we'll get into it for the next few days. 

The second language is Markdown. This is a formatting language for plain text, and there are only about 15 rules to know. 

Notice the syntax for:

- **headers** get rendered at multiple levels: `#`, `##`
- **bold**: `**word**`

There are some good [cheatsheets](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet) to get you started, and here is one built into RStudio: Go to Help > Markdown Quick Reference
<br />
<br />

**Important**: note that the hashtag `#` is used differently in Markdown and in R: 

- in R, a hashtag indicates a comment that will not be evaluated. You can use as many as you want: `#` is equivalent to `######`. It's just a matter of style.
- in Markdown, a hashtag indicates a level of a header. And the number you use matters: `#` is a "level one header", meaning the biggest font and the top of the hierarchy. `###` is a level three header, and will show up nested below the `#` and `##` headers.

Learn more: http://rmarkdown.rstudio.com/

### Your Turn

1. In Markdown, Write some italic text, and make a numbered list. And add a few subheaders.
Use the Markdown Quick Reference (in the menu bar: Help > Markdown Quick Reference). 
1. Reknit your html file. 


### Code chunks

OK. Now let's practice with some of those commands.

Create a new chunk in your RMarkdown first in one of these ways: 

- click "Insert > R" at the top of the editor pane
- type by hand 
   \```{r}
   \```
- if you haven't deleted a chunk that came with the new file, edit that one

Now, let's write some R code. 

```
x <- 4*3
x
```

Now, hitting return does not execute this command; remember, it's just a text file. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let's do each of them):

1. copy-paste this line into the console.
1. select the line (or simply put the cursor there), and click 'Run'. This is available from 
    a. the bar above the file (green arrow)
    b. the menu bar: Code > Run Selected Line(s)
    c. keyboard shortcut: command-return
1. click the green arrow at the right of the code chunk

### Your turn

Add a few more commands to your file. Execute them by trying the three ways above. Then, save your R Markdown file. 


## R functions, help pages

So far we've learned some of the basic syntax and concepts of R programming, how to navigate RStudio, and RMarkdown, but we haven't done any complicated or interesting programming processes yet. This is where functions come in!

A function is a way to group a set of commands together to undertake a task in a reusable way. When a function is executed, it produces a return value. We often say that we are "calling" a function when it is executed. Functions can be user defined and saved to an object using the assignment operator, so you can write whatever functions you need, but R also has a mind-blowing collection of built-in functions ready to use. To start, we will be using some built in R functions.

All functions are called using the same syntax: function name with parentheses around what the function needs in order to do what it was built to do. The pieces of information that the function needs to do its job are called arguments. So the syntax will look something like: `result_value <- function_name(argument1 = value1, argument2 = value2, ...)`. 

### A simple example

To take a very simple example, let's look at the `mean()` function. As you might expect, this is a function that will take the mean of a set of numbers. Very convenient!

Let's create our vector of weights again:

```{r}
weight_kg <- c(55, 25, 12)
```

and use the `mean` function to calculate the mean weight.

```{r}
mean(weight_kg)
```

### Getting help

What if you know the name of the function that you want to use, but don't know exactly how to use it? Thankfully RStudio provides an easy way to access the help documentation for functions.

To access the help page for `mean`, enter the following into your console:

```{r, eval = F}
?mean
```

The help pane will show up in the lower right hand corner of your RStudio.

The help page is broken down into sections:

 - Description: An extended description of what the function does.
 - Usage: The arguments of the function(s) and their default values.
 - Arguments: An explanation of the data each argument is expecting.
 - Details: Any important details to be aware of.
 - Value: The data the function returns.
 - See Also: Any related functions you might find useful.
 - Examples: Some examples for how to use the function.


### Your turn

> Exercise: Talk to your neighbor(s) and look up the help file for a function that you know or expect to exist. Here are some ideas: `?getwd()`, `?plot()`,  `min()`, `max()`, `?log()`).

And there's also help for when you only sort of remember the function name: double-questionmark:
```{r, eval=F}
??install 
```

Not all functions have (or require) arguments:
```{r}
date()
```

### Use a function to read a file into R

So far we have learned how to assign values to objects in R, and what a function is, but we haven't quite put it all together yet with real data yet. To do this, we will introduce the function `read.csv`, which will be in the first lines of many of your future scripts. It does exactly what it says, it reads in a csv file to R.

Since this is our irst time using this function, first access the help page for `read.csv`. This has a lot of information in it, as this function has a lot of arguments, and the first one is especially important - we have to tell it what file to look for. Let's get a file!

#### Download a file from the Arctic Data Center

Navigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. [Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X.](http://doi.org/10.18739/A25T3FZ8X), and download the first csv file called "BGchem2008data.csv"

Move this file from your Downloads folder into a place you can more easily find it. I recommend creating a folder called `data` in your previously-created directory `arctic_training_files`, and putting the file there.

Now we have to tell `read.csv` how to find the file. We do this using the `file` argument which you can see in usage section in the help page. In RMarkdown, you can either use absolute paths (which will start with your home directory `~/`) or paths **relative to the location of the RMarkdown.** RStudio and RMarkdown have some great autocomplete capabilities when using relative paths, so we will go that route. Assuming you have moved your file to a folder within `arctic_training_files` called `data`, your `read.csv` call will look like this:

```{r, eval = F}
bg_chem <- read.csv("data/BGchem2008data.csv")
```

You should now have an object of the class `data.frame` in your environment called `bg_chem`. Check your environment pane to ensure this is true.

Note that in the help page there are a whole bunch of arguments that we didn't use in the call above. Some of the arguments in function calls are optional, and some are required. Optional arguments will be shown in the usage section with a `name = value` pair, with the default value shown. If you do not specify a `name = value` pair for that argument in your function call, the function will assume the default value (example: `header = TRUE` for `read.csv`). Required arguments will only show the name of the argument, without a value. Note that the only required argument for `read.csv` is `file`.

You can always specify arguments in `name = value` form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want `file = "data/BGchem2008data.csv"`, since file is the first argument. If we wanted to add another argument, say `stringsAsFactors`, we need to specify it explicitly using the `name = value` pair, since the second argument is `header`. For functions I call often, I use this resolve by position for the first argument or maybe the first two. After that, I always use `name = value`.

Many R users (including myself) will override the default `stringsAsFactors` argument using the following call:

```{r}
bg_chem <- read.csv("data/BGchem2008data.csv", stringsAsFactors = FALSE)
```



## Using `data.frames`

A `data.frame` is a two dimensional data structure in R that mimics spreadsheet behavior. It is a collection of rows and columns of data, where each column has a name and represents a variable, and each row represents a measurement of that variable. When we ran `read.csv`, the object `bg_chem` that we created is a `data.frame`. There are a a bunch of ways R and RStudio help you explore data frames. Here are a few, give them each a try:

- click on the word `bg_chem` in the environment pane
- click on the arrow next to `bg_chem` in the environment pane
- execute `head(bg_chem)` in the console
- execute `View(bg_chem)` in the console

Usually we will want to run functions on individual columns in a `data.frame`. To call a specific column, we use the list subset operator `$`. Say you want to look at the first few rows of the `Date` column only. This would do the trick:

```{r}
head(bg_chem$Date)
```

How about calculating the mean temperature of all the CTD samples?

```{r}
mean(bg_chem$CTD_Temperature)
```

Or, if we want to save this to a variable to use later:

```{r}
mean_temp <- mean(bg_chem$CTD_Temperature)
```

You can also create basic plots using the list subset operator.

```{r}
plot(bg_chem$CTD_Depth, bg_chem$CTD_Temperature)
```

There are many more advancted tools and functions in R that will enable you to make better plots using cleaner syntax, we will cover some of these later in the course. 

### Your Turn

> Exercise: Spend a few minutes exploring this dataset. Try out different functions on columns using the list subset operator and experiment with different plots. 

## Troubleshooting

### My RMarkdown won't knit to PDF

If you get an error when trying to knit to PDF that says your computer doesn't have a LaTeX installation, one of two things is likely happening:

1. Your computer doesn't have LaTeX installed
2. You have an installation of LaTeX but RStudio cannot find it (it is not on the path)

If you already use LaTeX (like to write papers), you fall in the second category. Solving this requires directing RStudio to your installation - and isn't covered here.

If you fall in the first category - you are sure you don't have LaTeX installed - can use the R package `tinytex` to easily get an installation recognized by RStudio, as long as you have administrative rights to your computer.

To install `tinytex` run:

```{r, eval = F}
install.packages("tinytex")
tinytex::install_tinytex()
```

If you get an error that looks like `destination /usr/local/bin not writable`, you need to give yourself permission to write to this directory (again, only possible if you have administrative rights). To do this, run this command in the terminal:


    sudo chown -R `whoami`:admin /usr/local/bin

and then try the above install instructions again. More information about `tinytex` can be found [here](https://yihui.name/tinytex/)

### I just entered a command and nothing is happening

It may be because you didn't complete a command: is there a little `+` in your console? R is saying that it is waiting for you to finish. In the example below, I need to close that parenthesis.

```{r, eval=FALSE}
> x <- seq(1, 10
+ 
```

You can either just type the closing parentheses here and push return, or push the `esc` button twice.

### R says my object is not found

New users will frequently see errors that look like this: `Error in mean(myobject) : object 'myobject' not found`

This means that you do not have an object called `myobject` saved in your environment. The common reasons for this are:

- **typo**: make sure your object name is spelled exactly like what shows up in the console. Remember R is case sensitive.
- **not writing to a variable**: note that the object is only saved in the environment if you use the assignment operator, eg: `myobject <- read.csv(...)`
- **not executing the line in your RMarkdown**: remember that writing a line of code in RMarkdown is not the same as writing in the console, you have to execute the line of code using command + enter, running the chunk, or one of the other ways outlined in the RMarkdown section of this training

## Literate Analysis

RMarkdown is an excellent way to generate literate analysis, and a reproducible workflow. [Here](https://nceas.github.io/sasap-training/materials/reproducible_research_in_r_fairbanks/example-brood-table-analysis.html) is an example of a real analysis workflow written using RMarkdown.

## Exercise

Create an RMarkdown document with some of your own data. If you don't have a good dataset handy, use the example dataset here:

[Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. doi:10.18739/A25T3FZ8X.](http://doi.org/10.18739/A25T3FZ8X)

Your document should contain the following sections:

* Introduction to your dataset
    * Include an external link
* Simple analysis
* Presentation of a result
    * A plot
    * In-line description of results

<!--chapter:end:r-beginning-programming-with-rmarkdown.Rmd-->

# Data Documentation and Publishing 

## Learning Objectives

In this lesson, you will learn:

- About open data archives
- What science metadata is and how it can be used
- How data and code can be documented and published in open data archives

## Data sharing and preservation

![](images/WhyManage.png)

## Data repositories: built for data (and code)

- GitHub is not an archival location
- Dedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare
  + Rich metadata
  + Archival in their mission
- Data papers, e.g., Scientific Data
- List of data repositories: http://re3data.org

![](images/RepoLogos.png)

## Metadata

Metadata are documentation describing the content, context, and structure of 
data to enable future interpretation and reuse of the data.  Generally, metadata
describe who collected the data, what data were collected, when and where it was 
collected, and why it was collected.  

For consistency, metadata are typically structured following metadata content 
standards such as the [Ecological Metadata Language (EML)](https://knb.ecoinformatics.org/software/eml/).
For example, here's an excerpt of the metadata for a sockeye salmon data set:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<eml:eml packageId="df35d.442.6" system="knb" 
    xmlns:eml="eml://ecoinformatics.org/eml-2.1.1">
    <dataset>
        <title>Improving Preseason Forecasts of Sockeye Salmon Runs through 
            Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007</title>
        <creator id="1385594069457">
            <individualName>
                <givenName>Mark</givenName>
                <surName>Willette</surName>
            </individualName>
            <organizationName>Alaska Department of Fish and Game</organizationName>
            <positionName>Fishery Biologist</positionName>
            <address>
                <city>Soldotna</city>
                <administrativeArea>Alaska</administrativeArea>
                <country>USA</country>
            </address>
            <phone phonetype="voice">(907)260-2911</phone>
            <electronicMailAddress>mark.willette@alaska.gov</electronicMailAddress>
        </creator>
        ...
    </dataset>
</eml:eml>
```

That same metadata document can be converted to HTML format and displayed in a much
more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4

![](images/knb-metadata.png)
And as you can see, the whole data set or its components can be downloaded and
reused.

Also note that the repository tracks how many times each file has been downloaded,
which gives great feedback to researchers on the activity for their published data.

## Structure of a data package

Note that the data set above lists a collection of files that are contained within
the data set.  We define a *data package* as a scientifically useful collection of 
data and metadata that a researcher wants to preserve.  Sometimes a data package
represents all of the data from a particular experiment, while at other times it
might be all of the data from a grant, or on a topic, or associated with a paper.
Whatever the extent, we define a data package as having one or more data files,
software files, and other scientific products such as graphs and images, all tied
together with a descriptive metadata document.

![](images/data-package.png)
These data repositories all assign a unique identifier to every version of every
data file, similarly to how it works with source code commits in GitHub.  Those identifiers
usually take one of two forms.  A *DOI* identifier is often assigned to the metadata
and becomes the publicly citable identifier for the package.  Each of the other files
gets an internal identifier, often a UUID that is globally unique.  In the example above,
the package can be cited with the DOI `doi:10.5063/F1F18WN4`.

## DataONE Federation

DataONE is a federation of dozens of data repositories that work together to make their 
systems interoperable and to provide a single unified search system that spans
the repositories.  DataONE aims to make it simpler for researchers to publish
data to one of its member repositories, and then to discover and download that
data for reuse in synthetic analyses.

DataONE can be searched on the web (https://search.dataone.org/), which effectively
allows a single search to find data form the dozens of members of DataONE, rather
than visiting each of the currently 43 repositories one at a time.

![](images/DataONECNs.png)


## Publishing data from the web

Each data repository tends to have its own mechanism for submitting data and
providing metadata.  With repositories like the KNB Data Repository and the
Arctic Data Center, we provide some easy to use web forms for editing and submitting
a data package.  Let's walk through a web submission to see what you might expect.

### Download the data to be used for the tutorial

I've already uploaded the test data package, and so you can access the data here:

- https://dev.nceas.ucsb.edu/view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f

Grab both CSV files, and the R script, and store them in a convenient folder.

![](images/hatfield-knb-01.png)

### Login via ORCID

We will walk through web submission on https://demo.nceas.ucsb.edu, and start 
by logging in with an ORCID account. ORCID provides a common account for sharing
scholarly data, so if you don't have one, you can create one when you are redirected
to ORCID from the *Sign In* button.

![](images/knb-banner.png)

When you sign in, you will be redirected to [orcid.org](https://orcid.org), where you can either provide your existing ORCID credentials, or create a new account.  ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers.  Choose the one that is best suited to your use as a scholarly record, such as your university or agency login.

![](images/orcid-login.png)



### Create and submit the data set

After signing in, you can access the data submission form using the *Submit* button.
Once on the form, upload your data files and follow the prompts to provide the 
required metadata. Required sections are listed with a red asterisk.

#### Click **Add Files** to choose the data files for your package

You can select multiple files at a time to efficiently upload many files.


![](images/editor-knb-01.png)

The files will upload showing a progress indicator.  You can continue editing 
metadata while they upload.

![](images/editor-knb-02.png)

#### Enter Overview information

This includes a descriptive title, abstract, and keywords.

The title is the first way a potential user will get information about your dataset. It should be descriptive but succinct, lack acronyms,
and include some indication of the temporal and geospatial coverage of the data.

The abstract should be sufficently descriptive for a general scientific audience to understand your dataset at a high level. It should provide an overview of the scientific context/project/hypotheses, how this data package fits into the larger context, a synopsis of the experimental or sampling design, and a summary of the data contents.

![](images/editor-knb-03.png)

Keywords, while not required, can help increase the searchability of your dataset, particularly if they come from a semantically defined keyword thesaurus. 

![](images/editor-knb-04.png)

Optionally, you can also enter funding information, including a funding number, which can help link multiple datasets collected under the same grant.

Selecting a distribution license - either [CC-0](https://creativecommons.org/publicdomain/zero/1.0/) or [CC-BY](https://creativecommons.org/licenses/by/4.0/) is required.


![](images/editor-knb-05.png)

#### People Information

Information about the people associated with the dataset is essential to provide 
credit through citation and to help people understand who made contributions to
the product.  Enter information for the following people:

- Creators - **all the people who should be in the citation for the dataset**
- Contacts - one is required, but defaults to the first Creator if omitted
- Principal Investigators
- and any other that are relevant

For each, please strive to provide their [ORCID](https://orcid.org) identifier, which
helps link this dataset to their other scholarly works.

![](images/editor-knb-06.png)

#### Temporal Information

Add the temporal coverage of the data, which represents the time period to which
data apply. 

![](images/editor-knb-09.png)

#### Location Information

The geospatial location that the data were collected is critical for discovery 
and interpretation of the data.  Coordinates are entered in decimal degrees, and
be sure to use negative values for West longitudes.  The editor allows you to 
enter multiple locations, which you should do if you had noncontiguous sampling
locations.  This is particularly important if your sites are separated by large
distances, so that a spatial search will be more precise.

![](images/editor-knb-07.png)

Note that, if you miss fields that are required, they will be highlighted in red
to draw your attention. In this case, for the description, provide a comma-separated 
place name, ordered from the local to global. For example:

- Mission Canyon, Santa Barbara, California, USA

![](images/editor-knb-08.png)



#### Methods

Methods are critical to accurate interpretation and reuse of your data.  The editor 
allows you to add multiple different methods sections, include details of 
sampling methods, experimental design, quality assurance procedures, and computational
techniques and software.  Please be complete with your methods sections, as they
are fundamentally important to reuse of the data.

![](images/editor-knb-10.png)

#### Save a first version with **Submit**

When finished, click the *Submit Dataset* button at the bottom.  
If there are errors or missing fields, they will be highlighted.  
Correct those, and then try submitting again.  When you are successful, you should
see a green banner with a link to the current dataset view.  Click the `X` 
to close that banner, if you want to continue editing metadata.

![](images/editor-knb-11.png)

Success!

### File and variable level metadata 

The final major section of metadata concerns the structure and contents
of your data files. In this case, provide the names and descriptions of
the data contained in each file, as well as details of their internal structure.

For example, for data tables, you'll need the name, label, and definition of 
each variable in your file.  Click the **Describe** button to access a dialog to enter this information.

![](images/editor-knb-12.png)

The **Attributes** tab is where you enter variable (aka attribute) information. In the case of 
tabular data (such as csv files) each column is an attribute, thus there should be one attribute defined for
every column in your dataset. Attribute metadata includes:

- variable name (for programs)
- variable label (for display)



![](images/editor-knb-13.png)

- variable definition (be specific)
- type of measurement

![](images/editor-knb-14.png)

- units & code definitions 

![](images/editor-knb-15.png)

You'll need to add these definitions for every variable (column) in 
the file. When done, click **Done**.

![](images/editor-knb-16.png)

Now the list of data files will show a green checkbox indicating that you have
full described that file's internal structure.  Proceed with the other CSV
files, and then click **Submit Dataset** to save all of these changes.

Note that attribute information is not relevant for data files that do not contain variables, such as the R script in this example. Other examples of data files that might not need attributes are images, pdfs, and non-tabular text documents (such as readme files). The yellow circle in the editor indicates that attributes have not been filled out for a data file, and serves as a warning that they might be needed, depending on the file.

![](images/editor-knb-17.png)

After you get the green success message, you can visit your 
dataset and review all of the information that you provided. If
you find any errors, simply click **Edit** again to make changes.

### Add workflow provenance

Understanding the relationships between files in a package is critically important,
especially as the number of files grows.  Raw data are transformed and integrated
to produce derived data, that are often then used in analysis and visualization code
to produce final outputs.  In DataONE, we support structured descriptions of these
relationships, so one can see the flow of data from raw data to derived to outputs.

You add provenance by navigating to the data table descriptions, and selecting the 
`Add` buttons to link the data and scripts that were used in your computational 
workflow.  On the left side, select the `Add` circle to add an input data source
to the filteredSpecies.csv file.  This starts building the provenance graph to
explain the origin and history of each data object.

![](images/editor-knb-18.png)

The linkage to the source dataset should appear.

![](images/editor-knb-19.png)

Then you can add the link to the source code that handled the conversion
between the data files by clicking on `Add` arrow and selecting the R script:

![](images/editor-knb-20.png)

Select the R script and click "Done."

![](images/editor-knb-21.png)

![](images/editor-knb-22.png)

The diagram now shows the relationships among the data files and the R script, so
click **Submit** to save another version of the package.

![](images/editor-knb-23.png)

Et voilà!  A beatifully preserved data package!


<!--chapter:end:metadata-knb-data-documentation-hands-on.Rmd-->


# Version Control With git and GitHub

## Learning Objectives

In this lesson, you will learn:

- Why __git__ is useful for reproducible analysis
- How to use __git__ to track changes to your work over time
- How to use __GitHub__ to collaborate with others
- How to structure your commits so your changes are clear to others
- How to write effective commit messages

## The problem with filenames

![](images/phd_comics_final.png)

Every file in the scientific process changes.  Manuscripts are edited.
Figures get revised.  Code gets fixed when problems are discovered.  Data files
get combined together, then errors are fixed, and then they are split and 
combined again. In the course of a single analysis, one can expect thousands of
changes to files.  And yet, all we use to track this are simplistic *filenames*.  
You might think there is a better way, and you'd be right: __version control__.

Version control systems help you track all of the changes to your files, without
the spaghetti mess that ensues from simple file renaming.  In version control systems
like `git`, the system tracks not just the name of the file, but also its contents,
so that when contents change, it can tell you which pieces went where.  It tracks
which version of a file a new version came from.  So its easy to draw a graph
showing all of the versions of a file, like this one:

![](images/version-graph.png)

Version control systems assign an identifier to every version of every file, and 
track their relationships. They also allow branches in those versions, and merging
those branches back into the main line of work.  They also support having 
*multiple copies* on multiple computers for backup, and for collaboration.
And finally, they let you tag particular versions, such that it is easy to return 
to a set of files exactly as they were when you tagged them.  For example, the 
exact versions of data, code, and narrative that were used when a manuscript was originally 
submitted might be `eco-ms-1` in the graph above, and then when it was revised and resubmitted,
it was done with tag `eco-ms-2`.  A different paper was started and submitted with tag `dens-ms-1`, showing that you can be working on multiple manuscripts with closely related but not identical sets of code and data being used for each, and keep track of it all.

## Version control and Collaboration using Git and GitHub

First, just what are `git` and GitHub?

- __git__: version control software used to track files in a folder (a repository)
    - git creates the versioned history of a repository
- __GitHub__: web site that allows users to store their git repositories and share them with others

![](images/vc-local-github.png)

## Let's look at a GitHub repository

This screen shows the copy of a repository stored on GitHub,
with its list of files, when the files and directories were last modified, 
and some information on who made the most recent changes.  

![](images/ss3sim-github.png)
If we drill into the 
"commits" for the repository, we can see the history of changes made to all of 
the files.  Looks like `kellijohnson` and `seananderson` were fixing things in
June and July:

![](images/ss3sim-commits.png)

And finally, if we drill into the changes made on June 13, we can see exactly what
was changed in each file:

![](images/ss3sim-diff.png)
Tracking these changes, how they relate to released versions of software and files
is exactly what Git and GitHub are good for.  And we will show how they can really 
be effective for tracking versions of scientific code, figures, and manuscripts
to accomplish a reproducible workflow.

## The Git lifecycle

As a git user, you'll need to understand the basic concepts associated with versioned
sets of changes, and how they are stored and moved across repositories.  Any given
git repository can be cloned so that it exist both locally, and remotely.  But each of
these cloned repositories is simply a copy of all of the files and change history
for those files, stored in git's particular format.  For our purposes, we can consider
a git repository just a folder with a bunch of additional version-related metadata.

In a local git-enabled folder, the folder contains a workspace containing the 
current version of all files in the repository. These working files are linked to
a hidden folder containing the 'Local repository', which contains all of the other
changes made to the files, along with the version metadata.

So, when working with files using git, you can use git commands to indicate specifically
which changes to the local working files should be staged for versioning 
(using the `git add` command), and when to record those changes as a version in
the local repository (using the command `git commit`).

The remaining concepts are involved in synchronizing the changes in your local 
repository with changes in a remote repository.  The `git push` command is used to
send local changes up to a remote repository (possibly on GitHub), and the `git pull`
command is used to fetch changes from a remote repository and merge them into the
local repository.

![](images/git-flowchart.png)

- `git clone`: to copy a whole remote repository to local
- `git add` (stage): notify git to track particular changes
- `git commit`: store those changes as a version
- `git pull`: merge changes from a remote repository to our local repository
- `git push`: copy changes from our local repository to a remote repository
- `git status`: determine the state of all files in the local repository
- `git log`: print the history of changes in a repository

Those seven commands are the majority of what you need to successfully use git.  
But this is all super abstract, so let's explore with some real examples.

## Create a remote repository on GitHub

Let's start by creating a repository on GitHub, then we'll edit some files.

- Log into [GitHub](https://github.com)
- Click the New repository button
- Name it `training-test`
- Create a README.md
- Set the LICENSE to Apache 2.0
- Add a .gitignore file for `R`

![](images/new-repo-github.png)
You've now created your first repository! It has a couple of files that GitHub created
for you, like the README.md file, and the LICENSE file, and the .gitignore file.

![](images/sasap-test-repo.png)

For simple changes to text files, you can make edits right in the GitHub web interface.  For example,
navigate to the `README.md` file in the file listing, and edit it by clicking on the *pencil* icon.
This is a regular Markdown file, so you can just add text, and when done, add a commit message, and 
hit the `Commit changes` button.  

![](images/sasap-test-edit.png)

![](images/sasap-test-commit.png)

Congratulations, you've now authored your first versioned commit.  If you navigate back to 
the GitHub page for the repository, you'll see your commit listed there, as well as the
rendered README.md file.

![](images/sasap-test-displayed.png)
Let's point out a few things about this window.  It represents a view of the repository
that you created, showing all of the files in the repository so far.  For each file,
it shows when the file was last modified, and the commit message that was used to last
change each file.  This is why it is important to write good, descriptive commit
messages.  In addition, the blue header above the file listing shows the most recent
commit, along with its commit message, and its SHA identifer.  That SHA identifier is
the key to this set of versioned changes.  If you click on the SHA identifier (*810f314*), 
it will display the set of changes made in that particular commit.

In the next section we'll use the GitHub URL for the GitHub repository you created 
to `clone` the repository onto your local machine so that you can edit the files 
in RStudio.  To do so, start by copying the GitHub URL, which represents the repository
location:

![](images/sasap-test-clone-url.png)



## Working locally with Git via RStudio

RStudio knows how to work with files under version control with Git, but only if
you are working within an RStudio project folder.  In this next section, we will
clone the repository that you created on GitHub into a local repository as an 
RStudio project.  Here's what we're going to do:

- Create the new project
- Inspect the Git tab and version history
- Commit a change to the README.md file
- Commit the changes that RStudio made
- Inspect the version history
- Add and commit an Rmd file
- Push these changes to GitHub
- View the change history on GitHub

__Create a New Project.__ Start by creating a *New Project...* in R Studio, select the *Version Control* 
option, and paste the GitHub URL that you copied into the field for the 
remote repository *Repository URL*.  While you can name the local copy of the 
repository anything, its typical to use the same name as the GitHub repository to
maintain the correspondence.  You can choose any folder for your local copy, in
my case I used my standard `development` folder.

![](images/rstudio-clone-repo.png)
Once you hit `Create Project, a new RStudio windo will open with all of the files
from the remote repository copied locally.  Depending on how your version of RStudio
is configured, the location and size of the panes may differ, but they should all
be present, including a *Git* tab and the normal *Files* tab listing the files that 
had been created in the remote repository.

![](images/rstudio-sasap-test.png)
You'll note that there is one new file `sasap-test.Rproj`, and three files that we
created earlier on GitHub (`.gitignore`, `LICENSE`, and `README.md`).

In the *Git* tab, you'll note that two files are listed.  This is the status pane
that shows the current modification status of all of the files in the repository.
In this case, the `.gitignore` file is listed as *M* for Modified, and `sasap-test.Rproj` 
is listed with a *? ?* to indicate that the file is untracked.  This means that
git has not stored any versions of this file, and knows nothing about the file.
As you make version control decisions in RStudio, these icons will change to reflect
the current version status of each of the files.

__Inspect the history.__ For now, let's click on the *History* button in the Git tab, which will show the
log of changes that occurred, and will be identical to what we viewed on GitHub.
By clicking on each row of the history, you can see exactly what was added and
changed in each of the two commits in this repository.

![](images/rstudio-history-1.png)
__Commit a README.md change.__ Next let's make a change to the README.md file
in RStudio. Add a new section, with a markdown block like this:

<pre><code>
## Git from RStudio

From within RStudio, we can perform the same versioning actions that we can
in GitHub, and much more.  Plus, we have the natural advantages of the 
programming IDE with code completion and other features to make our work
easier.

- Add files to version control
- Commit changes
- Push commits to GitHub
</code></pre>

Once you save, you'll immediately see the *README.md* file show up in the Git
tab, marks as a modification.  You can select the file in the Git tab, and click
*Diff* to see the differences that you saved (but which are not yet committed to
your local repository).

![](images/rstudio-status-pane.png)

And here's what the newly made changes look like compared to the original file.
New lines are highlighted in green, while removed lines are in red.

![](images/rstudio-diff.png)
__Commit the RStudio changes.__

To commit the changes you made to the README.md file, check the *Staged*
checkbox next to the file (which tells Git which changes you want included in
the commit), then provide a descriptive Commit message, and then click *Commit*.

![](images/rstudio-commit-1.png)

Note that some of the changes in the repository, namely `.gitignore` and 
`sasap-test.Rproj` are still listed as having not been committed.  This means
there are still pending changes to the repository.  You can also see the note
that says:

<code>Your branch is ahead of 'origin/master' by 1 commit.</code>

This means that we have committed 1 change in the local repository, but that 
commit has not yet been pushed up to the `origin` repository, where `origin`
is the typical name for our remote repository on GitHub.  So, let's commit the
remaining project files by staging them and adding a commit message.

![](images/rstudio-commit-2.png)

When finished, you'll see that no changes remain in the *Git* tab, and the repository
is clean.  

__Inspect the history.__ Note that the message now says:

<code>Your branch is ahead of 'origin/master' by 2 commits.</code>

These 2 commits are the two we just made, and have not yet been pushed to GitHub.
By clicking on the *History* button, we can see that there are now a total of
four commits in the local repository (while there had only been two on GitHub).

![](images/rstudio-history-2.png)

__Push these changes to GitHub.__ Now that everything has been changed as desired
locally, you can *push* the changes to GitHub using the *Push* button.  This will
prompt you for your GitHub username and password, and upload the changes, leaving
your repository in a totally clean and synchronized state.  When finished, looking
at the history shows all four commits, including the two that were done on GitHub
and the two that were done locally on RStudio.  

![](images/rstudio-history-3.png)
And note that the labels indicate that both the local repository (`HEAD`) and the
remote repository (`origin/HEAD`) are pointing at the same version in the history.
So, if we go look at the commit history on GitHub, all the commits will be shown
there as well.

![](images/github-history.png)

## On good commit messages

Clearly, good documentation of what you've done is critical to making the version
history of your repository meaningful and helpful.  Its tempting to skip the 
commit message altogether, or to add some stock blurd like 'Updates'.  Its better
to use messages that will be helpful to your future self in deducing not just what
you did, but why you did it.  Also, commit messaged are best understood if they
follow the active verb convention.  For example, you can see that my commit
messages all started with a past tense verb, and then explained what was changed.

While some of the changes we illustrated here were simple and so easily explained
in a short phrase, for more complext changes, its best to provide a more complete
message.  The convention, however, is to always have a short, terse first sentence, 
followed by a more verbose explanation of the details and rationale for the change.
This keeps the high level details readable in the version log.  I can't count the
number of times I've looked at the commit log from 2, 3, or 10 years prior and
been so grateful for diligence of my past self and collaborators.

## Collaboration and conflict free workflows

Up to now, we have been focused on using Git and GitHub for yourself, which is a 
great use. But equally powerful is to share a GitHib repository with other
researchers so that you can work on code, analyses, and models together.  When
working together, you will need to pay careful attention to the state of the 
remote repository to avoid and handle merge conflicts.  A *merge conflict* occurs
when two collaborators make two separate commits that change the same lines of the
same file.  When this happens, git can't merge the changes together automatically,
and will give you back an error asking you to resolve the conflict. Don't be afraid
of merge conflicts, they are pretty easy to handle.  and there are some 
[great](https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/) [guides](https://stackoverflow.com/questions/161813/how-to-resolve-merge-conflicts-in-git).

That said, its truly painless if you can avoid merge conflicts in the first place.
You can minimize conflicts by:

- Ensure that you pull down changes just before you commit
  + Ensures that you have the most recent changes
  + But you may have to fix your code if conflict would have occurred
- Coordinate with your collaborators on who is touching which files
  + You still need to comunicate to collaborate

## Exercise

Use RStudio to add a new RMarkdown file to your `training-test`
repository, build a basic structure for the file, and then save it.

Next, stage and commit the file locally, and push it up to GitHub.

## Advanced topics

There's a lot we haven't covered in this brief tutorial.  There are some great
and much longer tutorials that cover advanced topics, such as:

- Using git on the command line
- Resolving conflicts
- Branching and merging
- Pull requests versus direct contributions for collaboration
- Using .gitignore to protect sensitive data
- GitHub Issues and why they are useful
- and much, much more

- [Try Git](https://try.github.io) is a great interactive tutorial
- Software Carpentry [Version Control with Git](http://swcarpentry.github.io/git-novice/)
- Codecademy [Learn Git](https://www.codecademy.com/learn/learn-git) (some paid)



<!--chapter:end:git-intro.Rmd-->

# Git: Collaboration and Conflict Management

## Learning Objectives

In this lesson, you will learn:

- How to use Git and GitHub to collaborate with colleagues on code
- What typically causes conflicts when collaborating
- Workflows to avoid conflicts
- How to resolve a conflict

## Collaborating with Git

Git is a great tool for working on your own, but even better for working with friends
and colleagues.  Git allows you to work with confidence on your own local copy of files
with the confidence that you will be able to successfully synchronize your changes
with the changes made by others.

The simplest way to collaborate with Git is to use a shared repository on a hosting
service such as [GitHub](https://github.com), and use this shared repository as
the mechanism to move changes from one collaborator to another.  While there are other
more advanced ways to sync git repositories, this "hub and spoke" model works really 
well due to its simplicity.

### Activity: Collaborating with a trusted colleague

*Settings*. Working in pairs, choose one person as the 'Owner' and one as the 'Collaborator'.
Then, have the Owner visit their `arctic-training-repo` repository created earlier,
and visit the *Settings* page, and select the *Collaborators* screen, and add
the username of your Collaborator in the box.

![](images/github-collaborators.png)

Once the collaborator has been added, they should check their email for an invitation
from GitHub, and click on the acceptance link, which will enable them to collaborate
onthe repository.

*Collaborator clone*.  To be able to contribute to a repository, the collaborator
must clone the repository from the Owner's github account. To do this, the Collaborator
should visit the github page for the Owner's repository, and then copy the clone URL.
In R Studio, the Collaborator will create a new project from version control by pasting
this clone URL into the appropriate dialog (see the earlier chapter introducing GitHub).

*Collaborator Edits*. With a clone copied locally, the Collaborator can now make changes to the
`index.Rmd` file in the repository, adding a line or statment somewhere noticeable
near the top.  Save your changes.

*Collaborator commit and push*.  To sync changes, the collaborator will need to add, commit, and
push their changes to the Owner's repository.  But before doing so, its good practice
to `pull` immediately before committing to ensure you have the most recent changes
from the owner.  So, in R Studio's Git tab, first click the "Diff" button to
open the git window, and then press the green "Pull" down arrow button.  This will
fetch any recent changes from the origin repository and merge them.  Next, add
the changed index.Rmd file to be committed by cicking the checkbox next to it, 
type in a commit message, and click 'Commit.'  Once that finishes, then the collaborator can
immediately click 'Push' to send the commits to the Owner's GitHub repository.

![](images/rstudio-commit-push.png)

*Owner pull*. Now, the owner can open their local working copy of the code
in RStudio, and `pull` those changes down to their local copy.

**Congrats, the owner now has your changes!**

*Owner edits, commit, and push*. Next, the owner should do the same. Make changes to
a file in the repository, save it, pull to make sure no new changes have been made
while editing, and then `add`, `commit`, and `push` the Owner changes to GitHub.

*Collaborator pull*.  The collaborator can now `pull` down those owner changes, 
and all copies are once again fully synced.  And you're off to collaborating.

## Merge conflicts

A merge conflict occurs when both the owner and collaborator change the same
lines in the same file without first pulling the changes that the other has made.
This is most easily avoided by good communication about who is working on various
sections of each file, and trying to avoid overlaps.  But sometimes it happens, 
and *git* is there to warn you about potential problems.  And git will not allow
you to overwrite one person's changes to a file with another's changes to the same
file if they were based on the same version.

The main problem with merge conflicts is that, when the Owner and Collaborator
both make changes to the same line of a file, git doesn't know whose changes
take precedence.  You have to tell git whose changes to use for that line.

## How to resolve a conflict

### Abort, abort, abort...

Sometimes you just made a mistake.  When you get a merge conflict, the repository
is placed in a 'Merging' state until you resolve it.  There's a commandline command
to abort doing the merge altogether:

```
git merge --abort
```

Of course, after doing that you stull haven't synced with your collaborator's
changes, so things are still unresolved.  But at least your repository is now
usable on your local machine.

### Checkout

The simplest way to resolve a conflict, given that you know whose version of the
file you want to keep, is to use the commandline `git` program to tell git to
use either *your* changes (the person doing the merge), or *their* changes (the other collaborator).

- keep your collaborators file: `git checkout --theirs conflicted_file.Rmd`
- keep your own file: `git checkout --ours conflicted_file.Rmd`

Once you have run that command, then run `add`, `commit`, and `push` the changes as normal.

### Pull and edit the file

But that requires the commandline. If you want to resolve from RStudio, or if 
you want to pick and choose some of your changes and some of your collaborator's,
then instead you can manually edit and fix the file.  When you `pull`ed the file
with a conflict, git notices that there is a conflict and modifies the file to show
both your own changes and your collaborator's changes in the file.  It also shows the
file in the Git tab with an orange `U` icon, which indicates that the file is `Unmerged`,
and therefore awaiting you help to resolve the conflict. It delimits
these blocks with a series of less than and greater than signs, so they are easy to find:

![](images/rstudio-merge-conflict.png)

To resolve the conficts, simply find all of these blocks, and edit them so that
the file looks how you want (either pick your lines, your collaborators lines,
some combination, or something altogether new), and save. Be sure you removed the
delimiter lines that started with `<<<<<<<`, `=======`, and `>>>>>>>`.

Once you have made those changes, you simply add, commit, and push the files
to resolve the conflict.

## Workflows to avoid merge conflicts

Communicate often.  Tell each other what you are working on.

Pull -> Edit -> Add -> Pull -> Commit -> Push

Pull before every commit, and commit often in small chunks.






<!--chapter:end:git-collaboration-conflicts.Rmd-->

# Publishing Analyses to the Web

## Learning Objectives

In this lesson, you will learn:

- How to use git, GitHub (+Pages), and (R)Markdown to publish an analysis to the web

## Introduction

Sharing your work with others in engaging ways is an important part of the scientific process.
So far in this course, we've introduced a small set of powerful tools for doing open science:

- R and its many packages
- RStudio
- git
- GiHub
- RMarkdown

RMarkdown, in particular, is amazingly powerful for creating scientific reports but, so far, we haven't tapped its full potential for sharing our work with others.

In this lesson, we're going to take an existing GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above.

## A Minimal Example

- Use your existing `nceas-training` repository if you have one
    - If not, Create a new repository on GitHub
    - Initialize the repository on GitHub without any files in it
    - In RStudio,
        - Create a new Project
        - When creating, select the option to create from Version Control -> Git
        - Enter your repository's clone URL in the Repository URL field and fill in the rest of the details
- Add a new file at the top level called `index.Rmd`. The easiest way to do this is through the RStudio menu. Choose File -> New File -> RMarkdown...  This will bring up a dialog box. You should create a "Document" in "HTML" format. These are the default options.  Be sure to use the exact capitalization (lower case 'index') as different operating systems handle capitalization differently and it can interfere with loading your web page later.
- Open `index.Rmd` (if it isn't already open)
- Press Knit
    - Observe the rendered output
    - Notice the new file in the same directory `index.html`.
    - This is our RMarkdown file rendered as HTML (a web page)
- Commit your changes (to both index.Rmd and index.html) and push to GitHub
- Open your web browser to the GitHub.com page for your repository
- Go to Settings > GitHub Pages and turn on GitHub Pages for the `master` branch

Now, the rendered website version of your repo will show up at a special URL.
    
GitHub Pages follows a convention like this:
    
![github pages url pattern](images/github_pages.png)
     
Note that it will no longer be at github.com but github.io
     
- Go to https://{username}.github.io/{repo_name}/ (Note the trailing `/`)
    Observe the awesome rendered output
    
Now that we've successfully published a web page from an RMarkdown document, let's make a change to our RMarkdown document and follow the steps to actually publish the change on the web:

- Go back to our `index.Rmd`
- Delete all the content, except the YAML frontmatter
- Type "Hello world"
- Commit, push
- Go back to https://{username}.github.io/{repo_name}/

## Exercise: Sharing your work

RMarkdown web pages are a great way to share work in progress with your colleagues.  To do so simply requires thinking through your presentation so that it highlights the workflow to be reviewed.  You can also include multiple pages and build a simple web site for walking through your work that is accessible to people who aren't all set up to open your content in R.  In this exercise, we'll publish another RMarkdown page, and create a table of contents on the main page to guide people to the main page.

First, in your trainnig repository, create a new RMarkdown file that describes some piece of your work and note the name.  I'll use an RMarkdown named `data-cleaning.Rmd`.

Once you have an RMarkdown created, `Knit` the document which will create the HTML version of the file, which in this case will be named `data-cleaning.html`.  

Now, return to editing your `index.Rmd` file from the beginning of this lesson.  The index file represents the 'default' file for a web site, and is returned whenever you visit the web site but don't specify an explicit file to be returned.  Let's modify the index page, adding a bulleted list, and in that list, include a link to the new markdown page that we created:

```{markdown mdexample, eval=FALSE}
## Analysis plan

- [Data Cleaning](data-cleaning.html)
- Data Interpolation and Gap filling
- Linear models
- Animal movement models based on telemetry
- Data visualization
```

Commit and push the web page to GitHub.  Now when you visit your web site, you'll see the table of contents, and can navigate to the new data cleaning page.

```{r weboutput, echo=FALSE, out.width = '50%', fig.align = 'left'}
knitr::include_graphics("images/git-index-page.png")
```





<!--chapter:end:git-github-publishing-analysis.Rmd-->

# Collaborating using Git

```{r gitcollab_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objectives

In this lesson, you will learn:

- New mechanisms to collaborate using __git__
- What is a __Pull Request__ in Github
- How top contribute code to colleague's repository using Pull Requests
- What is a __branch__ in git
- How to use a branch to organize code
- What is a __tag__ in git and how is it useful for collaboration


## Pull requests

We've shown in other chapters how to directly collaborate on a repository with
colleagues by granting them `write` privileges as a collaborator to your repository.
This is useful with close collaborators, but also grants them tremendous latitude to
change files and analyses, to remove files from the working copy, and to modify all
files in the repository.  

Pull requests represent a mechanism to more judiciously collaborate, one in which 
a collaborator can suggest changes to a repository, the owner and collaborator can 
discuss those changes in a structured way, and the owner can then review and accept
all or only some of those changes to the repository.  This is useful with open source
code where a community is contributing to shared analytical software, to students in 
a lab working on related but not identical projects, and to others who want the 
capability to review changes as they are submitted.

To use pull requests, the general procedure is as follows.  The collaborator first
creates a `fork` of the owner's repository, which is a cloned copy of the original
that is linked to the original.  This cloned copy is in the collaborator's GitHub
account, which means they ave the ability to make changes to it.  But they don't have
the right to change the original owner's copy.  Pull requests are a mechanism for someone
that has a forked copy of a repository to **request** that the original owner review and
pull in their changes.  This allows them to collaborate, but keeps the owner in control of
exactly what changed.

### Exercise: Create and merge pull requests

In this exercise, work in pairs.  Each pair should create a `fork` of their partner's
training repository, and then clone that onto their local machine.  Then they can make changes
that forked repository, and, from the GitHub interface, create a pull request that the
owner can incorporate.

## Branches and tags

Branches are a mechanism to isolate a set of changes in their own thread, allowing multiple 
types of work to happen in parallel on a repository at the same time.  These are most often
used for trying out experimental work, or for managing bug fixes for historical releases
of software.  Here's an example graph showing a `branch2.1` that has changes in parallel 
to the main branch of development:

![](images/version-graph.png)

The default branch in almost all repositories is called `master`, and it is the
branch that is typically shown in the GitHub interface and elsewhere.
There are many mechanisms to create branches.  The one we will try is 
through RStudio, in which we use the branch dialog to create and switch 
between branches.

### Exercise:

Create a new branch in your training repository called `exp-1`, and then make 
changes to the RMarkdown files in the directory.  Commit and push those changes
to the branch.  Now you can switch between branches using the github interface.

![](images/git-branch-create.png)

<!--chapter:end:git-pull-requests-branches.Rmd-->

# Data Modeling & Tidy Data

## Learning Objectives

- Understand basics of relational data models aka tidy data
- Learn how to design and create effective data tables

## Benefits of relational data systems

- Powerful search and filtering
- Handle large, complex data sets
- Enforce data integrity
- Decrease errors from redundant updates

## Data Organization

![](images/excel-org-01.png)

## Multiple tables

![](images/excel-org-02.png)

## Inconsistent observations

![](images/excel-org-03.png)

## Inconsistent variables

![](images/excel-org-04.png)

## Marginal sums and statistics

![](images/excel-org-05.png)

## Good enough data modeling

### Denormalized data

- Observations about different entities combined

![](images/table-denorm.png)
In the above example, each row has measurements about both the `site` at which observations
occurred, as well as observations of two individuals of possibly different species
found at that site.  This is *not normalized* data.

People often refer to this as *wide* format, because the observations are spread across a
wide number of columns.  Note that, should one encounter a new species in the survey, we
wold have to add new columns to the table.  This is difficult to analyze, understand, and
maintain.

### Tabular data

__Observations__. A better way to model data is to organize the observations about each type of entity in its own table.  This results in:

- Separate tables for each type of entity measured
- Each row represents a single observed entity
- Observations (rows) are all unique

- This is *normalized* data (aka *tidy data*)

__Variables__. In addition, for normalized data, we expect the variables to be organized such that:

- All values in a column are of the same type
- All columns pertain to the same observed entity (e.g., row)
- Each column represents either an identifying variable or a measured variable

Here's an example of tidy (normalized) data in which the top table is the collection
of observations about individuals of several species, and the bottom table are the
observations containing properties of the sites at which the species occurred.

![](images/tables-norm.png)


## Primary and Foreign Keys

When one has normalized data, we often use unique identifiers to reference
particular observations, which allows us to link across tables.  Two types of
identifiers are common within relational data:

- Primary Key: unique identifier for each observed entity, one per row
- Foreign Key: reference to a primary key in another table (linkage)

For example, in the second table below, the `site` column is the *primary key* 
of that table, because it uniquely identifies each row of the table as a unique
observation of a site.  Inthe first table, however, the `site` column is a 
*foreign key* that references the primary key from the second table.  This linkage
tells us that the first height measurement for the `DAPU` observation occurred
at the site with the name `Taku`.

![](images/tables-keys.png)

## Entity-Relationship Model (ER)

An Entity-Relationship model allows us to compactly draw the structure of the
tables in a relational database, including the primary and foreign keys in the tables.

![](images/plotobs-diagram.png)

In the above model, one can see that each site in the `SITES` table must have one
or more observations in the `PLOTOBS` table, whereas each `PLOTOBS` has one and 
only one `SITE`.

## Merging data

Frequently, analysis of data will require merging these separately managed tables
back together.  There are multiple ways to join the observations in two tables, based
on how the rows of one table are merged with the rows of the other.

When conceptualizing merges, one can think of two tables, one on the *left* and
one on the *right*. The most common (and often useful) join is when you merge the subset 
of rows that have matches in both the left table and the right table: 
this is called an *INNER JOIN*.  Other types of join are possible as well. 
A *LEFT JOIN* takes all of the rows from the left table, and merges on the data from matching rows in the right table.  Keys that don't match from the left table are still provided with a missing value (na) from the right table.  A *RIGHT JOIN* is the same, except that all of the rows from the right table are included with matching data from the left, or a missing value. Finally, a *FULL OUTER JOIN* includes all data from all rows in both tables, and includes missing values wherever necessary.

![](images/join-diagrams.png)

Sometimes people represent these as Venn diagrams showing which parts of the left and
right tables are included in the results for each join.  These however, miss part of the
story related to where the missing value come from in each result.

![](images/sql-joins.png)

In the figure above, the blue regions show the set of rows that are included in the result.
For the INNER join, the rows returned are all rows in A that have a matching row in B.

## Simple Guidelines for Effective Data

- Design to add rows, not columns
- Each column one type
- Eliminate redundancy
- Uncorrected data file
- Header line
- Nonproprietary formats
- Descriptive names
- No spaces

- [Borer et al. 2009. **Some Simple Guidelines for Effective Data Management.** Bulletin of the Ecological Society of America.](http://matt.magisa.org/pubs/borer-esa-2009.pdf)
- [White et al. 2013. **Nine simple ways to make it easier to (re)use your data.** Ideas in Ecology and Evolution 6.](https://doi.org/10.4033/iee.2013.6b.6.f)

## Data modeling exercise

- Break into groups, 1 per table

To demonstrate, we'll be working with a tidied up version of a dataset from ADF&G containing commercial catch data from 1878-1997.
The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2.
That site includes metadata describing the full data set, including column definitions.  Here's the first `catch` table:

```{r catch, cache=TRUE, echo=FALSE}
library(DT)
catch <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1", method = "libcurl"),
                  stringsAsFactors = FALSE)
datatable(catch)
```

And here's the `region_defs` table:
```{r regions, cache=TRUE, echo=FALSE}
region_defs <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1", method = "libcurl"),
                        stringsAsFactors = FALSE)
datatable(region_defs)
```

- Draw an ER model for the tables
  - Indicate the primary and foreign keys
- Is the `catch` table in normal (aka tidy) form?
  - If so, what single type of entity was observed?
  - If not, how might you restructure the data table to make it tidy?
    - Draw a new ER diatram showing this re-designed data structure

![](images/ERD_Relationship_Symbols_Quick_Reference-1.png)

## Related resources

- [Borer et al. 2009. **Some Simple Guidelines for Effective Data Management.** Bulletin of the Ecological Society of America.](http://matt.magisa.org/pubs/borer-esa-2009.pdf)
- [White et al. 2013. **Nine simple ways to make it easier to (re)use your data.** Ideas in Ecology and Evolution 6.](https://doi.org/10.4033/iee.2013.6b.6.f)
- [Software Carpentry SQL tutorial](https://swcarpentry.github.io/sql-novice-survey/)
- [Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf)


<!--chapter:end:data-modeling.Rmd-->

# Data Cleaning and Manipulation

## Learning Objectives

In this lesson, you will learn:

- What the Split-Apply-Combine strategy is and how it applies to data
- The difference between wide vs. tall table formats and how to convert between them
- How to use `dplyr` and `tidyr` to clean and manipulate data for analysis
- How to join multiple `data.frame`s together using `dplyr`

## Introduction

The data we get to work with are rarely, if ever, in the format we need to do our analyses.
It's often the case that one package requires data in one format, while another package requires the data to be in another format.
To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models.
The `dplyr` and `tidyr` R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst.

Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy.
This strategy follows a usual set of steps:

- **Split**: Split the data into logical groups (e.g., area, stock, year)
- **Apply:** Calculate some summary statistic on each group (e.g. mean total length by year)
- **Combine:** Combine the groups back together into a single table

![Figure 1: diagram of the split apply combine strategy](images/split-apply-combine-diagram.png)

As shown above (Figure 1), our original table is split into groups by `year`, we calculate the mean length for each group, and finally combine the per-year means into a single table.

`dplyr` provides a fast and powerful way to express this.
Let's look at a simple example of how this is done:

Assuming our length data is already loaded in a `data.frame` called `length_data`:

| year| length_cm|
|----:|---------:|
| 1991|  5.673318|
| 1991|  3.081224|
| 1991|  4.592696|
| 1992|  4.381523|
| 1992|  5.597777|
| 1992|  4.900052|
| 1992|  4.139282|
| 1992|  5.422823|
| 1992|  5.905247|
| 1992|  5.098922|

We can do this calculation using `dplyr` like this:

```{r, eval = FALSE}
length_data %>% 
  group_by(year) %>% 
  summarise(mean_length_cm = mean(length_cm))
```

Another exceedingly common thing we need to do is "reshape" our data.
Let's look at an example table that is in what we will call "wide" format:

| site   | 1990 | 1991 | ... | 1993 |
|--------|------|------|-----|------|
| gold   | 100  | 118  | ... | 112  |
| lake   | 100  | 118  | ... | 112  |
| ...    | ...  | ...  | ... | ...  |
| dredge | 100  | 118  | ... | 112  |

You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year).
Another way of describing this is that there is more than one measurement per row.
This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R.
For example, how would you fit a model with year as a predictor variable?
In an ideal world, we'd be able to just run:

```{r eval = FALSE}
lm(length ~ year)
```

But this won't work on our wide data because `lm` needs `length` and `year` to be columns in our table.

Or how would we make a separate plot for each year?
We could call `plot` one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset.

The `tidyr` package allows us to quickly switch between wide format and what is called tall format using the `pivot_longer` function:

```{r, eval=FALSE}
site_data %>% 
  pivot_longer(-site, names_to = "year", values_to = "length")
```

| site   | year | length |
|--------|------|-------:|
| gold   | 1990 |    101 |
| lake   | 1990 |    104 |
| dredge | 1990 |    144 |
| ...    | ...  |    ... |
| dredge | 1993 |    145 |

In this lesson we're going to walk through the functions you'll most commonly use from the `dplyr` and `tidyr` packages:

- `dplyr`
    - `mutate()`
    - `group_by()`
    - `summarise()`
    - `select()`
    - `filter()`
    - `arrange()`
    - `left_join()`
    - `rename()`
- `tidyr`
    - `pivot_longer()`
    - `pivot_wider()`
    - `extract()`
    - `separate()`

## Setup

Let's start going over the most common functions you'll use from the `dplyr` package.
To demonstrate, we'll be working with a tidied up version of a dataset from ADF&G containing commercial catch data from 1878-1997.
The dataset and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2.

First, let's load `dplyr` and `tidyr`:

```{r, message = F, warning = F}
library(dplyr)
library(tidyr)
```

Then let's read in the data and take a look at it:

```{r, cache=TRUE}
catch_original <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1", method = "libcurl"),
                    stringsAsFactors = FALSE)
head(catch_original)
```

Note: I copied the URL from the Download button on https://knb.ecoinformatics.org/#view/df35b.304.2

This dataset is relatively clean and easy to interpret as-is.
But while it may be clean, it's in a shape that makes it hard to use for some types of analyses so we'll want to fix that first.

## About the pipe (`%>%`) operator

Before we jump into learning `tidyr` and `dplyr`, we first need to explain the `%>%`.

Both the `tidyr` and the `dplyr` packages use the pipe operator - `%>%`, which may look unfamiliar. The pipe is a powerful way to efficiently chain together operations. The pipe will take the output of a previous statement, and use it as the input to the next statement.

Say you want to both `filter` out rows of a dataset, and `select` certain columns. Instead of writing

```
df_filtered <- filter(df, ...)
df_selected <- select(df_filtered, ...)
```

You can write

```
df_cleaned <- df %>% 
              filter(...) %>%
              select(...)
```

If you think of the assignment operator (`<-`) as reading like "gets", then the pipe operator would read like "then."

So you might think of the above chunk being translated as:

The cleaned dataframe gets the original data, and then a filter (of the original data), and then a select (of the filtered data).

The benefits to using pipes are that you don't have to keep track of (or overwrite) intermediate data frames. The drawbacks are that it can be more difficult to explain the reasoning behind each step, especially when many operations are chained together. It is good to strike a balance between writing efficient code (chaining operations), while ensuring that you are still clearly explaining, both to your future self and others, what you are doing and why you are doing it.

RStudio has a keyboard shortcut for `%>%` : Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac).

## Selecting/removing columns: `select()`

The first issue is the extra columns `All` and `notesRegCode`.
Let's select only the columns we want, and assign this to a variable called `catch_data`.

```{r}
catch_data <- catch_original %>% 
  select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum)

head(catch_data)
```

Much better!

`select` also allows you to say which columns you *don't* want, by passing unquoted column names preceded by minus (-) signs:

```{r}
catch_data <- catch_original %>% 
  select(-All, -notesRegCode)

head(catch_data)
```

## Quality Check

Now that we have the data we are interested in using, we should do a little quality check to see that it seems as expected. One nice way of doing this is the `summary` function.

```{r}
summary(catch_data)
```

Notice something seems a bit off? The Chinook catch data are `character` class. Let's fix it using the function `mutate` before moving on.

## Changing column content: `mutate()`

We can use the `mutate` function to change a column, or to create a new column. First Let's try to just convert the Chinook catch values to `numeric` type using the `as.numeric()` function, and overwrite the old Chinook column.

```{r}
catch_clean <- catch_data %>% 
  mutate(Chinook = as.numeric(Chinook))

head(catch_clean)
```

We get a warning "NAs introduced by coercion" which is R telling us that it couldn't convert every value to an integer and, for those values it couldn't convert, it put an `NA` in its place. This is behavior we commonly experience when cleaning datasets and it's important to have the skills to deal with it when it crops up.

To investigate, let's isolate the issue. We can find out which values are NAs with a combination of `is.na()` and `which()`, and save that to a variable called `i`.

```{r}
i <- which(is.na(catch_clean$Chinook))
i
```

It looks like there is only one problem row, lets have a look at it in the original data.

```{r}
catch_data[i,]
```

Well that's odd: The value in `catch_thousands` is `I`.
It turns out that this dataset is from a PDF which was automatically converted into a CSV and this value of `I` is actually a 1.

Let's fix it by incorporating the `ifelse` function to our `mutate` call, which will change the value of the `Chinook` column to 1 if the value is equal to I, otherwise it will leave the column as the same value.

```{r}
catch_clean <- catch_data %>% 
  mutate(Chinook = ifelse(Chinook == "I", 1, Chinook)) %>%
  mutate(Chinook = as.integer(Chinook))

head(catch_clean)
```

## Changing shape: `pivot_longer()` and `pivot_wider()`

The next issue is that the data are in a wide format and, we want the data in a tall format instead.
`pivot_longer()` from the `tidyr` package helps us do just this conversion:

```{r}
catch_long <- catch_clean %>% 
  pivot_longer(cols = -c(Region, Year), names_to = "species", values_to = "catch")

head(catch_long)
```

The syntax we used above for `pivot_longer()` might be a bit confusing so let's walk though it.

The first argument to `pivot_longer` is the columns over which we are pivoting. You can select these by listing either the names of the columns you do want to pivot, or the names of the columns you are not pivoting over. The `names_to` argument takes the name of the column that you are creating from the column **names** you are pivoting over. The `values_to` argument takes the name of the column that you are creating from the **values** in the columns you are pivoting over.

The opposite of `pivot_longer()`, `pivot_wider()`, works in a similar declarative fashion:

```{r}
catch_wide <- catch_long %>% 
  pivot_wider(names_from = species, values_from = catch)

head(catch_wide)
```

## Renaming columns with `rename()`

If you scan through the data, you may notice the values in the `catch` column are very small (these are supposed to be annual catches). If we look at [the metadata](https://knb.ecoinformatics.org/#view/df35b.304.2) we can see that the `catch` column is in thousands of fish so let's convert it before moving on.

Let's first rename the `catch` column to be called `catch_thousands`:

```{r}
catch_long <- catch_long %>% 
  rename(catch_thousands = catch)

head(catch_long)
```

## Adding columns: `mutate()`

Now let's use `mutate` again to create a new column called `catch` with units of fish (instead of thousands of fish).

```{r, eval=FALSE}
catch_long <- catch_long %>% 
  mutate(catch = catch_thousands * 1000)

head(catch_long)
```

Now let's remove the `catch_thousands` column for now since we don't need it. Note that here we have added to the expression we wrote above by adding another function call (mutate) to our expression. This takes advantage of the pipe operator by grouping together a similar set of statements, which all aim to clean up the `catch_long` `data.frame`.

```{r}
catch_long <- catch_long %>% 
  mutate(catch = catch_thousands * 1000) %>% 
  select(-catch_thousands)

head(catch_long)
```

We're now ready to start analyzing the data.

## `group_by` and `summarise`

As I outlined in the Introduction, `dplyr` lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the `group_by()` and `summarise()` functions:

```{r}
mean_region <- catch_long %>% 
  group_by(Region) %>%
  summarise(catch_mean = mean(catch))

head(mean_region)
```

- **Exercise:** Find another grouping and statistic to calculate for each group.
- **Exercise:** Find out if you can group by multiple variables.

Another common use of `group_by()` followed by `summarize()` is to count the number of rows in each group.
We have to use a special function from `dplyr`, `n()`.

```{r}
n_region <- catch_long %>% 
  group_by(Region) %>%
  summarize(n = n())

head(n_region)
```

## Filtering rows: `filter()`

`filter()` is the verb we use to filter our `data.frame` to rows matching some condition.
It's similar to `subset()` from base R.

Let's go back to our original `data.frame` and do some `filter()`ing:

```{r}
SSE_catch <- catch_long %>% 
  filter(Region == "SSE")

head(SSE_catch)
```

- **Exercise:** Filter to just catches of over one million fish.
- **Exercise:** Filter to just SSE Chinook

## Sorting your data: `arrange()`

`arrange()` is how we sort the rows of a `data.frame`.
In my experience, I use `arrange()` in two common cases:

- When I want to calculate a cumulative sum (with `cumsum()`) so row order matters
- When I want to display a table (like in an `.Rmd` document) in sorted order

Let's re-calculate mean catch by region, and then `arrange()` the output by mean catch:

```{r}
mean_region <- catch_long %>% 
  group_by(Region) %>% 
  summarise(mean_catch = mean(catch)) %>% 
  arrange(mean_catch)

head(mean_region)
```

The default sorting order of `arrange()` is to sort in ascending order.
To reverse the sort order, wrap the column name inside the `desc()` function:

```{r}
mean_region <- catch_long %>% 
  group_by(Region) %>% 
  summarise(mean_catch = mean(catch)) %>% 
  arrange(desc(mean_catch))

head(mean_region)
```

## Joins in dplyr

So now that we're awesome at manipulating a single `data.frame`, where do we go from here?
Manipulating **more than one** `data.frame`.

If you've ever used a database, you may have heard of or used what's called a "join", which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two.
We've already covered joins in [Data Modeling & Tidy Data] so let's see how it's done with `dplyr`.

The dataset we're working with, https://knb.ecoinformatics.org/#view/df35b.304.2, contains a second CSV which has the definition of each `Region` code.
This is a really common way of storing auxiliary information about our dataset of interest (catch) but, for analylitcal purposes, we often want them in the same `data.frame`.
Joins let us do that easily. 

Let's look at a preview of what our join will do by looking at a simplified version of our data:

![Visualisation of our `left_join`](images/left_join_catchdata.png)


First, let's read in the region definitions data table and select only the columns we want. Note that I have piped my `read.csv` result into a select call, creating a tidy chunk that reads and selects the data that we need.


```{r, cache=TRUE}
region_defs <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1",
                            method = "libcurl"),
                        stringsAsFactors = FALSE) %>% 
    select(code, mgmtArea)

head(region_defs)
```


If you examine the `region_defs` `data.frame`, you'll see that the column names don't exactly match the image above. If the names of the key columns are not the same, you can explicitly specify which are the key columns in the left and right side as shown below:

```{r}
catch_joined <- left_join(catch_long, region_defs, by = c("Region" = "code"))

head(catch_joined)
```

Notice that I have deviated from our usual pipe syntax (although it does work here!) because I prefer to see the `data.frames` that I am joining side by side in the syntax.

Another way you can do this join is to use `rename` to change the column name `code` to `Region` in the `region_defs` `data.frame`, and run the `left_join` this way:

```{r, eval = F}
region_defs <- region_defs %>% 
  rename(Region = code, Region_Name = mgmtArea)

catch_joined <- left_join(catch_long, region_defs, by = c("Region"))

head(catch_joined)
```



Now our catches have the auxiliary information from the region definitions file alongside them.
Note: `dplyr` provides a complete set of joins: inner, left, right, full, semi, anti, not just left_join.

## `separate()` and `unite()`

`separate()` and its complement, `unite()` allow us to easily split a single column into numerous (or numerous into a single).

This can come in really handle when we need to split a column into two pieces by a consistent separator (like a dash). 

Let's make a new `data.frame` with fake data to illustrate this. Here we have a set of site identification codes. with information about the island where the site is (the first 3 letters) and a site number (the 3 numbers). If we want to group and summarize by island, we need a column with just the island information.

```{r}
sites_df <- data.frame(site = c("HAW-101",
                                "HAW-103",
                                "OAH-320",
                                "OAH-219",
                                "MAI-039"),
                       stringsAsFactors = FALSE)

sites_df %>% 
  separate(site, c("island", "site_number"), "-")
```

- **Exercise:** Split the `city` column in the following `data.frame` into `city` and `state_code` columns:

```{r}
cities_df <- data.frame(city = c("Juneau AK", 
                                 "Sitka AK", 
                                 "Anchorage AK"),
                        stringsAsFactors = FALSE)

# Write your solution here
```

`unite()` does just the reverse of `separate()`. If we have a data.frame that contains columns for year, month, and day, we might want to unite these into a single date column.

```{r}
dates_df <- data.frame(year = c("1930",
                                "1930",
                                "1930"),
                       month = c("12",
                                "12",
                                "12"),
                       day = c("14",
                               "15",
                               "16"),
                       stringsAsFactors = FALSE)

dates_df %>% 
  unite(date, year, month, day, sep = "-")
```

- **Exercise:** Use `unite()` on your solution above to combine the `cities_df` back to its original form with just one column, `city`:

```{r}
# Write your solution here
```

## Summary

We just ran through the various things we can do with `dplyr` and `tidyr` but if you're wondering how this might look in a real analysis.
Let's look at that now:

```{r, catch=TRUE}
catch_original <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1", method = "libcurl"),
                  stringsAsFactors = FALSE)
region_defs <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1", method = "libcurl"),
                        stringsAsFactors = FALSE) %>% 
    select(code, mgmtArea)

mean_region <- catch_original %>%
  select(-All, -notesRegCode) %>% 
  mutate(Chinook = ifelse(Chinook == "I", 1, Chinook)) %>% 
  mutate(Chinook = as.numeric(Chinook)) %>% 
  pivot_longer(-c(Region, Year), names_to = "species", values_to = "catch") %>%
  mutate(catch = catch*1000) %>% 
  group_by(Region) %>% 
  summarize(mean_catch = mean(catch)) %>% 
  left_join(region_defs, by = c("Region" = "code"))

head(mean_region)
```

<!--chapter:end:data-cleaning-manipulation-tidyverse.Rmd-->

# Creating R Functions

Many people write R code as a single, continuous stream of commands, often drawn
from the R Console itself and simply pasted into a script.  While any script 
brings benefits over non-scripted solutions, there are advantages to breaking
code into small, reusable modules.  This is the role of a `function` in R.  In
this lesson, we will review the advantages of coding with functions, practice 
by creating some functions and show how to call them, and then do some exercises
to build other simple functions.

## Learning outcomes

- Learn why we should write code in small functions
- Write code for one or more functions
- Document functions to improve understanding and code communication

## Why functions?

In a word:

- DRY: Don't Repeat Yourself

By creating small functions that only one logical task and do it well, we quickly 
gain:

- Improved understanding
- Reuse via decomposing tasks into bite-sized chunks
- Improved error testing


## Temperature conversion

Imagine you have a bunch of data measured in Fahrenheit and you want to convert
that for analytical purposes to Celsius.  You might have an R script
that does this for you.

```{r}
airtemps <- c(212, 30.3, 78, 32)
celsius1 <- (airtemps[1]-32)*5/9
celsius2 <- (airtemps[2]-32)*5/9
celsius3 <- (airtemps[3]-32)*5/9
```

Note the duplicated code, where the same formula is repeated three times.  This
code would be both more compact and more reliable if we didn't repeat ourselves.

## Creating a function

Functions in R are a mechanism to process some input and return a value.  Similarly
to other variables, functions can be assigned to a variable so that they can be used
throughout code by reference.  To create a function in R, you use the `function` function (so meta!) and assign its result to a variable.  Let's create a function that calculates
celsius temperature outputs from fahrenheit temperature inputs.

```{r}
fahr_to_celsius <- function(fahr) {
  celsius <- (fahr-32)*5/9
  return(celsius)
}
```

By running this code, we have created a function and stored it in R's global environment.  The `fahr` argument to the `function` function indicates that the function we are creating takes a single parameter (the temperature in fahrenheit), and the `return` statement indicates that the function should return the value in the `celsius` variable that was calculated inside the function.  Let's use it, and check if we got the same value as before:

```{r}
celsius4 <- fahr_to_celsius(airtemps[1])
celsius4
celsius1 == celsius4
```

Excellent.  So now we have a conversion function we can use.  Note that, because 
most operations in R can take multiple types as inputs, we can also pass the original vector of `airtemps`, and calculate all of the results at once:

```{r}
celsius <- fahr_to_celsius(airtemps)
celsius
```

This takes a vector of temperatures in fahrenheit, and returns a vector of temperatures in celsius.

## Exercise

Now, create a function named `celsius_to_fahr` that does the reverse, it takes temperature data in celsius as input, and returns the data converted to fahrenheit.  Then use that formula to convert the `celsius` vector back into a vector of fahrenheit values, and compare it to the original `airtemps` vector to ensure that your answers are correct.

```{r}
# Your code goes here
```

Did you encounter any issues with rounding or precision?

## Documenting R functions

Functions need documentation so that we can communicate what they do, and why.  The `roxygen2` package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation.

Documentation in ROxygen is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters `#'`.  Here's a documented version of a function:

```{r}
#' Convert temperature data from Fahrenheit to Celsius
#'
#' @param fahr Temperature data in degrees Fahrenheit to be converted
#' @return temperature value in degrees Celsius
#' @keywords conversion
#' @export
#' @examples
#' fahr_to_celsius(32)
#' fahr_to_celsius(c(32, 212, 72))
fahr_to_celsius <- function(fahr) {
  celsius <- (fahr-32)*5/9
  return(celsius)
}
```

Note the use of the `@param` keyword to define the expectations of input data, and the `@return` keyword for defining the value that is returned from the function.  The `@examples` function is useful as a reminder as to how to use the function.  Finally, the `@export` keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize.

## Summary

- Functions are useful to reduce redundancy, reuse code, and reduce errors
- Build functions with the `function` function
- Document functions with `roxygen2` comments


## Spoiler -- the exercise answered. 

Don't peek until you write your own...

```{r}
# Your code goes here
celsius_to_fahr <- function(celsius) {
    fahr <- (celsius*9/5)+32
    return(fahr)
}

result <- celsius_to_fahr(celsius)
airtemps == result
```


<!--chapter:end:r-creating-functions.Rmd-->

# Creating R Packages

## Learning Objectives

In this lesson, you will learn:

- The advantages of using R packages for organizing code
- Simple techniques for creating R packages
- Approaches to documenting code in packages

## Why packages?

Most R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs.  Most people have never created a package for their own work, and most think the process is too complicated.  Really it's pretty straighforward and super useful in your personal work.  Creating packages serves two main use cases:

- Mechanism to redistribute reusable code (even if just for yourself)
- Mechanism to reproducibly document analysis and models and their results

At a minimum, you can easily produce a package for just your own useful code functions,
which makes it easy to maintain and use utilities that span your own projects.

The `usethis`, `devtools` and `roxygen2` packages make creating and maintining a package to be a straightforward experience.

## Install and load packages

```{r, eval=FALSE}
library(devtools)
library(usethis)
# install.packages("roxygen2")
library(roxygen2)
```

## Create a basic package

Thanks to the great [usethis](https://github.com/r-lib/usethis) package, it only takes one function call to create the skeleton of an R package using `create_package()`.  Which eliminates pretty much all reasons for procrastination.  To create a package called
`mytools`, all you do is:

```{r, eval=FALSE}
setwd('..')
create_package("mytools")
```

    ✔ Setting active project to '/Users/jones/development/mytools'
    ✔ Creating 'R/'
    ✔ Creating 'man/'
    ✔ Writing 'DESCRIPTION'
    ✔ Writing 'NAMESPACE'
    ✔ Writing 'mytools.Rproj'
    ✔ Adding '.Rproj.user' to '.gitignore'
    ✔ Adding '^mytools\\.Rproj$', '^\\.Rproj\\.user$' to '.Rbuildignore'
    ✔ Opening new project 'mytools' in RStudio

This will create a top-level directory structure, including a number of critical files under the [standard R package structure](http://cran.r-project.org/doc/manuals/r-release/R-exts.html#Package-structure).  The most important of which is the `DESCRIPTION` file, which provides metadata about your package. Edit the `DESCRIPTION` file to provide reasonable values for each of the fields,
including your own contact information. 

Information about choosing a LICENSE is provided in the [Extending R](http://cran.r-project.org/doc/manuals/r-release/R-exts.html#Licensing) documentation.
The DESCRIPTION file expects the license to be chose from a predefined list, but
you can use it's various utility methods for setting a specific license file, such
as the `Apacxhe 2` license:

```{r, eval=FALSE}
use_apl2_license(name="Matthew Jones")
```

    ✔ Setting License field in DESCRIPTION to 'Apache License (>= 2.0)'
    ✔ Writing 'LICENSE.md'
    ✔ Adding '^LICENSE\\.md$' to '.Rbuildignore'

Once your license has been chosen, and you've edited your DESCRIPTION file with your contact information, a title, and a description, it will look like this:

```{r, eval=FALSE}
Package: mytools
Title: Utility functions created by Matt Jones
Version: 0.1
Authors@R: "Matthew Jones <jones@nceas.ucsb.edu> [aut, cre]"
Description: Package mytools contains a suite of utility functions useful whenever I need stuff to get done.
Depends: R (>= 3.5.0)
License: Apache License (>= 2.0)
LazyData: true
```


## Add your code

The skeleton package created contains a directory `R` which should contain your source files.  Add your functions and classes in files to this directory, attempting to choose names that don't conflict with existing packages.  For example, you might add a file `environemnt_info.R` that contains a function `environment_info()` that you might want to reuse. This one might leave something to be desired..., but you get the point... The
`usethis::use_r()` function will help set up you files in the right places.  For example, running:

```{r eval=FALSE}
use_r("environment_info")
```

    ● Modify 'R/environment_info.R'

creates the file `R/environment_info.R`, which you can then modify to add the implementation fo the following function:

```{r eval=FALSE}
environment_info <- function(msg) {
    print(devtools::session_info())
    print(paste("Also print the incoming message: ", msg))
}
```

If your R code depends on functions from another package, then you must declare so
in the `Imports` list in the `DESCRIPTION` file for your package.  In our example
above, we depend on the `devtools` package, and so we need to list it as a dependency.
Once again, `usethis` provides a handy helper method:

```{r eval=FALSE}
usethis::use_package("devtools")
```

    ✔ Adding 'devtools' to Imports field in DESCRIPTION
    ● Refer to functions with `devtools::fun()`

## Add documentation

You should provide documentation for each of your functions and classes.  This is done in the `roxygen2` approach of providing embedded comments in the source code files, which are in turn converted into manual pages and other R documentation artifacts.    Be sure to define the overall purpose of the function, and each of its parameters.

```{r}
#' A function to print information about the current environment.
#'
#' This function prints current environment information, and a message.
#' @param msg The message that should be printed
#' @keywords debugging
#' @import devtools
#' @export
#' @examples
#' environment_info("This is an important message from your sponsor.")
environment_info <- function(msg) {
    print(devtools::session_info())
    print(paste("Also print the incoming message: ", msg))
}
```

Once your files are documented, you can then process the documentation using the `document()` function to generate the appropriate .Rd files that your package needs.

```{r, eval = F}
document()
```

    Updating mytools documentation
    Updating roxygen version in /Users/jones/development/mytools/DESCRIPTION
    Writing NAMESPACE
    Loading mytools
    Writing NAMESPACE
    Writing environment_info.Rd

That's really it.  You now have a package that you can `check()` and `install()` and `release()`.  See below for these helper utilities.

## Test your package

You can tests your code using the `tetsthat` testing framework.  The `ussethis::use_testthat()` 
function will set up your package for testing, and then you can use the `use_test()` function
to setup individual test files.  For example, if you want to create tests of our
environment_info functions, set it up like this:

```{r eval = FALSE}
usethis::use_testthat()
```
    ✔ Adding 'testthat' to Suggests field in DESCRIPTION
    ✔ Creating 'tests/testthat/'
    ✔ Writing 'tests/testthat.R'
    
```{r eval = FALSE}
usethis::use_test("environment_info")
```
    ✔ Writing 'tests/testthat/test-environment_info.R'
    ● Modify 'tests/testthat/test-environment_info.R'

You can now add tests to the `test-environment_info.R`, and you can run all of the
tests using `devtools::test()`.  For example, if you add a test to the `test-environment_info.R` file:

```{r eval=FALSE}
test_that("A message is present", {
    capture.output(result <- environment_info("A unique message"))
    expect_match(result, "A unique message")
})
```

Then you can run the tests to be sure all of your functions are working using `devtools::test()`:

```{r eval=FALSE}
devtools::test()
```

    Loading mytools
    Testing mytools
    ✔ | OK F W S | Context
    ✔ |  2       | test-environment_info [0.1 s]
    
    ══ Results ════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════
    Duration: 0.1 s
    
    OK:       2
    Failed:   0
    Warnings: 0
    Skipped:  0

Yay, all tests passed!

## Checking and installing your package

Now that your package is built, you can check it for consistency and completeness using `check()`, and then you can install it locally using `install()`, which needs to be run from the parent directory of your module.

```{r, eval = FALSE}
check()
install()
```

Your package is now available for use in your local environment.

## Sharing and releasing your package

The simplest way to share your package with others is to upload it to a [GitHub repository](https://github.com), which allows others to install your package using the `install_github('mytools','github_username')` function from `devtools`.

If your package might be broadly useful, also consider releasing it to CRAN, using the `release()` method from `devtools(). Releasing a package to CRAN requires a significant amoutn of work to ensure it follows the standards set by the R community, but it is entirely tractable and a valuable contribution to the science community.  If you are considering releasing a package more broadly, you may find that the supportive community at [ROpenSci](https://ropensci.org) provides incredible help and valuable feeback through their onboarding process.

## Exercise

Add temperature conversion functions with full documentation to your package, write tests to ensure the functions work properly, and then
`document()`, `check()`, and `install()` the new version of the package. Don't forget to update the version number before you install!

## More reading

- Hadley Wickham's awesome book: [R Packages](http://r-pkgs.had.co.nz/)
- Thomas Westlake's blog [Writing an R package from scratch](https://r-mageddon.netlify.com/post/writing-an-r-package-from-scratch/)




<!--chapter:end:r-creating-packages.Rmd-->

# Publication Graphics

## Learning Objectives

In this lesson, you will learn:

- The basics of the `ggplot2` package to create static plots
- How to use `ggplot2`'s theming abilities to create publication-grade graphics
- The basics of the `leaflet` package to create interactive maps

## Overview

ggplot2 is a popular package for visualizing data in R.
From the [home page](http://ggplot2.tidyverse.org/):

> ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
It's been around for years and has pretty good documentation and tons of example code around the web (like on [StackOverflow](https://stackoverflow.com/questions/tagged/ggplot2)).
This lesson will introduce you to the basic components of working with ggplot2.

### ggplot vs base vs lattice vs XYZ...

R provides **many** ways to get your data into a plot.
Three common ones are,

- "base graphics" (`plot()`, `hist()`, etc`)
- lattice
- ggplot2
All of them work!
I use base graphics for simple, quick and dirty plots.
I use ggplot2 for most everything else.
ggplot2 excels at making complicated plots easy and easy plots simple enough.


## Setup

First, let's load the packages we'll need:

```{r load_packages, message = F, warning = F}
  library(leaflet)
  library(dplyr)
  library(tidyr)
  library(ggplot2)
  library(DT)
  library(scales) # install.packages("scales")
```



### Load salmon escapement data

You can load the data table directly from the KNB Data Repository, if it isn't already present on your local computer.  This technique only downloads the file if you need it.

```{r read_data, warning = F, message = F, eval = F}
data_url <- "https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e"

esc <- tryCatch(
    read.csv("data/escapement.csv", stringsAsFactors = FALSE),
    error=function(cond) {
        message(paste("Escapement file does not seem to exist, so get it from the KNB."))
        esc <- read.csv(url(data_url, method = "libcurl"), stringsAsFactors = FALSE)
        return(esc)
    }
)

head(esc)
```

```{r, echo = F}
esc <- read.csv("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e", stringsAsFactors = F)
```


Now that we have the data loaded, let's calculate annual escapement by species and region:

```{r mean_esc}
annual_esc <- esc %>% 
  separate(sampleDate, c("Year", "Month", "Day"), sep = "-") %>% 
  mutate(Year = as.numeric(Year)) %>% 
  group_by(Species, SASAP.Region, Year) %>% 
  summarize(escapement = sum(DailyCount)) %>% 
  filter(Species %in% c("Chinook", "Sockeye", "Chum", "Coho", "Pink"))

head(annual_esc)
```

That command used a lot of the dplyr commands that we've used, and some that are new. The `separate` function is used to divide the sampleDate column up into Year, Month, and Day columns, and then we use `group_by` to indicate that we want to calculate our results for the unique combinations of species, region, and year.  We next use `summarize` to calculate an escapement value for each of these groups. Finally, we use a filter and the `%in%` operator to select only the salmon species.


## Static figures using `ggplot2`

Every graphic you make in `ggplot2` will have at least one aesthetic and at least one geom (layer). The aesthetic maps your data to your geometry (layer). Your geometry specifies the type of plot we're making (point, bar, etc.).

Now, let's plot our results using `ggplot`. `ggplot` uses a mapping *aesthetic* (set using `aes()`) and a *geometry* to create your plot. Additional geometries/aesthetics and theme elements can be added to a `ggplot` object using `+`.

```{r plot_esc}
ggplot(annual_esc, aes(x = Species, y = escapement)) +
  geom_col()
```

What if we want our bars to be blue instad of gray? You might think we could run this:

```{r}
ggplot(annual_esc, aes(x = Species, y = escapement, fill = "blue")) +
  geom_col()
```

Why did that happen?

Notice that we tried to set the fill color of the plot inside the mapping aesthetic call. What we have done, behind the scenes, is create a column filled with the word "blue" in our dataframe, and then mapped it to the `fill` aesthetic, which then chose the default fill color of red. 

What we really wanted to do was just change the color of the bars. If we want do do that, we can call the color option in the `geom_bar` function, **outside** of the mapping aesthetics function call.

```{r}
ggplot(annual_esc, aes(x = Species, y = escapement)) +
  geom_col(fill = "blue")
```

What if we did want to map the color of the bars to a variable, such as region.

`ggplot` is really powerful because we can easily get this plot to visualize more aspects of our data.

```{r}
ggplot(annual_esc, aes(x = Species, y = escapement, fill = SASAP.Region)) +
  geom_col()
```

Just like in `dplyr` and `tidyr`, we can also pipe a `data.frame` directly into the first argument of the `ggplot` function using the `%>%` operator.

Let's look at an example using a different geometry. Here, we use the pipe operator to pass in a filtered version of `annual_esc`, and make a line plot with points at each observation.

```{r}
annual_esc %>% 
  filter(SASAP.Region == "Kodiak") %>% 
ggplot(aes(x = Year, y = escapement, color = Species)) + 
    geom_line() +
    geom_point()
```

This can certainly be convenient, especially for cases like the above, but use it carefully! Combining too many data-tidying or subsetting operations with your `ggplot` call can make your code more difficult to debug and understand.

### Setting ggplot themes

Now let's work on making this plot look a bit nicer. Add a title using `ggtitle()`, adjust labels using `ylab()`, and include a built in theme using `theme_bw()`. There are a wide variety of built in themes in ggplot that help quickly set the look of the plot. Use the RStudio autocomplete `theme_` `<TAB>` to view a list of theme functions.

For clarity in the next section, I'll save the filtered version of the annual escapement `data.frame` to it's own object.

```{r}
kodiak_esc <- annual_esc %>% 
  filter(SASAP.Region == "Kodiak")
```


```{r}
ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + 
    geom_line() +
    geom_point() +
    ylab("Escapement") +
    ggtitle("Kodiak Salmon Escapement") +
    theme_bw()
```

You can see that the `theme_bw()` function changed a lot of the aspects of our plot! The background is white, the grid is a different color, etc. The built in theme functions change the default settings for many elements that can also be changed invididually using the`theme()` function. The `theme()` function is a way to further fine-tune the look of your plot. This function takes MANY arguments (just have a look at `?theme`). Luckily there are many great ggplot resources online so we don't have to remember all of these, just google "ggplot cheatsheet" and find one you like.

Let's look at an example of a `theme` call, where we change the position of our plot above from the right side to the bottom, and remove the title from the legend.

```{r}
ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + 
    geom_line() +
    geom_point() +
    ylab("Escapement") +
    ggtitle("Kodiak Salmon Escapement") +
    theme_bw() +
    theme(legend.position = "bottom", legend.title = element_blank())
```


Note that the `theme()` call needs to come after any built in themes like `theme_bw()` are used. Otherwise, `theme_bw()` will likely override any theme elements that you changed using `theme()`.

You can also save the result of a series of `theme()` function calls to an object to use on multiple plots. This prevents needing to copy paste the same lines over and over again!

```{r}
my_theme <- theme_bw() + 
  theme(legend.position = "bottom", legend.title = element_blank())
```

```{r}
ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + 
    geom_line() +
    geom_point() +
    ylab("Escapement") +
    ggtitle("Kodiak Salmon Escapement") +
    my_theme
```

### Smarter tick labels using `scales`

Fixing tick labels in `ggplot` can be really hard! The y-axis labels in the plot above don't look great. We could manually fix them, but it would likely be tedious and error prone.

The `scales` package provides some nice helper functions to easily rescale and relabel your plots. Here, we use `scale_y_continuous` from `ggplot2`, with the argument `labels`, which is assigned to the function name `comma`, from the `scales` package. This will format all of the labels on the y-axis of our plot with comma-formatted numbers.

```{r}
ggplot(kodiak_esc, aes(x = Year, y = escapement, color = Species)) + 
    geom_line() +
    geom_point() +
    scale_y_continuous(labels = comma) +
    ylab("Escapement") +
    ggtitle("Kodiak Salmon Escapement") +
    my_theme
```

### Creating multiple plots

What if we wanted to generate a plot for every region? A fast way to do this uses the function `facet_wrap()`. This function takes a mapping to a variable using the syntax `~variable_name`. The `~` (tilde) is a model operator which tells `facet_wrap` to model each unique value within `variable_name` to a facet in the plot.

The default behaviour of facet wrap is to put all facets on the same x and y scale. You can use the `scales` argument to specify whether to allow different scales between facet plots. Here, we free the y scale. You can also specify the number of columns using the `n_col` argument.

```{r, fig.height = 7, fig.width = 6}
ggplot(annual_esc, aes(x = Year, y = escapement, color = Species)) + 
    geom_line() +
    geom_point() +
    scale_y_continuous(labels = comma) +
    facet_wrap(~SASAP.Region, scales = "free_y", ncol = 2) +
    ylab("Escapement") +
    my_theme
```

## Interactive visualization using `leaflet` and `DT`

### Tables

Now that we know how to make great static visualizations, lets introduce two other packages that allow us to display our data in interactive ways. These packages really shine when used with GitHub pages, so at the end of this lesson we will publish our figures to the website created earlier in the week during [this lesson](publishing-analyses-to-the-web.html).

First let's show an interactive table of unique sampling locations using `DT`.
Write a `data.frame` containing unique sampling locations with no missing values using two new functions from `dplyr` and `tidyr`: `distinct()` and `drop_na()`.

```{r uniq_locations}
locations <- esc %>% 
  distinct(Location, Latitude, Longitude) %>% 
  drop_na()
```

And display it as an interactive table using `datatable()` from the `DT` package.

```{r display_locations}
datatable(locations)
```

### Maps

Similar to `ggplot2`, you can make a basic `leaflet` map using just a couple lines of code. Note that unlike `ggplot2`, the `leaflet` package uses pipe operators (`%>%`) and not the additive operator (`+`).

The `addTiles()` function without arguments will add base tiles to your map from [OpenStreetMap](https://www.openstreetmap.org/). `addMarkers()` will add a marker at each location specified by the latitude and longitude arguments. Note that the `~` symbol is used here to model the coordinates to the map (similar to `facet_wrap` in ggplot).

```{r simple_map}
leaflet(locations) %>% 
  addTiles() %>% 
  addMarkers(lng = ~Longitude, lat = ~Latitude, popup = ~ Location)
```

You can also use `leaflet` to import Web Map Service (WMS) tiles. Here is an example that utilizes the General Bathymetric Map of the Oceans [(GEBCO)](https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/#getcapabilities) WMS tiles. In this example, we also demonstrate how to create a more simple circle marker, the look of which is explicitly set using a series of style-related arguments..

```{r}
leaflet(locations) %>% 
  addWMSTiles("https://www.gebco.net/data_and_products/gebco_web_services/web_map_service/mapserv?",
              layers = 'GEBCO_LATEST',
              attribution = "Imagery reproduced from the GEBCO_2014 Grid, version 20150318, www.gebco.net") %>%
  addCircleMarkers(lng = ~Longitude,
                   lat = ~Latitude,
                   popup = ~ Location,
                   radius = 5,
                   # set fill properties
                   fillColor = "salmon",
                   fillOpacity = 1,
                   # set stroke properties
                   stroke = T,
                   weight = 0.5,
                   color = "white",
                   opacity = 1)
```

Leaflet has a ton of functionality that can enable you to create some beautiful, functional maps with relative ease. [Here](https://pages.github.nceas.ucsb.edu/NCEAS/sasap-data/language_vis.html) is an example of some we created as part of the SASAP project, created using the same tools we showed you here. This map hopefully gives you an idea of how powerful the combination of RMarkdown and GitHub pages can be. 


## Resources

- Lisa Charlotte Rost. (2018) [Why not to use two axes, and what to use instead: The case against dual axis charts](https://blog.datawrapper.de/dualaxis/)

<!--chapter:end:visualisation-ggplot-leaflet.Rmd-->

# Writing Good Data Management Plans

## Learning Objectives

In this lesson, you will learn:

- Why create data management plans
- The major components of data management plans
- Tools that can help create a data management plan
- Festures and functionality of the DMPTool

## When to Plan: The Data Life Cycle

Shown below is one version of the [Data Life Cycle](https://www.dataone.org/data-life-cycle) that was developed by DataONE. The data life cycle provides a high level overview of the stages involved in successful management and preservation of data for use and reuse. Multiple versions of a data life cycle exist with differences attributable to variation in practices across domains or communities. It is not neccesary for researchers to move through the data life cycle in a cylical fashion and some research activities might use only part of the life cycle. For instance, a project involving meta-analysis might focus on the Discover, Integrate, and Analyze steps, while a project focused on primary data collection and analysis might bypass the Discover and Integrate steps. However, 'Plan' is at the top of the data life cycle as it is advisable to initiate your data management planning at the beginning of your research process, before any data has been collected.


![](images/DLC.png)

## Why Plan?

Planning data management in advance povides a number of benefits to the researcher.

- **Saves time and increases efficiency**; Data management planning requires that a researcher think about data handling in advance of data collection, potentially raising any challenges before they are encountered. 
- **Engages your team**; Being able to plan effectively will require conversation with multiple parties, engaging project participants from the outset.
- **Allows you to stay organized**; It will be easier to organize your data for analysis and reuse.
- **Meet funder requirements**; Funders require a data management plan as part of the proposal process.
- **Share data**; Information in the DMP is the foundation for archiving and sharing data with community.

## How to Plan

1) As indicated above, engaging your team is a benefit of data management planning. Collaborators involved in the data collection and processing of your research data bring diverse expertise. Therefore, **plan in collaboration** with these individuals. 
2) Make sure to **plan from the start** to avoid confusion, data loss, and increase efficiency. Given DMPs are a requirement of funding agencies, it is nearly always neccesary to plan from the start. However, the same should apply to research that is being undertaken outside of a specific funded proposal.
3) Make sure to **utilize resources** that are available to assist you in helping to write a good DMP. These might include your institutional library or organization data manager, online resources or education materials such as these.
4) **Use tools** available to you; you don’t have to reinvent the wheel.
5) **Revise your plan** as situations change and you potentially adapt/alter your project. Like your research projects, data management plans are not static, they require changes and updates throughout the research project process.

## What to include in a DMP

If you are writing a data management plan as part of a solicitation proposal, the funding agency will have guidelines for the information they want to be provided in the plan. A good plan will provide information on the study design; data to be collected; metadata; policies for access, sharing & reuse; long-term storage & data management; and budget. 

*A note on Metadata:*
Both basic metadata (such as title and researcher contact information) and comprehensive metadata (such as complete methods of data collection) are critical for accurate interpretation and understanding. The full definitions of variables, especially units, inside each dataset are also critical as they relate to the methods used for creation. Knowing certain blocking or grouping methods, for example, would be necessary to understand studies for proper comparisons and synthesis. 

![](images/DMPSimpleRules.png)
The article [Ten Simple Rules for Creating a Good Data Management Plan](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004525) is a great resource for thinking about writing a data management plan and the information you should include within the plan. The ten simple rules are:

**1. Determine the research sponsor requirements**  
If you are writing your DMP in association with a proposal submission, your funding body will likely have specific requirements for your DMP. If you are creating a DMP for your own research purposes, it may be useful to refer to a generic plan or one from a funder aligned with your domain. 

**2. Identify the data to be collected**  
Consider the types, sources, volume, and data and file formats. 

**3. Define how the data will be organized**  
Will you store you data in excel spreadsheets? csv format? Are your data in a database structure?

**4. Explain how the data will be documented**  
Metadata!! What standard will your be using? How will you create your metadata? 

**5. Describe how quality data will be assured**  
What methods or approaches will be taken to assure data quality? Training activities, instrument calibration, verification tests, double-blind data entry, statistical and visual interpretation are all approaches to error detection.

**6. Present a sound storage & preservation strategy**  
How long will the data be accessible? How will data be stored and protected during the project? How will data be preserved and made available for future use? This is where you would include information on the repository that will preserve your data.

**7. Define the project’s data policies**  
What are your licensing and data sharing arrangements? Do you have any human subject or other sensitive data that requires special consideration?

**8. Describe how the data will be disseminated**  
More active, robust and preferred approaches include: (1) publishing the data in an open repository or archive; (2) publishing the data, metadata, and relevant code as a “data paper”.

**9. Assign roles and responsibilities**  
Roles may include data collection, data entry, QA/QC, metadata creation and management, backup, data preparation and submission to an archive, and systems administration. 

**10. Prepare a realistic budget**  
Review your plan and make sure that there are lines in the budget to support the people that manage the data as well as pay for the requisite hardware, software etc. 

## NSF DMP requirements

In the 2014 Proposal Preparation Instructions, Section J ['Special Information and Supplementary Documentation'](https://www.nsf.gov/pubs/policydocs/pappguide/nsf14001/gpg_2.jsp#IIC2j) NSF put foward the baseline requirements for a data management plan. In addition, there are specific divison and program requirements that provide additional detail. If you are working on a research project with funding that does not require a data management plan, or are developing a plan for unfunded research, the NSF generic requirements are a good set of guidelines to follow.

**Five Sections of the NSF DMP Requirements**

**1. Products of research**  
Types of data, samples, physical collections, software, curriculum materials, other materials produced during project

**2. Data formats and standards**  
Standards to be used for data and metadata format and content (for initial data collection, as well as subsequent storageand processing)

**3. Policies for access and sharing**  
Provisions for appropriate protection of privacy, confidentiality, security, intellectual property, or other rights or requirements

**4. Policies and provisions for re-use**  
Including re-distribution and the production of derivatives

**5. Archiving of data**  
Plans for archiving data, samples, research products and for preservation of access


## Tools in Support of Creating a DMP

![](images/dmptools.jpg)

The [DMP Tool](https://dmptool.org) and [DMP Online](https://dmponline.dcc.ac.uk) are both easy to use web based tools that support the development of a DMP. The tools are partnered and share a code base; the DMPTool incorporates templates from US funding agencies and the DMP Online is focussed on EU requirements.

## Hands-On: Creating a DMP

Go to [https://dmptool.org](https://dmptool.org)

![](images/DMP_1.png)

Click 'get Started' to login. You will have three options. Options 1 and 2 apply if your organization is partnered with the DMP Tool or if you already have an account, option 3 is in order to set up an account. Under Option 1 you will be prompted to search for your organization and can then log-in using your institutional ID.

![](images/DMP_2.png)

Once logged in you will be taken to your DMP dashboard. Here you will find a list of all the plans that are affiliated with your account.

![](images/DMP_3.png)

Before getting started, it is worth taking a look at a couple of resources within the DMPTool that are helpful. These can be found under 'Learn' at the top right.

![](images/DMP_4.png)

The first is the list of Funder Requirements. This details the full set of funder / division / program DMP requirements that have been converted into templates within the tool. For each set of requirements you can download the template to use outside of the tool, review the date of the most recent update, refer to the oringal guidance directly from the funder website and review sample plans. 

![](images/DMP_5.png)

Another place to discover example plans is under the 'Public Plans' section (Learn>Public Plans). Any plan submitted by a user that was marked as public can be found here. No information is provided on whether these plans were associated with a funded proposal, nor any evaluation of the plan quality. However, they are useful to review if writing a DMP for the first time.

![](images/DMP_6.png)

OK, back to your dashboard. To create a new plan, simply click the 'Create Plan' button on the right. You may also click on the words "Create Plan' aover the horizontal line, they go to the same location. 

![](images/DMP_3.png)

You are now in the DMP Tool editor which guides you through a series of questions in order to complete the plan. The first questions connect your plan to your institution and ensure the correct template is being used.

![](images/DMP_7.png)

For the purposes of this workshop, when completing the title, also check the box next to it indicating that this plan is a test. This ensures that the plan does not get included in the DMPTool reporting metrics. The boxes for research organization and funding agency will prompt you for affiliations. If there are multiple plan templates for a given funding agency, another box will pop up asking you to select a template from the drop-down list. For this workshop, we are going to use the NSF: Generic template.

![](images/DMP_8.png)

Click 'Create Plan'. This will take you into the template and you will see five tabs - Project Details, Plan Overview, Write Plan, Share and Download. We will work through these now.

**Project Details:**  Here we can provide more information about the project. For example, an abstract or funder grant number. As with all other fields throughout the tool, required answers are indicated by an asterisk. Note that you can add your ORCiD here. Since you would have logged in with your institutional ID, or created a specific DMPTool account, this field has not been filled automatically.

![](images/DMP_10.png)

You will also notice on the right hand side that you have the option to include guidance from up to six organizaitons. By default, the DMPTool guidance and your institutional affiliation guidance is included. If you have collaborators at other institutions you may choose to add those by selecting 'See the full list'.

![](images/DMP_9.png)

**Plan Overview:**  This page provides a synopsis of the funder template. It's a quick way to view what is going to be required when writing the plan. Note that in this example, there are five sections as we are using the NSF Generic template. You do not need to enter any information on this page and clicking 'Write Plan' will simply take you to the next tab.

![](images/DMP_11.png)


**Write Plan:**  We now see those same five sections as expandable options. These are titled as before and the numbers in parentheses indicate how many questions are contained within each section and how many of those questions have been answered. Because we are just beginning, the first number in each case is 0.

![](images/DMP_12.png)

I want to highlight that this page will look different according to the template that you are using. Different funding agencies / divisions / programs have different requirements for the data management plan. Therefore the template within the DMPTool may have more sections and more questions within each section.  We are working through a simple, generic example. Below is an example for creating a DMP using the NSF Polar Programs Arctic Section template.

![](images/DMP_13.png)

Back to the NSF Generic template.  
Clicking on any + symbol within the boxes will expand to show the editing pane. You will see the full question with a text box below. On the right hand side is a box containing guidance. The tabs here represent the organizations that were selected under the **Project Details** step. They provide links to useful information and guidance.

![](images/DMP_15.png)

Depending on the template that has been selected, there might also be an 'example answer' under the text box. It is not intended that you copy and paste this verbatim. Rather, this is example prose that you can refer to for answering the quesiton. The image below shows one such example when using the NSF Polar Programs Arctic Section template. This is one advantage of signing in through your institution and for institutions to partner with the DMPTool, they can provide specific example language and guidance for their researchers. 
![](images/DMP_14.png)

You will also notice a box for comments on the right hand side. In a moment you will see how collaboration can be managed. However, if you have shared your plan with others, this is where you will be able to see their comments.

There is no requirement for you to answer all questions in one sitting. Completing the plan can require information gathering from multiple sources. Saving the plan at this point does not submit the plan, it simply saves your edits and you can move between sections in any order by expanding the relevant pane.  

![](images/DMP_16.png)

**Share:** Whether you have completed the first draft or not, you may choose to share your plan with others. The 'Share' tab lets you set up visibility preferences and manage collaboration. Under visibility you may opt for:

- Public: Your plan will be viewable by anyone visiting the site under Learn>Public Plans
- Organization: If you have logged in through an institutional ID, others logging in through the same institution can also view your plan
- Private: The plan can only be viewed by you and those you permit. The institutional administrator of the DMPtool account will also be able to view your plan. Typically this is a data librarian and their contact information will be at the top right of the webpage, under your institutional logo.

*Note:* You will notice that the above options are not available to you as you step through this exercise. This is because your plan is a 'test' plan and so will not be posted to the website or managed in the same way. You can go back to your dashboard and unselect 'test' at any point if your test evolves into a real plan.

For a private plan you can assign collaborators to three different roles:

- Co-owner: The collaborator can edit project details, change visibility, and add collaborators. They have the same privilidges as you.
- Editor: The collaborator can comment and make changes. Comments will appear in the tab indicated above.
- Read only: The collaborator can view and comment, but not make changes.

The advantage of collaborating through the tool vs sending emails back and forth is that the tool reflects the most current version at all times, it increases efficiency and keeps all commentary together.

![](images/DMP_17.png)

**Download:** Here you can set your preferences for downloading your plan. *The DMPTool does not submit your plan to your funding agency* and so many researchers choose to download in text or docx formats so that they can make formatting edits to align with the rest of their proposal and meet page length requirements. You can do some of this formatting in advance using the options in this tab. Any plans that are shared publicly are shared in PDF format and include the project details coversheet by default.

![](images/DMP_18.png)

Below shows the base template for downloading your plan (with no content included to date). You can choose to omit unanswered quesitons.

![](images/DMP_19.png)

Finally, you can make changes to your plan at any point by returning to the dashboard. Here you can remove your plan from test status by unchecking the green box. You can also Edit, Share, Download, Copy or Remove your plan using the options in the drop down under 'Actions'.

![](images/DMP_20.png)

<!--chapter:end:data-management-plans.Rmd-->

# Introduction to Shiny

```{r shinysetup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objectives

In this lesson we will:

- review the capabilities in Shiny applications
- learn about the basic layout for Shiny interfaces
- learn about the server component for Shiny applications
- build a simple shiny application for interactive plotting

## Overview

[Shiny](http://shiny.rstudio.com/) is an R package for creating interactive data visualizations embedded in a web application that you and your colleagues can view with just a web browser. Shiny apps are relatively easy to construct, and provide interactive features for letting others share and explore data and analyses.

There are some really great examples of what Shiny can do on the RStudio webite like [this one exploring movie metadata](https://shiny.rstudio.com/gallery/movie-explorer.html). A more scientific example is a tool from the SASAP project exploring [proposal data from the Alaska Board of Fisheries](https://sasap-data.shinyapps.io/board_of_fisheries/).

```{r shinyapp_sasap, echo=FALSE}
knitr::include_graphics("images/shiny-sasap-app.png")
```

Most any kind of analysis and visualization that you can do in R can be turned into a useful interactive visualization for the web that lets people explore your data more intuitively  But, a Shiny application is not the best way to preserve or archive your data.  Instead, for preservation use a repository that is archival in its mission like the [KNB Data Repository](https://knb.ecoinformatics.org), [Zenodo](https://zenodo.org), or [Dryad](https://datadryad.org). This will assign a citable identfier to the specific version of your data, which you can then read in an interactive visualiztion with Shiny.

For example, the data for the Alaska Board of Fisheries application is published on the KNB and is citable as:

Meagan Krupa, Molly Cunfer, and Jeanette Clark. 2017. Alaska Board of Fisheries Proposals 1959-2016. Knowledge Network for Biocomplexity. [doi:10.5063/F1QN652R](https://doi.org/10.5063/F1QN652R). 

While that is the best citation and archival location of the dataset, using Shiny, one can also provide an easy-to-use exploratory web application that you use to make your point that directly loads the data from the archival site.  For example, the Board of Fisheries application above lets people who are not inherently familiar with the data to generate graphs showing the relationships between the variables in the dataset.

We're going to create a simple shiny app with two sliders so we can interactively control inputs to an R function.  These sliders will allow us to interactively control a plot.

## Create a sample shiny application

- File > New > Shiny Web App...
- Set some fields:
![creating a new Shiny app with RStudio](images/shiny-new-app.png)
    - Name it "myapp" or something else
    - Select "Single File"
    - Choose to create it in a new folder called 'shiny-demo'
    - Click Create

RStudio will create a new file called `app.R` that contains the Shiny application.  
Run it by choosing `Run App` from the RStudio editor header bar.  This will bring up
the default demo Shiny application, which plots a histogram and lets you control
the number of bins in the plot.

![](images/shiny-default-app.png)

Note that you can drag the slider to change the number of bins in the histogram.

## Interactive scatterplots

Let's modify this application to plot water biogeochemistry data in a scatterplot, and allow
aspects of the plot to be interactively changed.

### Load data for the example

Navigate to this dataset by Craig Tweedie that is published on the Arctic Data Center. 
[Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center. 
doi:10.18739/A25T3FZ8X.](http://doi.org/10.18739/A25T3FZ8X), and copy the URL for the first csv file 
called "BGchem2008data.csv"

The data can be read into the application using `read.csv()`.  The data contains chemistry measurements at various times and stations.

```{r load_bgchem}
data_url <- "https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A35ad7624-b159-4e29-a700-0c0770419941"
bg_chem <- read.csv(data_url, stringsAsFactors = FALSE)

# Sometimes you need to wrap the web address in url() on some platforms
#bg_chem <- read.csv(url(data_url, method = "libcurl"), stringsAsFactors = FALSE)
names(bg_chem)
```

### Add a simple scatterplot using ggplot

A number of chemistry parameters are sensitive to depth, so let's start by plotting `CTD_Salinitiy` as
a function of `CTD_Depth`.  We do so by switching out the histogram code for a simple ggplot, like so:

```{r server_chunk, eval=FALSE}
server <- function(input, output) {

    output$depthPlot <- renderPlot({

        ggplot(bg_chem, mapping = aes(CTD_Depth, CTD_Salinity)) +
            geom_point(colour="red", size=4) +
            theme_light()
    })
}
```

If you now reload the app, it will display the simple scatterplot instead of
the histogram.  At this point, we haven't added any interactivity.

In a Shiny application, the `server` function provides the part of the application
that creates our interactive components, and returns them to the user interface (`ui`)
to be displayed on the page.

### Add sliders to set the min depth and max depth for the X axis

To make the plot interactive, first we need to modify our user interface to include
widgits that we'll use to control the plot.  Specifically, we will add a new slider
for setting the `mindepth` parameter, and modify the existing slider to be used for
the `maxdepth` parameter.  To do so, modify the `sidebarPanel()` call to include two
`sliderInput()` function calls:

```{r ui_chunk, eval=FALSE}
        sidebarPanel(
            sliderInput("mindepth", "Min depth:", min = 0, max = 500, value = 0),
            sliderInput("maxdepth", "Max depth:", min = 1, max = 500, value = 50)
        )
```

If you reload the app, you'll see two new sliders, but if you change them, they don't
make any changes to the plot. Let's fix that.

### Connect the slider values to the plot

Finally, to make the plot interactive, we can use the `input` and `output` variables
that are passed into the `server` function to access the current values of the sliders.
In Shiny, each UI component is given an input identifier when it is created, which is used as the name of the value in the input list.  So, we can access the minimum depth as `input$mindepth` and
the max as `input$maxdepth`.  Let's use these values now by adding limits to our X axis
in the ggplot:

```{r shiny_ggplot_interactive, eval=FALSE}
        ggplot(bg_chem, mapping = aes(CTD_Depth, CTD_Salinity)) +
            geom_point(colour="red", size=4) +
            xlim(input$mindepth,input$maxdepth) +
            theme_light()
```

At this point, we have a fully interactive plot, and the sliders can be used to change the
min and max of the Depth axis.

![](images/shiny-bgchem-1.png)

Looks so shiny!

### Negative depths?

What happens if a clever user sets the minimum for the X axis at a greater value than the maximum?
You'll see that the direction of the X axis becomes reversed, and the plotted points display right
to left.  This is really an error condition.  Rather than use two independent sliders, we can modify 
the first slider to output a range of values, which will prevent the min from being greater than
the max.  You do so by setting the value of the slider to a vector of length 2, representing 
the default min and max for the slider, such as `value = c(0,100)`.  So, delete the second slider,
rename the first, and provide a vector for the value, like this:

```{r shiny_sidebar, eval=FALSE}
        sidebarPanel(
            sliderInput("depth", "Depth:", min = 0, max = 500, value = c(0,100))
        )
```

Now, modify the ggplot to use this new `depth` slider value, which now will be returned
as a vector of length 2. The first element of the depth vector is the min, and the
second is the max value on the slider.

```{r shiny_limvector, eval=FALSE}
        ggplot(bg_chem, mapping = aes(CTD_Depth, CTD_Salinity)) +
            geom_point(colour="red", size=4) +
            xlim(input$depth[1],input$depth[2]) +
            theme_light()
```

![](images/shiny-bgchem-2.png)

## Shiny architecture

A Shiny application consists of two functions, the `ui` and the `server`.  The `ui`
function is responsible for drawing the web page, while the `server` is responsible 
for any calculations and for creating any dynamic components to be rendered.

Each time that a user makes a change to one of the interactive widgets, the `ui`
grabs the new value (say, the new slider min and max) and sends a request to the
`server` to re-render the output, passing it the new `input` values that the user
had set.  These interactions can sometimes happen on one computer (e.g., if the 
application is running in your local RStudio instance).  Other times, the `ui` runs on
the web browser on one computer, while the `server` runs on a remote computer somewhere
else on the Internet (e.g., if the application is deployed to a web server).

## Extending the user interface with dynamic plots

If you want to display more than one plot in your application, and provide
a different set of controls for each plot, the current layout would be too simple.
Next we will extend the application to break the page up into vertical sections, and
add a new plot in which the user can choose which variables are plotted.  The current
layout is set up such that the `FluidPage` contains the title element, and then
a `sidebarLayout`, which is divided horizontally into a `sidebarPanel` and a 
`mainPanel`.

![](images/shiny-layout-1.png)

### Vertical layout

To extend the layout, we will first nest the existing `sidebarLayout` in a new
`verticalLayout`, which simply flows components down the page vertically.  Then
we will add a new `sidebarLayout` to contain the bottom controls and graph.

![](images/shiny-layout-2.png)

This mechanism of alternately nesting vertical and horizontal panels can be used 
to segment the screen into boxes with rules about how each of the panels is resized,
and how the content flows when the browser window is resized.  The `sidebarLayout`
works to keep the sidebar about 1/3 of the box, and the main panel about 2/3, which 
is a good proportion for our controls and plots.  Add the verticalLayout, and the
second sidebarLayout for the second plot as follows:

```{r shiny_vertical, eval=FALSE}
    verticalLayout(
        # Sidebar with a slider input for depth axis
        sidebarLayout(
            sidebarPanel(
                sliderInput("depth",
                            "Depth:",
                            min = 1,
                            max = 500,
                            value = c(1, 100))
            ),
            # Show a plot of the generated distribution
            mainPanel(
               plotOutput("depthPlot")
            )
        ),

        tags$hr(),

        sidebarLayout(
            sidebarPanel(
                selectInput("x_variable", "X Variable", cols, selected = "CTD_Salinity"),
                selectInput("y_variable", "Y Variable", cols, selected = "d18O"),
                selectInput("color_variable", "Color", cols, selected = "P")
            ),

            # Show a plot with configurable axes
            mainPanel(
                plotOutput("varPlot")
            )
        ),
        tags$hr()
```

Note that the second `sidebarPanel` uses three `selectInput` elements to provide dropdown
menus with the variable columns (`cols`) from our data frame.  To manage that, we need to
first set up the cols variable, which we do by selecting only the numeric variables
from the `bg_chem` data frame, and then saving the name in a variable:

```{r shiny_cols_2, eval=FALSE, echo=TRUE}
bg_chem <- read.csv(url(data_url, method="libcurl"), stringsAsFactors = FALSE) %>%
    select(-Date, -Time, -Station)
cols <- names(bg_chem)
```

Don't forget to add `library(dplyr)` to the library imports at the top.

### Add the dynamic plot

Because we named the second plot `varPlot` in our UI section, we now need to modify
the server to produce this plot.  Its very similar to the first plot, but this time
we want to use the selected vsriables from the user controls to choose which
variables are plotted. These variable names from the `$input` are character
strings, and so would not be recognized as symbols in the `aes` mapping in ggplot.
Instead, we can use `aes_string()` to provide character names for the variables to
be used in the mappings.

```{r shiny_aes_string, eval=FALSE, echo=TRUE}
    output$varPlot <- renderPlot({
        ggplot(bg_chem, mapping = aes_string(x = input$x_variable,
                                             y = input$y_variable,
                                             color = input$color_variable)) +
            geom_point(size=4) +
            scale_color_gradient2(low="midnightblue", 
                                  mid="white", 
                                  high="firebrick", 
                                  midpoint = mean(bg_chem[,input$color_variable])) +
            theme_light()
```

Notice that we also use `scale_color_gradient2` to set up a continuous color 
scale that uses the mean of the chosen color variable as the midpoint of the 
color ramp, which makes the display balance nicely.

### Finishing touches: data citation

Citing the data that we used for this application is the right thing to do, and easy.
You can add arbitrary HTML to the layout using utility functions in the `tags` list.

```{r shiny_citation, eval=FALSE}
    # Application title
    titlePanel("Water biogeochemistry"),
    p("Data for this application are from: "),
    tags$ul(
        tags$li("Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center.",
                tags$a("doi:10.18739/A25T3FZ8X", href="http://doi.org/10.18739/A25T3FZ8X")
        )
    ),
    tags$br(),
    tags$hr(),
```


The final application shows the data citation, the depth plot, and the 
configurable scatterplot in three distinct panels.

![](images/shiny-bgchem-app.png)

## Publishing Shiny applications

Once you've finished your app, you'll want to share it with others. To do so, you need to
publish it to a server that is set up to [handle Shiny apps](https://shiny.rstudio.com/deploy/).  
Your main choices are:

- [shinyapps.io](http://www.shinyapps.io/) (Hosted by RStudio)
    - This is a service offered by RStudio, which is initially free for 5 or fewer apps
      and for limited run time, but has paid tiers to support more demanding apps.  You 
      can deploy your app using a single button push from within RStudio.
- [Shiny server](https://www.rstudio.com/products/shiny/shiny-server/) (On premises)
    - This is an open source server which you can deploy for free on your own hardware.
      It requires more setup and configuration, but it can be used without a fee.
- [RStudio connect](https://www.rstudio.com/products/connect/) (On premises)
    - This is a paid product you install on your local hardware, and that contains the most 
      advanced suite of services for hosting apps and RMarkdown reports.  You can
      publish using a single button click from RStudio.
      
A comparison of [publishing features](https://rstudio.com/products/shiny/shiny-server/) is available from RStudio.

### Publishing to shinyapps.io

The easiest path is to create an account on shinyapps.io, and then configure RStudio to
use that account for publishing.  Instructions for enabling your local RStudio to publish
to your account are displayed when you first log into shinyapps.io:

![](images/shiny-io-account.png)

Once your account is configured locally, you can simply use the `Publish` button from the
application window in RStudio, and your app will be live before you know it!

![](images/shiny-publish.png)

## Summary

Shiny is a fantastic way to quickly and efficiently provide data exploration for your
data and code.  We highly recommend it for its interactivity, but an archival-quality
repository is the best long-term home for your data and products.  In this example, 
we used data drawn directly from the [KNB repository](http://doi.org/10.18739/A25T3FZ8X)
in our Shiny app, which offers both the preservation guarantees of an archive, plus
the interactive data exploration from Shiny.  You can utilize the full power of R and
the tidyverse for writing your interactive applications.

## Full source code for the final application

```{r shinyapp_source, eval=FALSE}
#
# This is a demonstration Shiny web application showing how to build a simple
# data exploration application.
#
library(shiny)
library(ggplot2)
library(dplyr)
library(tidyr)

# Load data from Arctic Data Center
data_url <- "https://arcticdata.io/metacat/d1/mn/v2/object/urn%3Auuid%3A35ad7624-b159-4e29-a700-0c0770419941"
bg_chem <- read.csv(url(data_url, method="libcurl"), stringsAsFactors = FALSE) %>%
    select(-Date, -Time, -Station)
cols <- names(bg_chem)

# Define UI for application that draws two scatter plots
ui <- fluidPage(

    # Application title
    titlePanel("Water biogeochemistry"),
    p("Data for this application are from: "),
    tags$ul(
        tags$li("Craig Tweedie. 2009. North Pole Environmental Observatory Bottle Chemistry. Arctic Data Center.",
                tags$a("doi:10.18739/A25T3FZ8X", href="http://doi.org/10.18739/A25T3FZ8X")
        )
    ),
    tags$br(),
    tags$hr(),

    verticalLayout(
        # Sidebar with a slider input for depth axis
        sidebarLayout(
            sidebarPanel(
                sliderInput("depth",
                            "Depth:",
                            min = 1,
                            max = 500,
                            value = c(1, 100))
            ),
            # Show a plot of the generated distribution
            mainPanel(
               plotOutput("depthPlot")
            )
        ),

        tags$hr(),
        
        # Sidebar with a select inputs to choose variables to plot
        sidebarLayout(
            sidebarPanel(
                selectInput("x_variable", "X Variable", cols, selected = "CTD_Salinity"),
                selectInput("y_variable", "Y Variable", cols, selected = "d18O"),
                selectInput("color_variable", "Color", cols, selected = "P")
            ),

            # Show a plot with configurable axes
            mainPanel(
                plotOutput("varPlot")
            )
        ),
        tags$hr()
    )
)

# Define server logic required to draw both plots
server <- function(input, output) {

    output$depthPlot <- renderPlot({
       ggplot(bg_chem, mapping = aes(x = CTD_Depth, y = CTD_Salinity)) +
            geom_point(size=4, color="firebrick") +
            xlim(input$depth[1],input$depth[2]) +
            theme_light()
    })

    output$varPlot <- renderPlot({
        ggplot(bg_chem, mapping = aes_string(x = input$x_variable,
                                             y = input$y_variable,
                                             color = input$color_variable)) +
            geom_point(size=4) +
            scale_color_gradient2(low="midnightblue", 
                                  mid="white", 
                                  high="firebrick", 
                                  midpoint = mean(bg_chem[,input$color_variable])) +
            theme_light()
    })
}

# Run the application
shinyApp(ui = ui, server = server)
```


## Resources

- [Main Shiny site](http://shiny.rstudio.com/)
- [Official Shiny Tutorial](http://shiny.rstudio.com/tutorial/)

<!--chapter:end:visualisation-shiny.Rmd-->

# Hands On: Clean and Integrate Datasets

## Learning Objectives

In this lesson, you will:

- Clean and integrate two datasets using dplyr and tidyr
- Make use of previously-learned knowledge of dplyr and tidyr

## Outline

In this one block, you will load data from the following two datasets into R,

- Alaska Department of Fish and Game. 2017. Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017. Knowledge Network for Biocomplexity. [doi:10.5063/F1TX3CKH](http://doi.org/10.5063/F1TX3CKH)
- Andrew Munro and Eric Volk. 2017. Summary of Pacific Salmon Escapement Goals in Alaska with a Review of Escapements from 2007 to 2015. Knowledge Network for Biocomplexity. [doi:10.5063/F1TQ5ZRG](http://doi.org/10.5063/F1TQ5ZRG)

and then clean, and integrate them together to answer a research question:

> Are Sockeye salmon escapement goals being met in recent years in Bristol Bay?

Depending on your familiarity with dplyr and tidyr, you will probably want to look up how to do things.
I suggest two strategies:

1. Look back on the [Data Cleaning and Manipulation] lesson
2. Use the official [dplyr documentation](http://dplyr.tidyverse.org/)
3. Once you know what function to use, use R's built-in help by prepending a `?` to the function name and running that (e.g., run `?select` to get help on the `select` function)

## High-level steps

The goal here is for you to have to come up with the functions to do the analysis with minimal guidance.
This is supposed to be hard.
Below is a set of high-level steps you can follow to answer our research question.
After the list is a schematic of the steps in table form which I expect will be useful in guiding your code.

Note: This need not be the exaxct order your code is written in.

1. Load our two datasets
    - Load the escapement goals CSV into R as a `data.frame`
        - Visit https://knb.ecoinformatics.org/#data and search for "escapement goals" and choose the 2007-2015 dataset
        - Click the following dataset:
        
        >  Andrew Munro and Eric Volk. 2017. **Summary of Pacific Salmon Escapement Goals in Alaska with a Review of Escapements from 2007 to 2015**. Knowledge Network for Biocomplexity.
        
        - Right-click and copy address for the file `MandV2016`
    - Load the escapement counts CSV into R as a `data.frame`
        - Visit https://knb.ecoinformatics.org/#data and search for 'oceanak'
        - Click the following dataset:
        
        >  Alaska Department of Fish and Game. 2017. **Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017**. Knowledge Network for Biocomplexity.
                
        - Right-click and copy address for the file `ADFG_firstAttempt_reformatted.csv`
    
2. Clean
    1. Clean the escapement goals dataset
        1. Filter to just the Bristol Bay region and the Sockeye salmon species
        2. Check whether the column types are wrong and fix any issues (Hint: One column has the wrong type)
    2. Clean the escapement counts dataset
        1. Filter to just the Bristol Bay region and the Sockeye salmon species
        2. Filter to just stocks we have escapement goals for
        3. Create new columns for the year, month, and day so we can calculate total escapements by year and stock
      4. Calculate annual total escapements for each stock
3. Integrate
    - Join the escapement goal lower and upper bounds onto the annual total escapement counts (Hint: We don't need all the columns)
4. Analyze
    - Make a table listing annual total escapements and whether they were in the escapement goal range or not
    - Calculate the proportion of years, for each stock, total escapement was within the escapement goal range

### Visual schematic of steps

Make this:

```
        System	  Lower	    Upper	  Initial.Year
 Kvichak River	2000000	 10000000	  2010
  Naknek River	 800000	  2000000	  2015
  Egegik River	 800000	  2000000	  2015
 Ugashik River	 500000	  1400000	  2015
    Wood River	 700000	  1800000	  2015
 Igushik River	 150000	  400000	  2015
Nushagak River   260000	  760000	  2012
Nushagak River   370000	  900000	  2015
```

and then make this:

```
     Location  Year Escapement
 Egegik River  2012    1233900
 Egegik River  2013    1113630
 Egegik River  2014    1382466
 Egegik River  2015    2160792
 Egegik River  2016    1837260
Igushik River  2012     193326
Igushik River  2013     387744
Igushik River  2014     340590
Igushik River  2015     651172
Igushik River  2016     469230
```

and join them together to make this:

```
     Location  Year Escapement  Lower   Upper is_in_range
 Egegik River  2012    1233900 800000 2000000        TRUE
 Egegik River  2013    1113630 800000 2000000        TRUE
 Egegik River  2014    1382466 800000 2000000        TRUE
 Egegik River  2015    2160792 800000 2000000       FALSE
 Egegik River  2016    1837260 800000 2000000        TRUE
Igushik River  2012     193326 150000  400000        TRUE
Igushik River  2013     387744 150000  400000        TRUE
Igushik River  2014     340590 150000  400000        TRUE
Igushik River  2015     651172 150000  400000       FALSE
Igushik River  2016     469230 150000  400000       FALSE
```

## Full solution

Warning: Spoilers!

First we'll load our packages:

```{r}
suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(DT) # Just for display purposes
})
```

Then download our two data files and save them as `data.frame`s:

```{r, cache=TRUE}
# http://doi.org/10.5063/F1TX3CKH
# Search "OceanAK"
esc <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Af119a05b-bbe7-4aea-93c6-85434dcb1c5e", method = "libcurl"), 
                    stringsAsFactors = FALSE)

# http://doi.org/10.5063/F1TQ5ZRG
# Search "escapement goals", choose 2007-2015 dataset
goals <- read.csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/knb.92014.1", method = "libcurl"),
                  stringsAsFactors = FALSE)
```

First, we'll clean up the escapement goals `data.frame` to have just the rows and columns we need and display it:

```{r warning=F}
bb_sockeye_goals <- goals %>% 
  filter(Region == "Bristol Bay", Species == "Sockeye") %>% 
  mutate(Lower = as.integer(Lower), Initial.Year = as.integer(Initial.Year)) %>% 
  select(System, Lower, Upper, Initial.Year) %>% 
  drop_na()

datatable(bb_sockeye_goals)
```

Then we'll clean up and summarize the escapement counts `data.frame`, join the escapement goals `data.frame` onto it, and calculate whether goals have been met:

```{r}
bb_sockeye_escapements <- esc %>% 
  filter(SASAP.Region == "Bristol Bay", 
         Species == "Sockeye",
         Location %in% bb_sockeye_goals$System) %>%
  separate(sampleDate, c("Year", "Month", "Day"), sep = "-") %>% 
  group_by(Location, Year) %>% 
  summarize(Escapement = sum(DailyCount))

datatable(bb_sockeye_escapements)
```

Finally join the two tables and display the final table:

```{r}
bb_escapement_with_goals <- 
  left_join(bb_sockeye_escapements, bb_sockeye_goals, by = c("Location" = "System")) %>% 
  mutate(is_goal_within = ifelse(Escapement < Upper & Escapement > Lower, TRUE, FALSE),
         drop_by_year = ifelse(Year >= Initial.Year, FALSE, TRUE))
         
datatable(bb_escapement_with_goals)
```

<!--chapter:end:exercise-data-cleaning-manipulation-tidyverse.Rmd-->

# Spatial vector analysis using `sf`

## Learning Objectives

In this lesson, you will learn:

- How to use the `sf` package to analyze geospatial data
- Static mapping with ggplot
- interactive mapping with `leaflet`

## Introduction

From the [**sf**](https://r-spatial.github.io/sf/articles/sf1.html) vignette:

> Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.


The **sf** package is an R implementation of [Simple Features](https://en.wikipedia.org/wiki/Simple_Features). This package incorporates:  

- a new spatial data class system in R  
- functions for reading and writing data  
- tools for spatial operations on vectors  

Most of the functions in this package starts with prefix `st_` which stands for *spatial* and *temporal*.

In this tutorial, our goal is to use a shapefile of Alaska regions and data on population in Alaska by community to create a map that looks like this:

![](images/alaska_population.png)

The data we will be using to create the map are:

* Alaska regional boundaries
* Community locations and population
* Alaksa rivers


## Reading a shapefile  

All of the data used in this tutorial are simplified versions of real datasets available on the KNB. I've simplified the original high-resolution geospatial datasets to ease the processing burden on your computers while learning how to do the analysis. These simplified versions of the datasets may contain topological errors. The original version of the datasets are indicated throughout the chapter.

For convience, I've hosted a zipped copy of all of the files on our test site. Please go to [this dataset](https://dev.nceas.ucsb.edu/view/urn:uuid:6f07cb25-a4a1-48e8-95cb-74f532f3ce2d) and download the zip folder. Put it in a sub-directory of your git repository and unzip it. You also will likely want to add that directory to your .gitignore file.

The first file we will use is a shapefile of regional boundaries in alaska derived from: Jared Kibele and Jeanette Clark. 2018. State of Alaska's Salmon and People Regional Boundaries. Knowledge Network for Biocomplexity. [doi:10.5063/F1125QWP](https://doi.org/10.5063/F1125QWP).

First load the libraries

```{r, warning = F, message = F}
library(sf)
library(dplyr)
library(ggplot2)
library(leaflet)
library(scales)
library(ggmap)
```

Read in the data and look at a plot of it.

```{r read_shp_sf}
## Read in shapefile using sf
ak_regions <- read_sf("data/shapefiles/ak_regions_simp.shp")

plot(ak_regions)  
```

We can also examine it's class.

```{r}
class(ak_regions)
```

**sf** objects usually have two types - `sf` and `data.frame`. Two main differences comparing to a regular `data.frame` object are spatial metadata (`geometry type`, `dimension`, `bbox`, `epsg (SRID)`, `proj4string`) and additional column - typically named `geometry`.

Since our shapefile object has the `data.frame` class, viewing the contents of the object using the `head` function shows similar results to data we read in using `read.csv`.

```{r}
head(ak_regions)
```

### Coordinate Reference System

Every `sf` object needs a coordinate reference system (or `crs`) defined in order to work with it correctly. A coordinate reference system contains both a datum and a projection. The datum is how you georeference your points (in 3 dimensions!) onto a spheroid. The projection is how these points are mathematically transformed to represent the georeferenced point on a flat piece of paper. All coordinate reference systems require a datum. However, some coordinate reference systems are "unprojected" (also called geographic coordinate systems). Coordinates in latitude/longitude use a geographic (unprojected) coordinate system. One of the most commonly used geographic coordinate systems is WGS 1984.

You can view what `crs` is set by using the function `st_crs`

```{r}
st_crs(ak_regions)
```

This is pretty confusing looking. Without getting into the details, that long string says that this data has a greographic coordinate system (WGS84) with no projection. A convenient way to reference `crs` quickly is by using the EPSG code, a number that represents a standard projection and datum. You can check out a list of (lots!) of EPSG codes [here](http://spatialreference.org/ref/epsg/?page=1). 

We will use several EPSG codes in this lesson. Here they are, along with their more readable names:

* 3338: Alaska Albers
* 4326: WGS84 (World Geodetic System 1984), used in GPS
* 3857: Pseudo-Mercator, used in Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI

You will often need to transform your geospatial data from one coordinate system to another. The `st_transform` function does this quickly for us. You may have noticed the maps above looked wonky because of the dateline. We might want to set a different projection for this data so it plots nicer. A good one for Alaska is called the Alaska Albers projection, with an EPSG code of [3338](http://spatialreference.org/ref/epsg/3338/).


```{r}
ak_regions_3338 <- ak_regions %>%
  st_transform(crs = 3338)

st_crs(ak_regions_3338)
```

```{r}
plot(ak_regions_3338)
```

Much better!

### Attributes

**sf** objects can be used as a regular `data.frame` object in many operations. We already saw the results of `plot` and `head`. Here are a couple more:

```{r}
nrow(ak_regions_3338)
```

```{r}
ncol(ak_regions_3338)
```

```{r}
summary(ak_regions_3338)
```

## `sf` & the Tidyverse

Since `sf` objects are dataframes, they play nicely with packages in the tidyverse. Here are a couple of simple examples:

`select()`

```{r select}
ak_regions_3338 %>%
  select(region)
```

Note the sticky geometry column! The geometry column will stay with your `sf` object even if it is not called explicitly.

`filter()`

```{r filter}
ak_regions_3338 %>%
  filter(region == "Southeast")
```


### Joins

You can also use the `sf` package to create spatial joins, useful for when you want to utilize two datasets together. As an example, let's ask a question: **how many people live in each of these Alaska regions?**

We have some population data, but it gives the number of people by city, not by region. To determine the number of people per region we will need to:

+ read in the city data from a csv and turn it into an `sf` object
+ use a spatial join (`st_join`) to assign each city to a region
+ use `group_by` and `summarize` to calculate the total population by region


First, read in the population data as a regular `data.frame`. This data is derived from: Jeanette Clark, Sharis Ochs, Derek Strong, and National Historic Geographic Information System. 2018. Languages used in Alaskan households, 1990-2015. Knowledge Network for Biocomplexity. [doi:10.5063/F11G0JHX](https://doi.org/10.5063/F11G0JHX). Unnecessary columns were removed and the most recent year of data was selected.

```{r}
pop <- read.csv("data/shapefiles/alaska_population.csv")
```

The `st_join` function is a spatial left join. The arguments for both the left and right tables are objects of class `sf` which means we will first need to turn our population `data.frame` with latitude and longitude coordinates into an `sf` object. 

We can do this easily using the `st_as_sf` function, which takes as arguments the coordinates and the `crs`. The `remove = F` specification here ensures that when we create our `geometry` column, we retain our original `lat` `lng` columns, which we will need later for plotting. Although it isn't said anywhere explicitly in the file, let's assume that the coordinate system used to reference the latitude longitude coordinates is WGS84, which has a `crs` number of 4236.

```{r}
pop_4326 <- st_as_sf(pop, 
                  coords = c('lng', 'lat'),
                  crs = 4326,
                  remove = F)

head(pop_4326)
```

Now we can do our spatial join! You can specify what geometry function the join uses (`st_intersects`, `st_within`, `st_crosses`, `st_is_within_distance`, ...) in the `join` argument. The geometry function you use will depend on what kind of operation you want to do, and the geometries of your shapefiles.

In this case, we want to find what region each city falls within, so we will use `st_within`.

```{r, eval = F}
pop_joined <- st_join(pop_4326, ak_regions_3338, join = st_within)
```

This gives an error! 

```
Error: st_crs(x) == st_crs(y) is not TRUE
```

Turns out, this won't work right now because our coordinate reference systems are not the same. Luckily, this is easily resolved using `st_transform`, and projecting our population object into Alaska Albers.

```{r}
pop_3338 <- st_transform(pop_4326, crs = 3338)
```

```{r}
pop_joined <- st_join(pop_3338, ak_regions_3338, join = st_within)

head(pop_joined)
```

### Group and summarize

Next we compute the total population for each region. In this case, we want to do a `group_by` and `summarise` as this were a regular `data.frame` - otherwise all of our point geometries would be included in the aggreation, which is not what we want. Our goal is just to get the total population by region. We remove the sticky geometry using `as.data.frame`, on the advice of the `sf::tidyverse` help page.

```{r}
pop_region <- pop_joined %>% 
  as.data.frame() %>% 
  group_by(region) %>% 
  summarise(total_pop = sum(population))

head(pop_region)
```

And use a regular `left_join` to get the information back to the Alaska region shapefile. Note that we need this step in order to regain our region geometries so that we can make some maps.

```{r}
pop_region_3338 <- left_join(ak_regions_3338, pop_region)

#plot to check
plot(pop_region_3338["total_pop"])
```


So far, we have learned how to use `sf` and `dplyr` to use a spatial join on two datasets and calculate a summary metric from the result of that join. 

The `group_by` and `summarize` functions can also be used on `sf` objects to summarize within a dataset and combine geometries. Many of the `tidyverse` functions have methods specific for `sf` objects, some of which have additional arguments that wouldn't be relevant to the `data.frame` methods. You can run `?sf::tidyverse` to get documentation on the `tidyverse` `sf` methods.

Let's try some out. Say we want to calculate the population by Alaska management area, as opposed to region.

```{r}
pop_mgmt_338 <- pop_region_3338 %>% 
  group_by(mgmt_area) %>% 
  summarize(total_pop = sum(total_pop))

plot(pop_mgmt_338["total_pop"])
```

Notice that the region geometries were combined into a single polygon for each management area.

If we don't want to combine geometries, we can specifcy `do_union = F` as an argument.

```{r}
pop_mgmt_3338 <- pop_region_3338 %>% 
  group_by(mgmt_area) %>% 
  summarize(total_pop = sum(total_pop), do_union = F)

plot(pop_mgmt_3338["total_pop"])
```

### Save

Save the spatial object to disk using `write_sf()` and specifying the filename. Writing your file with the extension .shp will assume an ESRI driver [driver](http://www.gdal.org/ogr_formats.html), but there are many other format options available.

```{r plot, eval = F}
write_sf(pop_region_3338, "shapefiles/ak_regions_population.shp", delete_layer = TRUE)
```

## Visualize with ggplot

`ggplot2` now has integrated functionality to plot sf objects using `geom_sf()`.

We can plot `sf` objects just like regular data.frames using `geom_sf`.

```{r}
ggplot(pop_region_3338) +
  geom_sf(aes(fill = total_pop)) +
  theme_bw() +
  labs(fill = "Total Population") +
  scale_fill_continuous(low = "khaki", high =  "firebrick", labels = comma)
```

We can also plot multiple shapefiles in the same plot. Say if we want to visualize rivers in Alaska, in addition to the location of communities, since many communities in Alaska are on rivers. We can read in a rivers shapefile, doublecheck the `crs` to make sure it is what we need, and then plot all three shapefiles - the regional population (polygons), the locations of cities (points), and the rivers (linestrings).


The rivers shapefile is a simplified version of Jared Kibele and Jeanette Clark. Rivers of Alaska grouped by SASAP region, 2018. Knowledge Network for Biocomplexity. doi:10.5063/F1SJ1HVW.

```{r}
rivers_3338 <- read_sf("data/shapefiles/ak_rivers_simp.shp")
st_crs(rivers_3338)
```

Note that although no EPSG code is set explicitly, with some sluething we can determine that this is `EPSG:3338`. [This site](https://epsg.io) is helpful for looking up EPSG codes.

```{r}
ggplot() +
  geom_sf(data = pop_region_3338, aes(fill = total_pop)) +
  geom_sf(data = rivers_3338, aes(size = StrOrder), color = "black") +
  geom_sf(data = pop_3338, aes(), size = .5) +
  scale_size(range = c(0.01, 0.2), guide = F) +
  theme_bw() +
  labs(fill = "Total Population") +
  scale_fill_continuous(low = "khaki", high =  "firebrick", labels = comma)
```

## Incorporate base maps into static maps using `ggmap`

The `ggmap` package has some functions that can render base maps (as raster objects) from open tile servers like Google Maps, Stamen, OpenStreetMap, and others.

We'll need to transform our shapefile with population data by community to `EPSG:3857` which is the CRS used for rendering maps in Google Maps, Stamen, and OpenStreetMap, among others.

```{r}
pop_3857 <- pop_3338 %>%
  st_transform(crs = 3857)
```

Next, let's grab a base map from the Stamen map tile server covering the region of interest.
First we include a function that transforms the bounding box (which starts in `EPSG:4326`) to also be in the `EPSG:3857` CRS, which is the projection that the map raster is returned in from Stamen. This is an issue with `ggmap` described in more detail [here](https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208)


```{r message=FALSE}
# Define a function to fix the bbox to be in EPSG:3857
# See https://github.com/dkahle/ggmap/issues/160#issuecomment-397055208
ggmap_bbox_to_3857 <- function(map) {
  if (!inherits(map, "ggmap")) stop("map must be a ggmap object")
  # Extract the bounding box (in lat/lon) from the ggmap to a numeric vector, 
  # and set the names to what sf::st_bbox expects:
  map_bbox <- setNames(unlist(attr(map, "bb")), 
                       c("ymin", "xmin", "ymax", "xmax"))
  
  # Coonvert the bbox to an sf polygon, transform it to 3857, 
  # and convert back to a bbox (convoluted, but it works)
  bbox_3857 <- st_bbox(st_transform(st_as_sfc(st_bbox(map_bbox, crs = 4326)), 3857))
  
  # Overwrite the bbox of the ggmap object with the transformed coordinates 
  attr(map, "bb")$ll.lat <- bbox_3857["ymin"]
  attr(map, "bb")$ll.lon <- bbox_3857["xmin"]
  attr(map, "bb")$ur.lat <- bbox_3857["ymax"]
  attr(map, "bb")$ur.lon <- bbox_3857["xmax"]
  map
}
```

Next, we define the bounding box of interest, and use `get_stamenmap()` to get the basemap. Then we run our function defined above on the result of the `get_stamenmap()` call.

```{r, message = F, warning = F}
bbox <- c(-170, 52, -130, 64)   # This is roughly southern Alaska
ak_map <- get_stamenmap(bbox, zoom = 4)
ak_map_3857 <- ggmap_bbox_to_3857(ak_map)
```

Finally, plot both the base raster map with the population data overlayed, which is easy now that everything is in the same projection (3857):

```{r message=FALSE}
ggmap(ak_map_3857) + 
  geom_sf(data = pop_3857, aes(color = population), inherit.aes = F) +
  scale_color_continuous(low = "khaki", high =  "firebrick", labels = comma)
```



## Visualize `sf` objects with leaflet

We can also make an interactive map from our data above using `leaflet`. 

Leaflet (unlike ggplot) will project data for you. The catch is that you have to give it both a projection (like Alaska Albers), and that your shapefile must use a geographic coordinate system. This means that we need to use our shapefile with the 4326 EPSG code. Remember you can always check what `crs` you have set using `st_crs`.

Here we define a leaflet projection for Alaska Albers, and save it as a variable to use later.

```{r}
epsg3338 <- leaflet::leafletCRS(
  crsClass = "L.Proj.CRS",
  code = "EPSG:3338",
  proj4def =  "+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs",
  resolutions = 2^(16:7))
```

You might notice that this looks familiar! The syntax is a bit different, but most of this information is also contained within the `crs` of our shapefile:

```{r}
st_crs(pop_region_3338)
```

Since `leaflet` requires that we use an unprojected coordinate system, let's use `st_transform` yet again to get back to WGS84.

```{r}
pop_region_4326 <- pop_region_3338 %>% st_transform(crs = 4326)
```



```{r}
m <- leaflet(options = leafletOptions(crs = epsg3338)) %>%
        addPolygons(data = pop_region_4326, 
                    fillColor = "gray",
                    weight = 1)

m
```

We can add labels, legends, and a color scale.

```{r}
pal <- colorNumeric(palette = "Reds", domain = pop_region_4326$total_pop)

m <- leaflet(options = leafletOptions(crs = epsg3338)) %>%
        addPolygons(data = pop_region_4326, 
                    fillColor = ~pal(total_pop),
                    weight = 1,
                    color = "black",
                    fillOpacity = 1,
                    label = ~region) %>% 
        addLegend(position = "bottomleft",
                  pal = pal,
                  values = range(pop_region_4326$total_pop),
                  title = "Total Population")

m

```


We can also add the individual communities, with popup labels showing their population, on top of that!

```{r}

pal <- colorNumeric(palette = "Reds", domain = pop_region_4326$total_pop)

m <- leaflet(options = leafletOptions(crs = epsg3338)) %>%
        addPolygons(data = pop_region_4326, 
                    fillColor = ~pal(total_pop),
                    weight = 1,
                    color = "black",
                    fillOpacity = 1) %>% 
        addCircleMarkers(data = pop_4326,
                         lat = ~lat,
                         lng = ~lng,
                         radius = ~log(population/500), # arbitrary scaling
                         fillColor = "gray",
                         fillOpacity = 1,
                         weight = 0.25,
                         color = "black",
                         label = ~paste0(pop_4326$city, ", population ", comma(pop_4326$population))) %>%
        addLegend(position = "bottomleft",
                  pal = pal,
                  values = range(pop_region_4326$total_pop),
                  title = "Total Population")

m

```

There is a lot more functionality to `sf` including the ability to `intersect` polygons, calculate `distance`, create a `buffer`, and more. Here are some more great resources and tutorials for a deeper dive into this great package:


[Raster analysis in R](http://jafflerbach.github.io/spatial-analysis-R/intro_spatial_data_R.html)  
[Spatial analysis in R with the sf package](https://cdn.rawgit.com/rhodyrstats/geospatial_with_sf/bc2b17cf/geospatial_with_sf.html)  
[Intro to Spatial Analysis](https://cdn.rawgit.com/Nowosad/Intro_to_spatial_analysis/05676e29/Intro_to_spatial_analysis.html#1)  
[sf github repo](https://github.com/r-spatial/sf)    
[Tidy spatial data in R: using dplyr, tidyr, and ggplot2 with sf](http://strimas.com/r/tidy-sf/)    
[mapping-fall-foliage-with-sf](https://rud.is/b/2017/09/18/mapping-fall-foliage-with-sf/)    



<!--chapter:end:geospatial-vector-analysis.Rmd-->

# Reproducibility and Provenance

## Learning Objectives

In this lesson, you will learn:

- About the importance of computational reproducibility
- The role of provenance metadata
- Tools and techniques for reproducibility supportred by the Arctic Data Center
- How to build a reproducible paper in RMarkdown

A great overview of this approach to reproducible papers comes from:

- Ben Marwick, Carl Boettiger & Lincoln Mullen (2018) **Packaging Data Analytical Work Reproducibly Using R (and Friends)**, The American Statistician, 72:1, 80-88, [doi:10.1080/00031305.2017.1375986](https://doi.org/10.1080/00031305.2017.1375986)

This lesson will draw from existing materials:

- [Accelerating synthesis science through reproducible science practices](files/2019-10-11-repro-sci.pdf)
- [rrtools](https://github.com/benmarwick/rrtools)
- [Reproducible papers with RMarkdown](https://nceas.github.io/oss-lessons/reproducible-papers-with-rmd/reproducible-papers-with-rmd.html)

To start a reproducible paper with `rrtools`, run:

```{r eval=FALSE}
devtools::install_github("benmarwick/rrtools")
setwd("..")
rrtools::use_compendium("arcticpaper")
```

Then, add some more structure to the package:

```{r eval=FALSE}
usethis::use_apl2_license(name="Matthew B. Jones")
rrtools::use_readme_rmd()
rrtools::use_analysis()
```

Now write a reproducible paper!

<!--chapter:end:provenance-reproducibility-datapaper.Rmd-->

# Additional Resources: Using NetCDF files

## Learning Objectives

In this lesson, you will:

- Learn to read data from a NetCDF file
- Wrangle the example data into a data frame
- Make some plots

## Introduction

NetCDF files are hierarchical data files that contain embedded metadata and allow for efficient extraction of data. They are particularly useful for storing large data, such as raster data and model outputs.

This lesson draws from a previous lesson written by Leah Wasser, available [here](https://nceas.github.io/oss-lessons/spatial-data-gis-law/4-tues-spatial-analysis-in-r.html).

This [R blog post](https://www.r-bloggers.com/a-netcdf-4-in-r-cheatsheet/) also contains some good introduction material.

## Reading in data

First let's load the `ncdf4` package

```{r, warning = F, message = F}
library(ncdf4)
library(ggplot2)
library(dplyr)
library(tidyr)
```

Let's grab an example file. Download the .nc file from Fiamma Straneo. 2019. Temperature and salinity profiles adjacent to a tidewater glacier in Sarqardleq Fjord, West Greenland, collected during July 2013. Arctic Data Center. doi:10.18739/A2B853H78. [http://doi.org/10.18739/A2B853H78](http://doi.org/10.18739/A2B853H78)

First we open a connection to our NetCDF file using `nc_open`.

```{r}
nc <- nc_open("data/WG2013CTD.nc")
```

You can print information about what is contained in the file using the `print` function on the `nc` object.

```{r}
print(nc)
```

The netcdf file has a lot of information in the top level. You can navigate through the `nc` connection using the list selector operator. For example:

```{r}
nc$filename
```

You can return the names of the variables by using the `attributes` function on the `var` element within the `nc` object.

```{r}
vars <- attributes(nc$var)$names
vars
```

Note that we haven't read in any data yet - we have only read in all of the **attributes**, which are all of the different fields used to store metadata.  

You can retrieve individual variables by calling `ncvar_get`, and the variable by name.

```{r}
sal <- ncvar_get(nc, "sal")
time_mat <- ncvar_get(nc, "time")
```

Note that if the file also has dimension variables, you can retrieve these values the same way as if they were variables.

```{r}
#examine dimension variable names
names(nc$dim)
```

Read in the depth dimension variable.

```{r}
depth <- ncvar_get(nc, "z")
```

## Reshaping the data into a data.frame

Depending on what your analysis goals are, you may want to convert your data into a `data.frame` structure. These data would work well in one since it is not a big dataset, and it is not gridded. Other dataset types, like gridded raster data, should be dealt with differently (such as using the `raster` package). 

First, we might want to convert the MATLAB date-time number to a POSIXct number.

```{r}
time <- as.POSIXct((time_mat + 719529)*86400, origin = "1970-01-01", tz = "UTC")
```

Next we coerce the salinity matrix, which is represented with rows according to time and columns according to depth, into a data frame,

```{r}
# coerce matrix to data frame
salinity_data <- as.data.frame(sal) 
```

We then assign column names to the character value of our depth vector.

```{r}
# name columns according to depth dimension
names(salinity_data) <- as.character(depth) 
```

And finally, we add the time column to our matrix, gather over the depth columns, and turn the depth column back to a numeric value,

```{r}
salinity_data <- salinity_data %>% 
    mutate(time = time) %>% 
    gather(key = "depth", value = "salinity", -time) %>% 
    mutate(depth = as.numeric(depth))
```

## Plotting

First let's try to make a `raster` plot using `geom_raster`.

```{r}
ggplot(salinity_data, aes(x = time, y = depth, fill = salinity)) +
    geom_raster() +
    theme_bw() +
    ylab("Depth (m)") +
    xlab("") +
    scale_fill_continuous(low = "gray", high = "red", name = "Salinity (psu)")
```

Turs out the data are fairly discontinuous, so we might want something like this instead, overlaying the profile data together.

```{r}
ggplot(salinity_data, aes(x = salinity,
                          y = depth,
                          group = time,
                          color = time)) +
    geom_line(size = .1) +
    scale_y_reverse() +
    theme_bw() +
    ylab("Depth (m)") +
    xlab("Salinity (psu)") +
    theme(legend.title = element_blank())
```


<!--chapter:end:data-netcdf-intro.Rmd-->

# Additional Resources: Collaboration, authorship and data policies

## Resources

Example Codes of Conduct

- [NCEAS Code of Conduct](https://www.nceas.ucsb.edu/files/NCEAS_Code-of-Conduct_2019.pdf)
- [Arctic Data Center Code of Conduct](https://docs.google.com/document/d/1g5VKoKkxAcP-hm_25KJnpG3pZSkqfdOU1iFPyTjCT2s/edit)
- [Carpentries Code of Conduct](https://docs.carpentries.org/topic_folders/policies/code-of-conduct.html)
- [Mozilla Science Code of Conduct](https://science.mozilla.org/code-of-conduct)
- [Mozilla Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/)
- [Ecological Society of America Code of Conduct](https://www.esa.org/esa/code-of-conduct-for-esa-events/)
- [American Geophysical Union Code of Conduct](https://fallmeeting.agu.org/2018/agu-meetings-code-of-conduct/)

Policy Templates: 

- [Authorship Policy](files/template-authorship-policy-ADC-training.docx)
- [Data Policy](files/template-data-policy-ADC-training.docx)

An [example lab policy](https://github.com/temporalecologylab/labgit/blob/master/datacodemgmt/tempeco_DMP.pdf) that combines data management and sharing practices with authorship guidelines from the [Wolkovich Lab](http://temporalecology.org/). Shared with permission from Elizabeth Wolkovich.


## References

- [Cheruvelil, K. S., Soranno, P. A., Weathers, K. C., Hanson, P. C., Goring, S. J., Filstrup, C. T., & Read, E. K. (2014). Creating and maintaining high-performing collaborative research teams: The importance of diversity and interpersonal skills. Frontiers in Ecology and the Environment, 12(1), 31-38. DOI: 10.1890/130001](https://esajournals.onlinelibrary.wiley.com/doi/epdf/10.1890/130001)

<!--chapter:end:collaboration-social-data-policies.Rmd-->

